{"start":[0,1680,3700,7440,11690,14930,17050,19710,21330,23590,25700,28170,31730,34010,36390,37900,40580,42930,46880,49120,51540,53600,56560,58300,60730,62110,64904,68000,72500,75640,77150,79990,83940,87480,90870,92110,93530,95580,98890,100860,102690,103770,107850,108920,109450,111210,112360,114040,116580,116666,117140,118970,121030,123620,124240,126940,129669,133490,134560,137390,140730,142790,143330,144820,147650,149750,150980,152270,155360,156500,159920,161390,164110,165850,170270,173260,175530,178700,180780,184140,188880,190700,193960,197480,201300,203230,205840,208150,209790,212050,215280,217980,220270,221926,226080,226765,229910,231310,232790,236100,238830,241340,245645,247130,248860,251390,255450,258450,260079,262660,265470,267440,269240,270500,273890,274790,276670,278410,282560,285360,286820,290440,291600,295800,298880,304630,308340,308760,311100,312670,315630,316760,318720,320720,322730,324420,328520,329730,333710,334970,338730,340270,343120,344310,346700,350260,353600,356760,358802,360330,364060,366010,369520,371950,375870,379050,380820,385470,389740,393750,395580,398050,400890,406240,408870,411120,415100,416500,420730,421790,426130,430110,433840,436960,442950,447680,449460,450990,454470,458130,461840,464215,465310,467320,471930,476520,478900,481750,484240,486650,487980,490010,494160,495940,498790,502630,507000,510050,513990,516360,519960,523549,527440,531940,534900,538240,539870,541400,544480,545950,548640,550890,554210,556790,558540,561340,565200,570600,576050,579010,581860,584880,588350,591510,593090,596460,598690,600350,603360,607240,610440,612800,615960,619480,621410,624090,627320,630250,633710,634960],"end":[1680,3700,7440,11690,14930,17050,19710,21330,23590,25700,28170,31730,34010,36390,37900,40580,42930,46880,49120,51540,53600,56560,58300,60730,62110,64904,68000,72500,75640,77150,79990,83940,87480,90870,92110,93530,95580,98890,100860,102690,103770,107850,108920,109450,111210,112360,114040,116580,116666,117140,118970,121030,123620,124240,126940,129669,133490,134560,137390,140730,142790,143330,144820,147650,149750,150980,152270,155360,156500,159920,161390,164110,165850,170270,173260,175530,178700,180780,184140,188880,190700,193960,197480,201300,203230,205840,208150,209790,212050,215280,217980,220270,221926,226080,226765,229910,231310,232790,236100,238830,241340,245645,247130,248860,251390,255450,258450,260079,262660,265470,267440,269240,270500,273890,274790,276670,278410,282560,285360,286820,290440,291600,295800,298880,304630,308340,308760,311100,312670,315630,316760,318720,320720,322730,324420,328520,329730,333710,334970,338730,340270,343120,344310,346700,350260,353600,356760,358802,360330,364060,366010,369520,371950,375870,379050,380820,385470,389740,393750,395580,398050,400890,406240,408870,411120,415100,416500,420730,421790,426130,430110,433840,436960,442950,447680,449460,450990,454470,458130,461840,464215,465310,467320,471930,476520,478900,481750,484240,486650,487980,490010,494160,495940,498790,502630,507000,510050,513990,516360,519960,523549,527440,531940,534900,538240,539870,541400,544480,545950,548640,550890,554210,556790,558540,561340,565200,570600,576050,579010,581860,584880,588350,591510,593090,596460,598690,600350,603360,607240,610440,612800,615960,619480,621410,624090,627320,630250,633710,634960,635330],"text":["","PROFESSOR: This brings us to one of t he most important ideas in","reinforcement learning, and that is because you want to do the optimal","thing but you don't know what it is yet, you have this inevitable","trade-off between exploration, where you try things--","which may, of course, be disastrous--","and exploitation, where you do the things which currently","appear to be good.","You meet this all the time in your daily life.","Some new restaurant opens and you have a choice.","You can go to your favorite place, which you know will bring you good","rewards, or you can try this new thing, which is probably bad.","But it may be your new favorite restaurant.","If you try it, you may have a bad meal.","You may get food poisoning.","But if you don't try it, you may be missing out on your favorite","restaurant until the end of time.","And so inevitably, the outcome is you have to try some stuff.","And that means you have to make some mistakes.","Let's take a look at that.","So this one's manual.","This is a variance of the grid world, where--","well, I won't tell you what it is.","You look at this and you think how should I act?","What is this?","Well, you've been listening to this lecture long enough that you know this","is probably some fire pits on the side of a bridge or something.","But for all you know, this is just 12 doors, one of which has a huge prize.","So first of all, just looking at this, you kind of have to try everything.","Now, what if we did this?","What if we went over there and we did that a couple times?","It looks like we're going to get plus 1 if we go there.","Now if all we did was exploit, we would continue going left forever and","we would receive a string of plus 1's, episode after episode.","Is that the best we can do?","Well, maybe.","We could be following the optimal policy right now.","But, in some sense, we've got to try other stuff.","Because we're going to be playing this game for a while.","As far as we know, we're going to play it forever.","And so we need to explore.","So we might do things like jump off the cliff.","That was bad.","OK.","Maybe we'll jump off again, just to make sure we","didn't get a weird outcome.","No, it's looking pretty bad.","We might go over here.","Oh.","That one's good.","It's so good that now the other one doesn't even look","good to the left anymore.","And so now, we go here.","Now are done?","Well, we've got a lot of things we should try.","Now, let me point out something, which is that you have a lot of knowledge","about the structure of this MDP, so once you see one fire pit, you imagine","maybe there are some others.","The algorithm has no such concept right now.","As far as we know, jumping off this cliff will be equally good as the good","exit, but in fact, it's equally unsuccessful.","Well, what about this pit?","We've got to try that too.","So you spend a lot of time jumping into pits because you really just","don't know their pits until you try it.","So you've got to make some mistakes.","You've got to explore.","But, of course, you don't want to just be jumping in them over and over again","once you know they're bad.","So you don't want to explore randomly or forever.","We know we've got to try things.","The simplest way to force exploration is what's called an","epsilon greedy policy.","So let's remember that Q-Learning did not specify how we selected actions,","it just required that they have sufficient variety to converge to the","optimal policy and Q values.","Now we're talking about how to select actions in a way that will enable","Q-Learning to do it's magic.","The simplest thing we could do is be epsilon greedy, which means at every","time step, with some presumably small probability, we just act randomly,","just to try something out.","And with, presumably, a large probability, we act according to our","current policies best guess, which means we often go to our favorite","place, but every now and then when the coin comes up random, we try","something at random.","There are some problems with taking random actions.","Let's see an agent taking random action and see what","those problems are.","So here's the crawler.","And now you can finally see what this epsilon control here is.","This means 80% of the time, we take a random action.","That's a lot of exploration.","It's not, however, a lot of progress.","","Now will this work?","K Q-Learning works just fine with lots of random actions.","It just doesn't necessarily work quickly.","So I skip some steps.","And we've got some vague clue.","I'm going to skip a million steps.","So we compute and we compute and we compute and we compute.","And now it actually turns out that this bot has had a lot of experience","this year, even though I fast forwarded it.","Presumably it should have learned something.","Why is it not making any progress?","Well, we're forcing it to act randomly 80% of the time.","If whenever you walked, your left leg, 80% of the time, flew out at some","random angle, you wouldn't make fast progress.","","So what happens if I turn epsilon down.","Turn it off, and just let it go.","Be optimal.","It's not so bad.","So Q-Learning learned the right thing, but the exploration was preventing you","from doing it.","So there were a couple issues there.","Let's see what they were.","One issue is you do eventually explore the space, you eventually learn the","right thing, but you still keep thrashing around if you","keep epsilon high.","So at a bare minimum, you would eventually have to stop exploring so","much random.","Eventually, you would have to stop doing so much exploration and allow","yourself to exploit this optimal policy you've learned.","Another problem with epsilon greedy is that the exploration is unstructured.","You try random things whether you need to or not, even if you know what the","actions do.","If you know what all the actions do, you shouldn't necessarily","be exploring anymore.","So one solution here is we just lower epsilon over time and let the","randomness decrease.","But there are better ways, and there are a lot of better ways.","I'm going to sketch one simple one, which is what's called","an exploration function.","So why should we explore?","Well, random actions explore a fixed amount, and they kind of explore","uniformly everywhere.","Every state has an equal chance of doing an exploration action.","Here's a better idea.","Why don't we think about our actions, and thereby our Q values, as being","something we're trying to learn?","Now, sometimes we know what they are, and when we know what they are, we","don't have to try them anymore.","You get food poisoning a couple times and you just stop.","On the other, when we don't know something and our uncertainty is high,","it should be a pretty high priority to figure that out right now.","So when the restaurant opens, you try it, because your uncertainty is high.","Once you've tried it a couple times, now you know.","So how can we encode this?","We'd like something that forces us to explore areas whose badness is not yet","established, but eventually stopped.","And the basic idea here is is the quote you often hear is, in the face","of uncertainty, you should have optimism.","So fly into those unknown caves and see what's in there.","Optimism shouldn't last forever, so we might have a function like this.","Here's a very crude way of doing it.","But this basically accomplishes the goal I sketched, which is rather than","looking at just utilities of Q states or states, we have a function which","considers our guess at the utility, u, and the number of","times we've been there.","And from a Q state, the number of times we've been there means the","number of times we've tried that action out.","So we take the utility and we add to it a bonus that decrease as we visit","the state more times, n.","So the regular Q update looks like this.","It says take your current estimate of Qsa for the action, a, you","just tried from s.","This arrow means blend in this new value on the right- hand side with","weight alpha.","So this says take our estimate and replace it with a one-step look ahead.","And it's a sample of the instantaneous reward we got plus our current best","estimate of the future value in the landing state, s prime.","With an exploration function, we can modify this.","We can modify this by replacing the Q value with a boosted Q value, which is","artificially inflated if we haven't tried that Q state many times, meaning","we're uncertain.","And what this does is kind of cool.","Not only does this tell you that a Q state you haven't tried has higher","value, but also it will propagate this bonus back.","So you don't just try things that are unknown, you try things that are known","to lead to states that are unknown.","And that's great.","Let's see it in action.","So here is that same bot.","It's still doing a tiny bit of exploration through randomness, but","it's implementing an exploration function.","","So this bot is implementing an exploration function.","And as time goes on, the exploration function will","contribute less and less.","And what that means is that even though at the beginning it's trying","all kinds of stuff, very quickly it figures out that it actually knows","what those actions do.","And the dominant behavior is now one of exploitation.","","And it's a slightly weird policy, but he's off.","So unlike the other one, which had to run millions of steps and then I had","to turn its exploration off, this one is already moving after a very small","number of iterations.","Regret is another one of those terms that you use all the time in daily","life, but it means something specific here in this course.","The basic idea of regret is even though you learn the optimal policy","eventually, because your transitions and rewards are initially unknown,","it's inevitable that you will make some mistakes along the way.","So you don't get to be the wise, optimal robot without making some","mistakes in your youth.","So, for example, here, this robot.","It's wise, it's optimal, and it's remembering back to its youth where it","jumped into that fire pit.","Now, it knows now not to do that.","And you can think of this as regret.","But a certain amount of fire pit jumping is inevitable, because you","just had no way of knowing whether or not it was a fire pit","until you tried it.","So what is regret formally in this class?","Formally, regret is a measure of your total mistakes.","It's the difference between you rewards that you got, fire pit testing","and all, compared to the optimal rewards you would have gotten had you","had the optimal policy all along.","Now, of course, all those rewards have to be expected rewards, because you've","got to average all the stuff over all the possible outcomes.","But there's this idea that there's a total volume of mistakes you made","compared to having the optimal policy.","We'd like to minimize that.","So we've already seen Q-Learning subject to some mild conditions.","We'll eventually learn the optimal policy.","You're going to be optimal in the end.","So really, in practice, the game is minimizing your regret.","Get to that optimal policy while making as few suboptimal, in","retrospect, actions as possible.","Minimizing regret is more than learning to be optimal.","It's more like optimally learning to be optimal.","And one example of this is random exploration, like epsilon greedy,","compared to an exploration function.","You'll be optimal in the end with Q-Learning either way, but you'll have","higher regret with random exploration, because with random explanation, you","kind of keep making mistakes and your mistakes are unguided.","You make them even when you know yo u shouldn't do them.","So that's regret.",""]}