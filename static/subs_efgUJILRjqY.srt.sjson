{"start":[0,1590,2690,5760,7450,10420,12730,14260,18900,21390,24580,30310,32520,34300,36670,38250,39810,41595,42800,45770,48770,50260,51310,52920,56490,58035,60060,61710,64700,66560,69700,73690,77200,81260,83060,86290,89270,91540,94380,98300,99825,101750,102600,105370,106060,107750,110670,112550,114740,115990,118570,119670,122950,125650,126780,130539,134710,137000,139650,143340,146440,147040,149690,152350,152820,154200,156010,158190,162650,166400,167700,169110,173260,177390,182750,186820,190730,194890,198380,198860,201740,202890,208290,214350,217150,221210,223550,229430,231080,234340,237420,239080,239635,242255,245250,248070,251880,254770,259170,261360,264910,268270,269080,272080,275700,278740,284070,287220,290630,295750,296920,298910,303020],"end":[1590,2690,5760,7450,10420,12730,14260,18900,21390,24580,30310,32520,34300,36670,38250,39810,41595,42800,45770,48770,50260,51310,52920,56490,58035,60060,61710,64700,66560,69700,73690,77200,81260,83060,86290,89270,91540,94380,98300,99825,101750,102600,105370,106060,107750,110670,112550,114740,115990,118570,119670,122950,125650,126780,130539,134710,137000,139650,143340,146440,147040,149690,152350,152820,154200,156010,158190,162650,166400,167700,169110,173260,177390,182750,186820,190730,194890,198380,198860,201740,202890,208290,214350,217150,221210,223550,229430,231080,234340,237420,239080,239635,242255,245250,248070,251880,254770,259170,261360,264910,268270,269080,272080,275700,278740,284070,287220,290630,295750,296920,298910,303020,304300],"text":["","DAN KLEIN: So what's rationality?","We have this idea that we want rational agents.","We want utilities.","We know we have to have preferences.","Are there kind of good preferences and bad preferences?","Are there always utility functions?","It all gets down to what exactly we mean when we say rationality.","So preferences are the starting point.","Agents have preferences, and we're going to try to figure out under what","circumstances those preferences can be boiled down to a utility function.","So we'd like some constraints on these preferences before","we call them rational.","There's a wide variety of preferences that are OK.","Maybe the agent wants to make money.","Maybe the agent wants to spend money.","Maybe the agent likes ice cream.","Maybe the agent hates ice cream.","These are all perfectly good preferences, but there are certain","kinds of preference that just make no sense, and so we need some","constraints.","Here's one constraint.","The axiom of transitivity.","This says if you prefer a to b, and you prefer b to c, you","better prefer a to c.","Well, that seems reasonable.","Why should we have this?","Well, if an agent has preferences which do not obey","this, it has some flaws.","Like you can make it give away all of its money, or anything else.","Because if, for example, if I likes b better than c and it's got c, you can","give it b in return for, maybe a penny.","But if it likes a better, you can now give it a in return for a penny.","Oh, but wait, it likes c better than a.","So now you give it c, which is where it started, in return for a penny.","So it's back where it started, except now you have three of its pennies.","And you keep doing this until it has no more money.","This seems like a defect in behavior.","And so we're going to say we require this axiom in order to declare","preferences rational.","That seems pretty reasonable.","There are more of them.","These are the actions of rationality.","So what do these say?","Let's look at them.","Orderability says given two things, you either have to like a better, or","you have to like b better, or you have to be indifferent.","It's kind of hard to argue with that.","Transitivity, we just saw.","It says if a is better than b, and b is better than c, then a should be","better than c.","We've already seen what can happen when that goes wrong.","The other ones are a little more subtle, but they're pretty plausible.","So here's continuity.","Continuity says if you like a better than c, and b is somewhere in the","middle, then there has to be some lottery between a and c for which you","are indifferent between the lottery and b.","And essentially, this means you can interpolate.","If b is in between a and c, then it's equivalent to some lottery","between a and c.","Substitutability.","This says if you're indifferent between a and b, then you're","indifferent whenever they're plugged into lotteries.","That makes sense.","If they're exchangeable on their own, they should be","exchangeable in a lottery.","Monotonicity, also pretty reasonable.","This says that if a is better than b, and you have a lottery between a and","b, then you'd prefer more a in the mix.","That seems to make sense.","So here's a bunch of axioms.","There's a theorem that says rational preferences that they obey these","axioms can be described as maximization of expected utility.","Basically, if you accept these axioms, there's a theorem that says all of","your preferences can be described with the utility function.","So if you obey these axioms, we give you the stamp of rationality.","And that means that preferences that violate this are irrational.","Preferences that meet this are rational, but there's still a lot of","wiggle room.","Lots of different preferences meet this.","Here's the theorem.","The theorem says given any preferences satisfying the axioms of rationality,","there exists a real valued function u that has the following properties.","If you prefer a to b, it has a higher utility.","So this utility function captures your ordinal preferences.","And here's the amazing part.","The utility of a lottery is the expectation of the utilities of the","elements of the lottery.","This means that this utility function not only captures your ordinal","preferences amongst prizes, but it captures your","preferences between lotteries.","It's amazing.","It means your utilities are preserved under expectation.","","Now we're back to the maximum expected utility principal.","The principal says choose the action that maximizes your expected utility.","And this is now well defined, because the theorem says that if our","preferences were actually rational in the first place, there is some such","utility function.","Now, an agent can be rational, meaning consistent with the MEU principle","without ever actually representing a utility, or manipulating a","probability.","For example, you can have a look up table for tic-tac-toe.","You can have a reflex vacuum cleaner that in a given configuration, just","goes after the closest dirt, and that turns out to be optimal.","But still, you want your behaviors to be consistent with the MEU principle.","And also what this theorem means is even though you are human and you are","individual, and you have all these subtle preferences that are nuanced","and reflect your innermost workings, sorry, you can in fact be reduced to a","utility function.","It's just the way it is.","Unless you violate the axioms of rationality, and hopefully you don't.",""]}