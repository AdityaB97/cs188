{"start":[0,1290,1530,5180,10990,14070,18140,21980,26220,29380,31430,35760,39250,42600,46990,49890,54040,59450,61270,64150,67870,71590,73810,75690,78030,79740,82500,84770,87040,89420,92410,93990,98940,101690,104300,106230,109880,110960,115290,120520,122500,127710,128860,130870,132410,135730,137340,143030,146980,150050,151670,155830,162310,166430,168500,170370,173840,178500,181670,186260,190250,194140,198900,201600,207480,209870,213320,214850,219230,221800,223930,227120,230340,234710,237000,239290,245790,247870,253190,258279,261730,264100,266740,268590,272590,273740,277180,280370,285790,289320,293500,295670,298760,300310,301330,303570,308470,309840,314970,315830,319930,325210,329230,334540,337980,343030,345590,350440,353430,354790,358340,360990,362180,366670,369510,371530,376260,380600,384780,387140,390740,394100,398550,399850,403390,405750,409040,411745,413550,416790,420130,422960,427880,431340,433110,436530,441460,445350,446640,450390,453550,455900,461280,464396,468930,471670,475270,478030,480390,484610,487660,489580,493780,496650,500110,502480,506050,510380,512210,516299,520169,524390,528270,531800,534680,537050,538650,542740,546080,549720,551860,553660,556910,560760,562010],"end":[1290,1530,5180,10990,14070,18140,21980,26220,29380,31430,35760,39250,42600,46990,49890,54040,59450,61270,64150,67870,71590,73810,75690,78030,79740,82500,84770,87040,89420,92410,93990,98940,101690,104300,106230,109880,110960,115290,120520,122500,127710,128860,130870,132410,135730,137340,143030,146980,150050,151670,155830,162310,166430,168500,170370,173840,178500,181670,186260,190250,194140,198900,201600,207480,209870,213320,214850,219230,221800,223930,227120,230340,234710,237000,239290,245790,247870,253190,258279,261730,264100,266740,268590,272590,273740,277180,280370,285790,289320,293500,295670,298760,300310,301330,303570,308470,309840,314970,315830,319930,325210,329230,334540,337980,343030,345590,350440,353430,354790,358340,360990,362180,366670,369510,371530,376260,380600,384780,387140,390740,394100,398550,399850,403390,405750,409040,411745,413550,416790,420130,422960,427880,431340,433110,436530,441460,445350,446640,450390,453550,455900,461280,464396,468930,471670,475270,478030,480390,484610,487660,489580,493780,496650,500110,502480,506050,510380,512210,516299,520169,524390,528270,531800,534680,537050,538650,542740,546080,549720,551860,553660,556910,560760,562010,563150],"text":["","DAN KLEIN: All right.","Now let's talk about an important algorithm called policy iteration,","which combines the idea of evaluating one policy with the idea of improving","that policy on the basis of those values.","You can think of policy iteration as you've got a policy in front of you,","and you're constantly trying to make it better.","So let's think about y value iteration as not always the best solution.","Well, what value iteration does is essentially","mimics the Bellman updates.","You have iterations where k gets larger and larger, starting at zero.","And for each iteration, you visit each state.","And for each state, you look at each action.","And for each action, you look at each outcome.","One problem with that process is it's slow.","And the reason it's slow is each iteration not only looks at every","source and target state, and so is s squared, it also has to consider each","action each time.","And there are often many actions.","Another problem is that although you're considering all of these","actions, the maximum often doesn't change.","Let's see a demo of that.","So here is a grid.","This is sum setting of grid world.","And let's watch what happens.","At v0, all the values are going to be zero because there is","0 time steps left.","At v1, you can see one step into the future.","If you're at an exit, you get the exit value.","If you're anywhere else, you get just the living reward, which in this case","is minus 0.1.","If I look further into the future, we know that in value iteration, the","rewards from that plus 1 are going to propagate out.","","This is nine iterations in.","If I run it to convergence, things keep changing.","Why is that?","That's because there are very long sequences that still give you points","from that plus 1, that take a while to register in the lower left.","But let's look a little more closely.","What I want you to watch this time is I want you to watch as I increase k in","value iteration.","I want you to watch the arrows.","And we're going to this twice.","First I want you to watch them changing.","So at the beginning, they're going to change a lot.","","But eventually, they stop changing.","And then even when I run into 100, they still haven't changed since","iteration eight or nine.","Secondly, watch in kind of the lower left.","These numbers are changing, but it takes a while before the change is to","become large enough that they impact one of the arrows.","So now we're in a position where we can see these","problems with value iteration.","Each iteration is slow because it considers all of the actions.","But that maximum, that best action that you got when you considered them","all usually doesn't change.","So you wasted all that time, if you have 100 actions, checking the other","99 that weren't very good last round.","They're still not very good, but you still check them all.","In addition, as we saw in that example, the policy may be converged","when the values are nowhere near converged yet.","So the policy tends to finish long before the values converge.","So what can we do?","The idea here is an algorithm called policy iteration, which is an","alternative approach.","And the basic sketch is we're going to have two steps that we alternate.","Step one is policy evaluation.","We're going to have some fixed policy.","It's generally not going to be an optimal policy, but we're going to","figure out its values using policy evaluation.","Which if we remember, was fast because it didn't have that factor of a.","So we evaluate the policy we have, even though it's","not an optimal policy.","Then we take those values, and we extract a better policy from them.","That's called policy improvement.","And the way we do policy improvement is we extract the policy implied by a","one step look ahead from those values we just computed in step one.","We repeat these until the policy converges, and it turns out you're","guaranteed that the policy will always keep getting","better, and then converge.","This algorithm is policy iteration.","It's still optimal, and it can converge much faster under some","conditions.","The conditions under which it converges much faster tend to be ones","where there are a large number of actions, but where the maximizing","action doesn't change very much during value iteration rounds.","If we wrote this out in equations, it would look something like this.","Step one is evaluation, so I fix some policy, pi, and I'm going","to compute v pi.","That is we asked before why would we want to compute the","values for a fixed policy?","Here's a case.","I fixed pi, and I compute the values.","And so I do this modified, or simplified update where I don't max","over the actions.","I assume that my policy pi is fixed, and I only do the averaging at the","chance nodes.","So this thing is fast, because I don't consider all the actions.","Once this thing converges, or gets close enough, I then stop and I say,","let's give the actions a chance to change, because we've done enough of","this evaluation that maybe by now, some of the actions need to change.","So again, we're going to do a one step look ahead, and just unroll that","expectimax one layer, and say the new policy is the action that I would get","if I did a one step expectimax.","And then plugged in the values, not optimal values, but the values","computed against my old policy.","That's a little weird, right?","We do one step of actual expectimax, and our truncation","function is our old policy.","Now, why should that be better?","Well, maybe the policy was good, or maybe the old policy was bad.","But whatever it was, good or bad, we've just pushed it one","step into the future.","And that step where we did that one layer of expectimax, that layer,","insofar as a layer can be optimal, did the optimal computation.","So we've got one layer of real optimal expectimax, and then the truncation","function is the old values.","And every time we do this, we push the old stuff farther and farther into the","future, and thereby make progress.","If you think about this, it actually turns out the improvement step is just","like value iteration.","We loop over all the actions, and for each action, we loop over all of the","results s prime.","So the improvement step isn't going to be particularly fast.","It's essentially the evaluation step where we get a big speed","up over value iteration.","And because the evaluation step happens many times before we allow an","improvement step, we get a big speed up.","Another way of looking at this algorithm is thinking that we're doing","value iteration, but on most rounds we just go with the last action that","optimized for the state, rather than considering them all.","OK, let's compare them.","Both value iteration and policy iteration do the same thing.","You input an MDP, and what is output is an optimal quantity.","And on the surface that's the optimal values, but they both also provide","optimal policies.","In the case of policy iteration, it explicitly provides that.","In the case of value iteration, you just keep track of it as you go.","So they both compute optimal values.","In value iteration, every iteration updates the values and implicitly by","doing this max every time, the policy.","In policy iteration, we do lots of passes that only update the utilities,","the values for this fixed policy.","Only when we do a policy improvement step do we allow the policy to change,","and because of that, it's often faster.","Both of them of the same class of thing.","They are both dynamic programs which take an MDP as input, and produce","optimal values and policies as outputs.","We've got a lot of algorithms by now.","Even setting aside expectimax, which was a way of choosing actions in an","MDP, we've still got a lot of different things.","If you want to compute optimal values, we've got value iteration, and we've","got policy iteration, which do the same thing.","Policy iteration's a little more complex, but it's often faster.","If all you wanted to do was compute values for a particular policy, we","have policy evaluation.","If all you wanted to do was turn your values into a policy, we have policy","extraction, which you can think of as just a one step look ahead.","Depending on what you have as input, a policy or not, and what you want as","output, optimal quantities or just the evaluation of a single policy, that","determines what equations you use and what algorithm you use.","And so you might be saying, these all look the same.","That is a good reaction to have.","They basically are all the same.","These are all Bellman updates turned into an iterative algorithm.","They all use a one step look ahead expectimax fragment.","The difference is whether or not you plug in a fixed policy, or actually do","a max over the actions.","That's it.","So once you really understand these algorithms, you'll start feeling like,","oh, it's all just Bellman equations and the corresponding dynamic program.","And that's a great place to be.",""]}