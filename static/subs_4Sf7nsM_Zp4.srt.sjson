{"start":[0,680,2930,6250,9090,10300,13670,20070,23560,27980,29790,32509,34390,37240,38630,40540,43720,45160,47220,49180,51550,55070,57460,60670,63460,64120,67520,69180,71330,75110,76410,79760,82436,83910,87090,90720,92280,92810,94680,99120,102060,105170,107430,109190,111490,114790,117600,120470,122790,124815,127140,130070,134160,135380,137910,141330,144090,145640,147130,150460,151810,153080,154210,155120,159510,163410,166330,169030,174110,177010,179170,182480,184970,186540,189040,190840,194400,196900,198390,201040,204550,205430,207340,209560,211970,215970,219070,223450,225870,229010,230860,232010,233365,236190,237650,239740,243500,245930,249870,251010,253090,258860,263080,266810,269880,271510,275220,276630,281570,284350,285390,288360,289750,292400,294120,298580,302690,305490,308740,309830,314460,316150,318530,321280,323205,327500,332380,334820,338390,339290,340620],"end":[680,2930,6250,9090,10300,13670,20070,23560,27980,29790,32509,34390,37240,38630,40540,43720,45160,47220,49180,51550,55070,57460,60670,63460,64120,67520,69180,71330,75110,76410,79760,82436,83910,87090,90720,92280,92810,94680,99120,102060,105170,107430,109190,111490,114790,117600,120470,122790,124815,127140,130070,134160,135380,137910,141330,144090,145640,147130,150460,151810,153080,154210,155120,159510,163410,166330,169030,174110,177010,179170,182480,184970,186540,189040,190840,194400,196900,198390,201040,204550,205430,207340,209560,211970,215970,219070,223450,225870,229010,230860,232010,233365,236190,237650,239740,243500,245930,249870,251010,253090,258860,263080,266810,269880,271510,275220,276630,281570,284350,285390,288360,289750,292400,294120,298580,302690,305490,308740,309830,314460,316150,318530,321280,323205,327500,332380,334820,338390,339290,340620,341870],"text":["","PROFESSOR: Let's see another example of an MDP.","Much like in search problems, it's very easy to think of these things in","terms of being on a grid, because that's often a great running example","that we use.","So let's take the example of a race car.","We have some robotic race car, and it would like to travel far and quickly.","So we're going to imagine that this car has three states it can be in.","Its engine can be cool, it can be getting warm, or it can be completely","overheated and break down.","So we're going to show them like this.","So cool is kind of the good state.","As you get warm, you're risking overheating.","What can the agent do?","Well, the agent has two actions.","It can drive slow or it can drive fast.","What do those actions do?","Well, slow tends to cool the agent down.","Fast tends to heat the agent up.","But there's some uncertainty involved.","So if you're cool and you drive slow, then with probability","one, you stay cool.","If you drive fast, there's a 50% chance of warming up.","Now you might say, how come I don't know whether or not I'm","going to warm up?","Why don't I instrument my car with sensors everywhere and build a complex","model of thermodynamics?","Of course, you can always try to do that.","You can always make your model of the world richer, and you can always sense","more about the world.","And when you do those things, your uncertainty reduces.","One, it's never going to go away, because as we'll see later, your model","is never going to be perfect.","And two, in many cases, that's not worth the cost.","In this very simple model, going fast simply has a 50/50 chance of","overheating.","All right.","So that's what slow and fast do from cool.","From warm, slow either keeps you the same or it has a 50/50 chance of","cooling you back down.","If you go fast, you're guaranteed to overheat.","All right, what about from overheated?","If you get to overheated, the game's done.","You break down and you drive no more.","So a smart race car will try not to overeat.","If you go faster, you get double reward.","Otherwise you would just always go slow, because why not?","So let's see what the rewards are on this.","Well, all the slow actions get a plus one.","It doesn't matter whether you get warm, right?","In and of itself, that's not part of the reward structure.","You get a plus 1 when you go slow, and a plus 2 when you go fast, except if","you overheat.","Then you get a minus 10 and the game ends.","If you think about this MDP, it's actually more important that the game","ends than what that number minus 10 is, because you basically","never want to do it.","So what do we see here?","This is a state transition diagram for this Markov decision process.","It shows the states.","In this case, there are three of them.","It shows the actions.","There are two of them.","And it shows the transition function, which says what each action does.","It also shows the rewards which are associated with a state, an action,","and a result.","Now this is the kind of thing we might have used Expectimax for.","And we can think about an MDP, any MDP, as defining a search tree.","So if you're in some particular state-- for example, if you're in the","state where the car is cool--","well, you have two actions, just like you did in an Expectimax tree.","You can go slow or fast.","If you go slow, you stay cool.","If you go fast, you might heat up.","And this continues down.","This is very much like an Expectimax tree, but we'll see very shortly why","we might not want use Expectimax to solve it.","You can already see hints of it.","What do you notice about this tree when you look at it?","It's a big tree, but it's kind of the same blue and red stuff over and over","again, right?","There's just not that many states.","So this is a case where Expectimax would do a lot of work, and","we'll see why later.","In general, any MDP is going to project these search trees.","You can think of the MVP as applying the rules of a non-deterministic game,","and from any state, this is what could happen, right?","I'm not talking about how to compute things like values.","I'm just talking about what the possible outcomes could be.","Well, you're a state.","What can happen?","There are actions.","The actions are under your control.","You get to pick one.","But there's a variety of actions.","From each action, there's an uncertain outcome.","And so we need a very important concept.","It's actually analogous to chance nodes from before, but it's kind of","confusing when you first see.","It's called a q-state.","When I'm in a state and I take an action, I end up in a q-state, which","you can think of as kind of the pair of the state and the action, where","I've committed to the action but I haven't done it yet.","So this is illustrated here by the agent having decided","to move to the right.","But since the agent hasn't actually done it, the action hasn't resolved","and we don't know the outcome.","So from a q-state, which is a commitment to an action, there's then","a variety of possible outcomes.","What are the outcomes?","Well, they're all the states, s-prime, that this action can lead","to from that state.","So these are the important concepts.","s here is a state.","s, a is a q-state, a state-action pair.","The actual resolution where we land in some s-prime--","that's called a transition.","And we're going to see these triples over and over and over again--","s, a, s-prime.","I was in state s, I took action a, and the outcome was s-prime.","That's called a transition.","It has a probability associated with it.","That's T(s, a, s-prime), the transition function.","And it has a reward associated with it.","That reward is shown here as a gem, but it could be positive or negative.","So that's kind of like an Expectimax tree except, one, the probabilities","are given to you by the transition function.","And two, the rewards, instead of being at the bottom, are kind of smeared","throughout the tree.","They come to you step by step.",""]}