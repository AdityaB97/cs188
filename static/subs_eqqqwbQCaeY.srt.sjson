{"start":[0,1380,3110,5720,6880,9880,12040,16810,21280,22970,26400,29910,33260,35970,38280,42950,44270,46050,51370,53000,54660,58000,61660,65489,66710,68600,70600,73870,74880,76880,82180,87710,91730,95880,97360,98690,99940,101930,104470,105310,108160,111260,112940,117740,120970,122350,127170,130669,132110,136270,141180,145220,147150,148610,152010,153190,158680,162950,164930,166940,168940,172320,173810,177310,178850,179580,180200,181080,182310,184300,185970,189820,191250,194520,197970,200160,203040,204380,205580,209520,211810,216380,219880,223040,227160,230120,233370,234760,237840,239895,241720,244490,247360,252710,254850,256820,260620,262370,263920,266120,267640,271540,273060,277120,278790,281220,285610,288700,290650,294380,296220,297810,301890,304920,309640,312910,315550,319020,321370,325690,327140,330030,332770,333670,337250,340870,344520,345790,348660,353160,355310,357000,359870,361090,363310,365340,370210,374640,376730,380660,381890,385660,387920,390830,394730,399060,399990,403970,405850,407560,409630,413520,415020,418100,420380,424630,427630,433540,437160,438950,443110,447210,449390,451960,452570,453690,455240,459830,461860,463110,465640,467450,469990,470480,472920,473940,476030,477490,480530,482060,483300,487130,489450,493480,496900,498850,502200,503660,505800,506880,510200,515059,517980,523179,527890,529300,531780,534410,536460,537560,538415,538700,539210,539730,540230,543890,547330,549950,551510,553220,555740,556480,560920,563230,567800,568750,570770,572700,578550,583570,586370,587890,591660,592710,597340,600130,601430,602190,605070,608260,609460,611620,612870,615990,617130,618870,623550,627550,629180,633040,635410,640080,643800,648000,649310,651410,656040,660360,662385,665890,668520,671680,675820,677030,681360,683060,688030,690980,693490,695660,699880,703200,706610,707600,712750,713830,718030,721830,726610,730730,733100,737600,739680,743120,745280,748870,752940,757140,761720,764580,766890,768510,770010,771740,774630,779380,784030,786000,786630,789550,794410,796910,799760,802390,803230,805230,808830,813240,815490,816980,819180,821910,824820,828810,832630,835920,837760,839600,844470,848470,853500,855050,858960,862790,866540,870620,872570,877420,881510,886920,889650],"end":[1380,3110,5720,6880,9880,12040,16810,21280,22970,26400,29910,33260,35970,38280,42950,44270,46050,51370,53000,54660,58000,61660,65489,66710,68600,70600,73870,74880,76880,82180,87710,91730,95880,97360,98690,99940,101930,104470,105310,108160,111260,112940,117740,120970,122350,127170,130669,132110,136270,141180,145220,147150,148610,152010,153190,158680,162950,164930,166940,168940,172320,173810,177310,178850,179580,180200,181080,182310,184300,185970,189820,191250,194520,197970,200160,203040,204380,205580,209520,211810,216380,219880,223040,227160,230120,233370,234760,237840,239895,241720,244490,247360,252710,254850,256820,260620,262370,263920,266120,267640,271540,273060,277120,278790,281220,285610,288700,290650,294380,296220,297810,301890,304920,309640,312910,315550,319020,321370,325690,327140,330030,332770,333670,337250,340870,344520,345790,348660,353160,355310,357000,359870,361090,363310,365340,370210,374640,376730,380660,381890,385660,387920,390830,394730,399060,399990,403970,405850,407560,409630,413520,415020,418100,420380,424630,427630,433540,437160,438950,443110,447210,449390,451960,452570,453690,455240,459830,461860,463110,465640,467450,469990,470480,472920,473940,476030,477490,480530,482060,483300,487130,489450,493480,496900,498850,502200,503660,505800,506880,510200,515059,517980,523179,527890,529300,531780,534410,536460,537560,538415,538700,539210,539730,540230,543890,547330,549950,551510,553220,555740,556480,560920,563230,567800,568750,570770,572700,578550,583570,586370,587890,591660,592710,597340,600130,601430,602190,605070,608260,609460,611620,612870,615990,617130,618870,623550,627550,629180,633040,635410,640080,643800,648000,649310,651410,656040,660360,662385,665890,668520,671680,675820,677030,681360,683060,688030,690980,693490,695660,699880,703200,706610,707600,712750,713830,718030,721830,726610,730730,733100,737600,739680,743120,745280,748870,752940,757140,761720,764580,766890,768510,770010,771740,774630,779380,784030,786000,786630,789550,794410,796910,799760,802390,803230,805230,808830,813240,815490,816980,819180,821910,824820,828810,832630,835920,837760,839600,844470,848470,853500,855050,858960,862790,866540,870620,872570,877420,881510,886920,889650,890933],"text":["","DAN KLEIN: All right, what are we doing?","We got estimates of the values of each state.","So we've got a table.","Each one has a number, which is our estimate of the score you will get","from that state going forward.","What we'd like to do is we'd like to make them better by replacing each one","with an average of what you'd get in the next time step plus the current","approximation in the future.","That's this Bellman update equation that's written here.","What we'd like to do is we'd like to take that average of our outcomes s","prime without knowing how likely each s prime is.","So what we're going to do is we're going to take samples of outcomes.","How can we get a sample for s prime?","How can we get a sample of what happens if you take action pi of s","from state s?","There's only one way to get that sample.","Wait until you find yourself in state s, do pi of s, and carefully watch the","result and record it.","You have to do the action.","And every time you do the action, the environment is nice enough to hand you","a sample back, which is one of the possible outcomes.","So it looks like this in the equation that we have and in the diagram that","I've been drawing.","You have one action that you know you're going to take.","And you have many s primes that can happen.","Of course, when you actually take the action, only one of","those things happens.","Let's call it s1 prime.","If I'm in state s and I take this action, pi of s, and s1 prime happens,","now I have an estimate of what my score is going to be from s.","It's going to be the reward I just got, this instantaneous reward here,","plus the discounted future from the landing state.","Well what's the reward I just got?","I just got it, right?","So I know that number.","What's the discounted future value?","It's somewhere else in the table that I'm maintaining.","And it's wrong.","But it's just been discounted by gamma, so who cares.","Now of course, one sample is not the same as an average.","I need more samples.","I need sample 2, I need sample 3, and so on all the way up to sample n.","Now what would happen if I have lots of samples from this","same state and action?","And for each of these samples I saw various outcomes s prime, each one had","a reward, and a discounted approximate future?","What would I do?","Well I'd say, really, the most informed thing I can say about the","state s is that, on average, I'm going to get the average of these samples.","So I could write down that, on average, I get 1/n times the sum of","the end samples.","We're almost there.","If we could do this, we would be, essentially, executing the policy","evaluation update.","We'd be taking that average according to T, without knowing T. So it's","correct insofar as, with enough samples, this implements that update.","There's something really wrong with this algorithm.","What's really wrong with this algorithm?","How did we get our first sample?","We got our first sample by finding ourselves in state s and","taking the action pi.","You can't get your second sample, until you're back in state s.","Well, how do you do that?","Well, you don't know.","You don't know what's going on.","This is reinforcement learning.","You don't even know what your actions do.","How are you going to get back to state s?","So you kind of can't do this.","Maybe someday you'll be in s again, but you can't just keep executing the","same action from the same state.","Maybe, if you could rewind time, you could do it.","You execute the action, rewind back, and then try again.","But you can't do that, right?","If you had a crystal ball, maybe you could do it going forward.","But you can't do that either.","So what do we need to do?","We need to somehow be satisfied with the one sample we get.","Because once we get that sample, we're off in some other state.","And who knows if we'll ever be back to state s.","So that's the big idea in temporal difference learning.","In temporal difference learning, we learn from every experience.","Every time we undergo a transition from some state s to some state s","prime, we're going to update our values.","And it's gong to be a little tricky, because we only get one sample of what","might have happened.","So we need to somehow make sure that, over time, we","accumulate the right averages.","But right now, we've got one sample.","We've got to incorporate it and move on.","The reason this is called temporal difference learning is because we","compare our current estimate of a value with what we actually see.","All right, so let's take a look at how this works.","And then we'll see it in a demo.","We're doing temporal difference learning right now of values.","So the policy is still going to be fixed.","We're still evaluating this policy.","We're still not worrying about how to choose actions.","That will come later.","The idea is we're going to move the values towards whatever we actually","see happen.","It's going to be some kind of running average, but we've got to be careful.","So here's the idea.","We imagine we were at state s.","Well, before we took any action, we had some estimate of how good it was","that encapsulates all of our knowledge to date.","So we've got some estimate V of s.","It's not correct, but it's an approximation.","From s, we take an action.","In this case, we do what pi tells us.","And we land in some particular sample outcome s prime.","So we get one of the many possible outcomes of our actions.","We get the reward that's associated with it, that's R. And then we get put","in a state s prime for which we have an estimated future","utility from s prime.","So on one hand, we think we're going to get V of s.","That's what our past experiences has told us.","On the other hand, this particular time, it looks like we're on track to","get this sample.","It looks like we're on track to get the instantaneous reward","plus V pi of s prime.","So what do we do?","We've got our old experiences all summarized in our value.","But then we have this new experience we need to incorporate, which includes","a real reward, R, and then an approximation that's been discounted","into the future.","So how do we take the average of one thing?","What we do is we keep around our old value with some weight.","And we average in the new thing with some weight.","So we interpolate.","There's going to be some value, alpha, called the learning rate.","Alpha is usually small.","You can imagine it's like 0.1 or something like","that, maybe even smaller.","And every time we get a sample of the new value, which is a reward plus a","discounted future, we average it in by interpolating it with","the old value estimate.","So we mostly keep around our old information, but we roll in a tiny bit","of this new information.","And if we ever come back to here again, we'll roll something else in.","And we'll end up with a kind of running average.","So an important question here is why we use alpha.","Why don't we keep track of every experience we've ever had?","If we really, really wanted an even average, that would be a way to","achieve it.","It turns out this is not only easier, but it's in fact better.","We'll see why in a second.","Here's one way of writing the update.","It says, I have my current estimate.","I have my sample, which alone is noisy, but which is good because it","represents an experience.","And I'm going to average them together by interpolation where most of the","weight is on the old experience.","I can also write that by just algebraic manipulation like this.","And this is another useful way of thinking about this update.","It says, take your value and add to it alpha times the difference between","what you thought would happen and what actually did.","You can think of that as an error.","And this says, whenever you see an error in your estimates, adjust your","estimate in the direction of the error by some small step size alpha.","Why not step size 1?","Well, because then we wouldn't be balancing all of our different","experiences.","So there's a trade-off.","Larger alphas learn faster.","Smaller alphas do a better job of balancing across experiences.","OK, let's see a demo of that.","Here's a grid world.","In this grid world, we are about to do temporal","difference learning of values.","Remember, that means we keep a big table of values.","And guess what?","They all start off with zero, because we have no idea what's going on in","this grid world.","The blue dot represents the agent.","And I'm going to issue commands.","And the agent will then watch what happens.","So I'm going to try to go north.","I went north.","In this version, the living reward is set to zero, so that we can see how","the exit rewards propagate.","In this case, no update happened, because I thought that I was going to","receive a total utility of zero from the start state.","I in fact received a zero.","And I landed in a state where I thought there was zero left to come.","So I thought I was going to get a zero.","I do seem to be on track to get zero.","Nothing to do.","So nothing interesting happens, until I receive a non-zero reward, which I","receive not when I enter this square here, but when I exit it.","Right now, I think I'm going to get a zero total future score.","When I take the exit action, I will in fact receive a plus 1.","And then I'll be forced to reconcile the plus 1 I got with the zero I","thought I was going to get.","And right now, alpha is set to 0.5.","And so the average value will now be 0.5.","And I update my array.","I play again.","So I go north.","Nothing happens.","Nothing happens.","Nothing happens.","Nothing happens.","All right, what happens when I move into this square?","From here, I'm about to move east.","I think I'm going to get a total score of zero.","Let's see what actually happens.","Well, I go east.","What did I receive that time step?","Zero.","But I landed in a state that looks, right now, to be worth 0.5.","So on one hand, I thought my total would be zero.","On the other hand, this experience says I'm going to get zero plus 0.5.","I average them.","And I get the 0.25.","Now that's going to continue happening.","As I exit here, that 0.5 approaches 1, because I'm going to get 1 every time.","And as I go through here, now, when I move into the next square, even though","I will receive no reward, I'll land in a square which looks","like it's worth 0.25.","And I'll get half of that as my value update.","And I'm going to execute this.","Notice that we learn about the square we leave because, when I leave this","square, I'm going to learn the my estimate of zero isn't","the best I can do.","I can do better.","I can say it's probably more like 0.13.","All right, so you learn about the state you leave, not the","state you land in.","And let me just do this a couple of times, so you can see the values","propagating back.","","All right.","Now what's the magic of this?","The magic of this is I am learning the values of these squares.","And they're the values under the policy that I'm executing.","What would happen if I went the other way around?","Well, that start state looks worse because, instead of landing in a nice","juicy 0.5, I just landed in zero.","On the other hand, when I get to here and I go north, even though I've never","been in this state before and this is my first experience with it, as soon","as I see that going north lands me at 0.98, I know that going north is","pretty good.","And so I'll have an update.","That's exactly the effect that direct evaluation didn't have.","Now, of course, if I start doing dumb things, I'll start getting values the","reflect this now dumb policy.","","So we only get values that reflect what's actually going on.","But it does average things in the appropriate way.","And it does share information across episodes in a good way.","OK, a couple of quick points.","First of all, we're taking an exponential moving average here.","And that's because we're doing this update.","We're doing an update that says the new value is alpha times some Xn plus","1 minus alpha times the old value.","And you can think of this as averaging things together.","But of course, new things have more weight.","In the extreme case, when alpha is 1, new things have all the weight.","If we actually worked out the algebra and telescoped everything, we would","actually see that the form of this tells you that most recent samples are","more important.","So the average here is an average, but exponentially weighted where older","things are decayed.","And so we had a good question before, which is, why would we not want to","take the extra effort to make sure we were averaging things equally?","And the answer is, because the things we're averaging have two components.","They've got the instantaneous reward, which is absolutely correct.","And they've got an estimate of the future.","The next time we come back to a state, the estimate of the future state will","be better than it is now.","And so, more recent samples are actually better, because they","incorporate better estimates of the future.","And so there's this nice dove-tailing of this update that's easy to do.","It also has a nice property of remembering the past, but also slowly","forgetting it and down-weighting it as our updates become better.","In essence, the Bellman updates have been smeared across space and time.","We had a question about decreasing the learning rate.","If you decrease it, then you can actually converge","to the correct things.","But there's an art to decreasing it.","It can't go down too fast or too slow.","And we'll come back to this point later.","OK, so we just saw a method of learning the values of every state","under a policy that's executing, just by watching the outcomes of every","transition.","This is great.","It's a model-free way of doing policy evaluation.","We end up doing the Bellman updates, in effect, taking those averages that","were hard to take without the transition function.","And we do it without ever computing a transition function.","We just let the sample frequencies speak for themselves.","So that's great.","It did what we wanted.","But it's not going to work for the general problem of active","reinforcement learning where we want to not only evaluate,","but also choose actions.","Why is that?","Well, let's imagine we've run this thing.","And we've got this big table of state values.","For every state, I can tell you, according to the policy we've been","running, it's worth seven points in total and so on.","The problem is, if we want to turn those values into a policy--","and in particular, we'll want to turn them into a new policy which is,","hopefully, better than the old policy--","now we're sunk.","And the reason why we're sunk is we know how to produce a policy from","values, but it involves one step of expect-a-max.","We would say the policy is whatever action achieved the largest Q-value","for that state.","But of course, those Q-values involve an average of their outcomes.","And we can't do that because, again, we don't have T and R.","The key idea here is that, if we want to be able to do action selection as","well, we should be learning not just the values as we have","been, but the Q-values.","And in fact, this is why we even have a notion of Q-values in this course.","They're critical for choosing actions in reinforcement learning.","This idea of learning Q-values makes action selection model-free as well,","because we just look at the Q-values and choose whichever one's best.",""]}