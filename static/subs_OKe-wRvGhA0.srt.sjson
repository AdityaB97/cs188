{"start":[0,1365,2120,5400,7570,11080,14850,17200,18820,23210,24850,28870,34230,38040,44120,47630,50150,51650,54710,57830,61310,66590,70660,73360,78940,83800,86010,89150,91000,95270,99090,101670,105980,112120,114580,117520,119440,121860,123750,129000,132220,133590,135510,139770,142960,146300,151030,152460,154950,157500,159950,162300,168220,173800,177480,181070,182780,186420,191270,197530,199290,200930,205760,209150,214170,217410,219090,221970,223470,223900,227430,229580,232500,236140,237730,243050,246280,249940,252640,256720,258339,260410,263700,268410,271610,274510,275900,277970,281280,283550,287860,292710,294300,297580,302770,307070,308020,308970,311530,313870,315540,321550,323600,326530,328440,333480,334620,337670,339470,342530,345560,347710,348870,351660,354250,357770,361620,365410,369010,371110,372220,376320,378410,380775,383300,386810,389080,393310,395420,401810,403600,404660,411250,417030,420670,423830,426870,432110,433360,439380,443225,445390,447240,451270,455110,456030,459680,463200,465020,467420,471420,474590,477620,479890,482350,487410,490000,493850,497640,501647,504650,507340,510670,515700,517520,519959,522760,524870,528950,530260,533640,539650,542850,544410,546570,548170,552440,554200,556100,559030,561030,562370,565140,568810,573060,578570,583350,587500,591160,595280,596420,598500,601440,609110,613910,617090,621080,623070,627070,631090,634510,637380,639960,642450,645380,647210,649610,653330,655440,657730,660260,662250,664140,666420,667540,668430,670800,672180,673740,675700,679390,682590,687280,689940,691240,694700,696070,700220,703060,706300,708820,711350,712600,715200,719400,723010,724770,728400,730540,733520,737000,739470,744310,748550,752870,755840,758400,761240,762660,765690,766420,769230,772230,775520,775800,780380,784210,787250,789650,793420,797060,800150,805100,809940,811420,814560,819410,824230,826320,827460,828520,831210,834650,838330,842090,843070,845420,846540,847780,848950,852210,853280,854960,857330,860420,863550,867090,869850,871130,872160,875020,877820,880360,881630,881970,884230,889360,894530,897510,901370,903040,906190,909630,913640,917280,920730,922740,924250,926360,931230,932870,937090,939050,940660,941500,943830,947230,948990,955370,959140,962680,968160,969730,970940,973340,974670,977620,979570,984590,990160,996310,999000,1000360,1003790,1005970,1010260,1014100,1015410,1019060,1022160,1025890,1027220,1031220,1032790,1034710,1035849,1038829,1042050,1045859,1046700,1049720,1055340,1057140,1060020,1063580,1066470,1067560,1070020,1072990,1074630,1078280,1081060,1083740,1086220,1089260,1092440,1095280,1097240,1099700,1101800,1104170,1109200,1111920,1114530,1117450,1120010,1122510,1125560,1129130,1130380,1132580,1136750,1138790,1142990,1147160,1151710,1153790],"end":[1365,2120,5400,7570,11080,14850,17200,18820,23210,24850,28870,34230,38040,44120,47630,50150,51650,54710,57830,61310,66590,70660,73360,78940,83800,86010,89150,91000,95270,99090,101670,105980,112120,114580,117520,119440,121860,123750,129000,132220,133590,135510,139770,142960,146300,151030,152460,154950,157500,159950,162300,168220,173800,177480,181070,182780,186420,191270,197530,199290,200930,205760,209150,214170,217410,219090,221970,223470,223900,227430,229580,232500,236140,237730,243050,246280,249940,252640,256720,258339,260410,263700,268410,271610,274510,275900,277970,281280,283550,287860,292710,294300,297580,302770,307070,308020,308970,311530,313870,315540,321550,323600,326530,328440,333480,334620,337670,339470,342530,345560,347710,348870,351660,354250,357770,361620,365410,369010,371110,372220,376320,378410,380775,383300,386810,389080,393310,395420,401810,403600,404660,411250,417030,420670,423830,426870,432110,433360,439380,443225,445390,447240,451270,455110,456030,459680,463200,465020,467420,471420,474590,477620,479890,482350,487410,490000,493850,497640,501647,504650,507340,510670,515700,517520,519959,522760,524870,528950,530260,533640,539650,542850,544410,546570,548170,552440,554200,556100,559030,561030,562370,565140,568810,573060,578570,583350,587500,591160,595280,596420,598500,601440,609110,613910,617090,621080,623070,627070,631090,634510,637380,639960,642450,645380,647210,649610,653330,655440,657730,660260,662250,664140,666420,667540,668430,670800,672180,673740,675700,679390,682590,687280,689940,691240,694700,696070,700220,703060,706300,708820,711350,712600,715200,719400,723010,724770,728400,730540,733520,737000,739470,744310,748550,752870,755840,758400,761240,762660,765690,766420,769230,772230,775520,775800,780380,784210,787250,789650,793420,797060,800150,805100,809940,811420,814560,819410,824230,826320,827460,828520,831210,834650,838330,842090,843070,845420,846540,847780,848950,852210,853280,854960,857330,860420,863550,867090,869850,871130,872160,875020,877820,880360,881630,881970,884230,889360,894530,897510,901370,903040,906190,909630,913640,917280,920730,922740,924250,926360,931230,932870,937090,939050,940660,941500,943830,947230,948990,955370,959140,962680,968160,969730,970940,973340,974670,977620,979570,984590,990160,996310,999000,1000360,1003790,1005970,1010260,1014100,1015410,1019060,1022160,1025890,1027220,1031220,1032790,1034710,1035849,1038829,1042050,1045859,1046700,1049720,1055340,1057140,1060020,1063580,1066470,1067560,1070020,1072990,1074630,1078280,1081060,1083740,1086220,1089260,1092440,1095280,1097240,1099700,1101800,1104170,1109200,1111920,1114530,1117450,1120010,1122510,1125560,1129130,1130380,1132580,1136750,1138790,1142990,1147160,1151710,1153790,1155040],"text":["","PROFESSOR: All right.","Now that we know what Markov decision processes are, we can talk about how","to solve them.","For solving MDPs, there's a bunch of important quantities we need to define","precisely so that we can come up with algorithms that compute them.","Some of these quantities you already know, or you know something","very much like them.","So the first one is the notion of the utility or the value of a state s.","So you actually already know this.","This is just what would happen from that state if you ran Expectimax.","And in particular, to define it, we can write V* of s.","And what the star means is this is the value under optimal action.","V* of s is the expected utility starting in s and acting optimally.","The idea here is, if you act optimally from s, sometimes you'll get big","rewards and sometimes you'll get small rewards, because you don't actually","control your outcomes.","But on average, V* s is the best you can do.","That is actually what Expectimax computed.","So states have values which are their Expectimax utilities.","Remember a q-state, s, a, is being in the situation where you are in state s","and you are committed to action a, but you haven't done it yet.","So anything could still happen.","The value or utility of a q-state is written Q* of s comma a.","That is the expected utility from that q-state if you act","optimally in the future.","Now if we think about this in the tree, the values are","associated with states.","So we can write the value of a state and say, that's what would happen if","you ran Expectimax from a state, meaning you started out by picking the","best action and then recursed.","If, however, you run Expectimax starting at a chance node, which here","are q-states, you would still get some optimal thing, but it would be the","best you can do starting at that chance node.","What does it mean to start at a q-state or a chance node?","It means you are already taking that action.","It's too late to decide a wasn't the right thing.","Whatever is going to happen is going to happen.","And then from s-prime forward, you will behave optimally.","The third important quantity is pi*.","Pi is a policy.","Pi* is an optimal policy.","And pi* from s is the optimal action from state s.","Let's take a look at what these values actually are.","So here we see that same grid.","And now what's also shown are numbers in the squares and arrows.","The arrows are easy.","The arrows are the policy that has been computed.","And in this case, it's the optimal policy.","Now in order to know whether we actually got it right, I need to tell","you which setting of grid world this is.","And in this case, this is a setting where the living reward is minus 0.1.","And there's a small decay or discount of 0.99.","So we can see the values of these states, because the only thing that","can happen in these states is that you choose exit, and then you get the","reward deterministically.","These states have values of 1 and minus 1.","If you're down here in the lower left, the policy suggests that you, over","time, make your way up and then over, and then take the good exit.","This 0.49--","what's that?","That is, if you start in this state and you ran this game over and over","again, and sometimes you slipped and sometimes you didn't, and you added up","all of those utilities, on average you would get 0.49.","And you would achieve it by trying to go north whenever you","were in this state.","So you can see it's better to be right here by the exit than it","is to be over here.","Why?","Because if you're over here, you have to pay that living reward and some","discount to even get to the exit.","The best thing is actually be in the exit.","Similarly, you can see, if you're essentially two steps--","assuming nothing goes wrong--","from the exit, it's better to be up here at 0.74 than down here at 0.57.","Because when something goes wrong at 0.74, it's not a big deal.","When something goes wrong at 0.57, you fall into the pit and lose.","So that's what the utilities look like, and also the","policies on this grid.","So these are utilities.","These are the values of the states.","Let's in particular remember 0.57.","0.57 is the value of being in the state, assuming you are smart in the","future, which means in particular from the state going north.","","What's this?","This is the q-values.","From each state, except for the exit states, you've got","four choices of action.","If you are in this state where you're between the wall and the pit, and the","action you've committed to is north, then you get that same 0.57.","That's where the value comes from.","It comes from going north and then acting optimally.","If, however, you're in the q-state of being in this square and committing to","going right, your q-value is a lot lower, because probably you're going","to fall into the pit.","Now, you might not.","And because of the definition of a q-value, if you don't, you will then","act optimally in the future, which will involve not","trying for the pit again.","But the point is each one of these states has four q-values, but only one","value-- and unsurprisingly, the value is going to be","the max of the q-values.","So we want to be able to compute these things.","We'd like to be able to take an MDP and compute these Expectimax values","for a state.","And actually, what we usually do with these algorithms is we compute the","values for all of the states.","And we'll see that there are ways to save time by doing all the states at","once, provided your MDP is small enough that you can actually go","through all the states.","What are we computing?","We're computing the expecting utility under optimal actions.","And that's going to be a sum of discounted rewards.","Because this is what Expectimax computed, we can write down a","mathematical definition of this thing we're trying to compute that basically","parallels Expectimax, written out in equations.","So where we used to have a recursive function, now we're just going to have","equations that mention each other.","So let's take a look.","Well, from a state, we would like to know, what is the","optimal value of a state?","That's Expectimax from the root of the tree.","Well, what is it?","I'm not really sure, but I've got all these actions I can choose from.","And I should maximize over them.","And I can write that as an equation, as a max over the actions","available to me.","And for each action, if I go down in the tree, I end up at a q-state.","And so I can essentially recurse.","What does that mean in an equation?","Well, I just write down the quantity that's below, which is the value of a","state is the max over all the actions available to you of the q-values, of","the chance nodes you would get if you committed to that action.","That's just what the max function of Expectimax did.","It's a max over all the chance nodes.","The tricky one, and the one that's actually changed, is what do I do at a","chance node?","So in this case, I'm here at s comma a.","And I want to know how many points I'm going to get under optimal action.","It kind of depends what happens.","But I know what's going to happen.","Whatever state, s-prime, I land in, I'm going to get some reward.","The reward is going to depend on s and a, but also on whether or not the","outcome was good.","So the reward depends on the whole transition.","That's going to be my one-time step of reward.","But of course I'm going to keep getting rewards from","s-prime when I land.","So I'm also going to get all of the future rewards from s-prime.","But that's just a value.","So again, you see the mutual recursion.","Now there's something missing here.","There's actually a couple things missing here.","One thing that's missing is I don't know which s-prime is going to happen.","And so at a chance node, what do I do?","What I do with all of these different possible outcomes, s-prime?","I have to consider them all and I have to take a weighted average.","And the way I write that in math is I write that I'm going to sum up over","all s-primes.","And they each get a weight.","And that weight is given by the transition function.","And it is the probability of s-prime, given s and a, which we write as T of","s, a, s-prime.","So let's look at the pieces and make sure we understand.","We're going to get a reward in the next timestep, and","then a future reward.","They're going to be weighted by the relative probabilities that come from","our transition function.","And we're missing one small but important thing.","And that is V* is the score we would get from s-prime.","But it's happening one timestep in the future.","So it's worth less to me.","How much less?","Gamma less.","And now we have the recursive expression for Q*.","And these are the pieces.","You have a reward and a future value.","You have a discount on the future value.","And you have probabilities that average together","the different outcomes.","So this expression may look complicated, but you're going to be","seeing this so many times in so many variations that it's important to make","sure you don't go into math shock and you actually process it.","To that end, I am going to make it appear in beautiful LaTeX.","Even though your code alternates between maximizing in one function and","taking an average in another function, it's often the case that","mathematically we essentially inline q.","And we write the values V* in terms of other values.","Let's try to do that.","Let's try to do it in one step.","I'm basically going to write the exact same Expectimax recurrence.","I'm going to say, well, from here I would get a reward on my transition,","and then a discounted future value from wherever I land in s-prime.","","And this would all depend on which action I chose.","And I should choose the best one.","And it would depend on the outcome of that action.","And so I need to take a weighted average according to T.","So this is an expression of values in terms of values.","As a function, it would be mutually recursive.","As an equation, it's a system of equations.","And because there's a max in there, it's non-linear.","We'll come back to this equation many times.","It's in fact what's called a Bellman equation.","We'll be seeing this a lot starting next lecture.","","So here's that racing search tree.","And remember, we looked at this and we said, all right, we just wrote some","equations, but they basically said to run Expectimax, right?","You want the thing at the root, so you take a max.","You want a chance node, so you take an average.","Yeah, there was some discounting thrown in, but I can do that with","Expectimax.","OK, fine.","But the racing tree is shown here for two steps.","What happens if we go further?","Well, here, it looks like this.","It gets very big very quickly.","But if I look at this, I might think that Expectimax is somehow a colossal","waste of time for this problem.","Why is Expectimax such a colossal waste of time?","Basically, we're doing way too much work with Expectimax.","And there's two reasons.","One reason why Expectimax is wasting a lot of time is","that states are repeated.","So you look at this, you see that blue car that's in the cool state over and","over and over, and the red car over and over and over.","And somehow, we shouldn't be computing that value over and over and over,","because it's always going to be the same.","And this should remind you, basically, of graph search.","We don't want to do the work again.","","In addition to having lots of repeated states, the tree also has the problem","that it's infinite, and it's hard to compute infinite things.","So this tree goes on forever.","And we already have an idea for dealing with infinite depth, and that","is, do something depth-limited.","But we need to be a little bit careful, because we don't necessarily","know how deep we want to go until we've done it.","But the basic idea is we're going to do some kind of depth-limited","computation, and we'll increase the depth until the change is small, and","then we'll say, OK, we're close enough, and we'll declare victory.","Now the reason why that might work is because, if we have a discount, gamma,","then as you go deeper into the tree, you're seeing those same rewards-- the","plus 1's, the plus 2's, the minus 10's--","except as they get buried deeper and deeper in the tree, they start picking","up factors of gamma.","And if you go 100 steps into the tree, they've got a gamma to the 100 in","front of them.","And so if you go deep enough into the tree, even though the tree goes on","forever, eventually it stops contributing very much, because it's","out of the agent's horizon in a soft way.","All right.","So how can we not do Expectimax in a way that does all this work over and","over again, and then never terminates because the tree is infinite?","The key idea here is we'll have time-limited values, which you can","think of as a stopwatch.","Even though this game might in fact go on forever, we've got a stopwatch","that's counting down the number of timesteps remaining until, as far as","the agent's concerned, the end of days.","And you've got k steps remaining, and then the game just stops.","So we can define here not V*, which would be some optimal thing letting","the game run forever.","We can define something much more concrete and easy to think about,","which is V sub k, the time-limited value of a state.","V sub k is the optimal value of s if the game ends after","exactly k more timesteps.","What's a timestep?","It's a reward.","So exactly k more rewards.","This is actually kind of nice, because this is exactly what a depth-k","Expectimax would give if we started it from s.","So if you think about this tree here, this is not the whole tree from the","blue states.","However, this is a depth-2 tree.","There's two rewards.","Where do the rewards come?","This is actually pretty important.","The rewards come here when this chance layer resolves, and they come here","when this chance layer resolves.","That's where the transitions are.","OK, this is V sub 2.","If I ran this Expectimax and I truncated it after two","steps, I get V sub 2.","And because we're just truncating and we're just ending the game, we don't","need an evaluation function, because we don't have to talk about what","happens after the game ends.","Nothing happens.","There are no rewards after two steps.","So let's take a look at what these V sub k's actually are.","Then we'll figure out how to compute them, and that's going to give us an","important algorithm.","All right.","So let's look here.","What this shows right now is, from each of the states, what you can","achieve under optimal play if you are allowed to take zero more actions.","Well, if there are zero more timesteps, I'm not getting any more","rewards, which means the pit, the gem, the wall, whatever--","it's all zero.","What happens if I can take one more step?","Well, what happens if I'm in the good exit?","Well, I have time to take exit and get my plus 1.","If I'm in the bad exit, I have time to take exit and get my minus 1.","And from anywhere else, what can I achieve?","Well, it depends on the living reward, but let's imagine the","living reward is zero.","I can achieve zero.","If I switch to V sub 1, then you can see here that we have a plus 1, a","minus 1, and a whole bunch of zeros.","So you can see that from this ledge, we get a zero.","Because in one timestep, what can I do?","I can move north, I can jump into the pit.","It doesn't matter.","I can't get any of the plus 1's or minus 1's.","Even here, where I have time to get to the good exit, I don't have time to","leave and get my reward.","If I'm allowed two timesteps, you can see that from this square, I have time","with some probability to get into the good exit and leave","and receive my reward.","From here, the ledge, I have time to do various stupid things, like go into","the pit and receive a negative.","But that's not the optimal thing.","The optimal thing is anything else.","And so I have a zero.","But if I'm allowed three steps, I can get some rewards even from there.","And four steps, and five steps.","How many steps would it take for this start state, if the lower left is the","start state, to have a value V sub k which is greater than zero?","Well the shortest way I could possibly get to a positive reward is one, two,","three, four, five, and then six to take the exit.","So I look at V sub 6.","I can see that I have now 0.34 here.","Now what happens if I allow seven steps?","Is this 0.34 going to go up or is it going to stay the same?","Well, if I'm allowed seven steps, I can not only get there in the lucky","way where nothing goes wrong.","I can also get there in various ways where something goes wrong once.","So in fact, if I go to depth 7, that will increase.","And if I go all the way to depth 100, then I start to see","the particular values.","And now at depth 100, because of the specific settings of this problem,","most of the states are pretty good.","It's pretty much, as long as you're not in the pit, you get","a reasonable value.","And of course, these actual numbers and how quickly they get smaller as","you go away from the reward will depend on the specific probabilities","involved and discounts and so on, in the instance of grid world.","All right.","So we saw what these V sub k's are.","They are your Expectimax value under optimal play if the game","ends after k steps.","Now we can go back and we can look at this big Expectimax tree, and we can","think about how V sub k relates to this tree.","Let's imagine that in fact this is the whole tree and it","doesn't go down to infinity.","It's hard to fit infinite trees onto your PowerPoint.","So if this is the whole tree and we looked at the bottom, we would say,","all right, what is at the bottom?","Well, it's a huge number of states I would have to check in Expectimax.","But there's kind of only three things down there, right?","Because there's only three states in this MDP.","And for each one of these, it's a terminal state.","Nothing else happens, so no more rewards.","And that means that I've got, essentially, lots and lots of copies","of V0, lots of depth-zero Expectimax.","In fact, we know what V0 is.","It's zero for any state.","Now what's in this layer up here?","Well, if I look, again we've got a bunch of repetitions.","But each one of these computations is a depth-one Expectimax computation.","Which means, in fact, all we have is a whole bunch of copies","of V1 in this layer.","And if we go up, we've got a whole bunch of copies of V2.","And then a whole bunch of copies of V3.","Except now we're actually close enough to the top that it's not that many.","And then at the very top, we're only computing one value.","But conceptually this top layer is well-represented by the","collection of V4's.","","So do you see how, at the bottom, even though the tree has grown immensely,","it's still only three values?","And each layer is V sub k for a larger k.","So this actually gives us an idea of how we can compute these values in an","efficient way, where we don't get the explosion in depth, because you can","see at the bottom it's no worse than at the top.",""]}