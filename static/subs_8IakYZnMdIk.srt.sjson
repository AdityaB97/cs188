{"start":[0,1281,3500,6770,8380,11500,14870,17870,19670,22460,23360,27530,29230,30950,34630,37240,40670,44270,46710,48030,51320,54010,58260,61540,62800,66790,70100,71430,73680,76180,80500,84920,86780,89230,92230,95570,98470,101920,104860,108530,109930,111210,113760,117400,121620,125470,128039,129310,132180,133510,135290,136080,137340,141100,144960,148210,149730,153100,155290,158170,161140,165130,165878,166714,167550,169145,169570,170420,173050,174130,175240,176160,176770,178930,180840,182630,187890,189990,191680,194560,195610,198690,200590,206030,207280,210390,211880,217460,218540,220290,222000,223390,229160,231620,234850,235640,239100,240240,245122,254086,254900,257100,258200,260730,262660,266490,270800,274920,277460,280870,282210,283430,285630,288110,291600,294990,298040,301670,303080,304280,306970,310540,311960,314000,315810,318670,320700,323490,326270,327610,330940,333000,336170,340110,341870,344830,348450,351680,354050,355490,358470,360880,364490,368610,370650,373430,377450,380040,383050,385480,388810,391990,393020,395850,399680,403310,405750,407320,409010,413650,417060,420430,421550,422670,424740,427600,431170,432660,436060,437210,438290,439145,440540,443130,443730,445520,448760,450790,455180,457950,460060,462880,466100,470030,471850,474700,479340,482430,487080,490370,492930,495170,497850,503040,504780,508010,510840,514990,518370,521919,525650,526820,530640,531670,534060,537970,540900,543620,544730,547200,550160,551180,552180,554100,556470,559140,560580,561800,565760,567680,570010,572920,575940,579210,581930,586600,588040,592980,596740,600370,602720,605140,608760,611980,615750,620580,622430,624450,627370,631960,633790,635450,639410,642750,646900,652130,655820,657200,660720,664763,666560,668210,670300,672300,674590,677120,680480,683690,686840,691620,695850,697880,702400,704940,707670,710130,712390,713450,715720,718980,722500,723980,725890,729220,731800,736070,740830,745450,746780,750130,751970,754250,755320,758390,759790,762150,764760,767300,770330,772440,775920,778960,781790,782750,785940,790400,792160,793130,796152,797070,799250,811450,820570,823830,826800,829010,830760,835700,838940,841040,845000,847380,851880,853040,856260,858850,861950,863790,867520,869930,872790,874870,876290,878590,882070,883420,884820,887620,890390,892610,896600,899590,902370,904425,905510,908810,912300,916690,920360,921590,924590,925570,928490,929380,932250,933240,934830,937470,941650,943710,946070,948300,951570,954350,956170,959120,961610,963940,967820,968950,971310,974320,975330,980300,983070,985900,986760,988500,991060,992160,995540,998230,1000990,1005070,1008690,1012480,1014600,1016390,1020600,1024000,1025770,1029550,1031720,1033180,1034720,1038200,1039329,1042579,1045170,1049780,1053190,1056450,1059200,1061160,1062990,1065280,1066990,1068880,1073040,1076200,1079360,1081730,1083640,1085130,1087020,1090310,1091990,1095010,1097640,1099590,1100490,1103510,1105080,1105970,1107590,1108970,1111990,1115840,1117690,1118930,1123000,1127100,1129100],"end":[1281,3500,6770,8380,11500,14870,17870,19670,22460,23360,27530,29230,30950,34630,37240,40670,44270,46710,48030,51320,54010,58260,61540,62800,66790,70100,71430,73680,76180,80500,84920,86780,89230,92230,95570,98470,101920,104860,108530,109930,111210,113760,117400,121620,125470,128039,129310,132180,133510,135290,136080,137340,141100,144960,148210,149730,153100,155290,158170,161140,165130,165878,166714,167550,169145,169570,170420,173050,174130,175240,176160,176770,178930,180840,182630,187890,189990,191680,194560,195610,198690,200590,206030,207280,210390,211880,217460,218540,220290,222000,223390,229160,231620,234850,235640,239100,240240,245122,254086,254900,257100,258200,260730,262660,266490,270800,274920,277460,280870,282210,283430,285630,288110,291600,294990,298040,301670,303080,304280,306970,310540,311960,314000,315810,318670,320700,323490,326270,327610,330940,333000,336170,340110,341870,344830,348450,351680,354050,355490,358470,360880,364490,368610,370650,373430,377450,380040,383050,385480,388810,391990,393020,395850,399680,403310,405750,407320,409010,413650,417060,420430,421550,422670,424740,427600,431170,432660,436060,437210,438290,439145,440540,443130,443730,445520,448760,450790,455180,457950,460060,462880,466100,470030,471850,474700,479340,482430,487080,490370,492930,495170,497850,503040,504780,508010,510840,514990,518370,521919,525650,526820,530640,531670,534060,537970,540900,543620,544730,547200,550160,551180,552180,554100,556470,559140,560580,561800,565760,567680,570010,572920,575940,579210,581930,586600,588040,592980,596740,600370,602720,605140,608760,611980,615750,620580,622430,624450,627370,631960,633790,635450,639410,642750,646900,652130,655820,657200,660720,664763,666560,668210,670300,672300,674590,677120,680480,683690,686840,691620,695850,697880,702400,704940,707670,710130,712390,713450,715720,718980,722500,723980,725890,729220,731800,736070,740830,745450,746780,750130,751970,754250,755320,758390,759790,762150,764760,767300,770330,772440,775920,778960,781790,782750,785940,790400,792160,793130,796152,797070,799250,811450,820570,823830,826800,829010,830760,835700,838940,841040,845000,847380,851880,853040,856260,858850,861950,863790,867520,869930,872790,874870,876290,878590,882070,883420,884820,887620,890390,892610,896600,899590,902370,904425,905510,908810,912300,916690,920360,921590,924590,925570,928490,929380,932250,933240,934830,937470,941650,943710,946070,948300,951570,954350,956170,959120,961610,963940,967820,968950,971310,974320,975330,980300,983070,985900,986760,988500,991060,992160,995540,998230,1000990,1005070,1008690,1012480,1014600,1016390,1020600,1024000,1025770,1029550,1031720,1033180,1034720,1038200,1039329,1042579,1045170,1049780,1053190,1056450,1059200,1061160,1062990,1065280,1066990,1068880,1073040,1076200,1079360,1081730,1083640,1085130,1087020,1090310,1091990,1095010,1097640,1099590,1100490,1103510,1105080,1105970,1107590,1108970,1111990,1115840,1117690,1118930,1123000,1127100,1129100,1131100],"text":["","DAN KLEIN: Now we're going to think about the problem of what you do in a","game like Pacman where there are so many states that you can't","learn about each one.","The basic idea we're going to have here is called approximate Q-Learning","and boils down to the fact that, when you learn that a ghost is scary","through one experience, you should transfer that to all other states that","are similar.","So we want to be able to generalize across states because there's just so","many of them.","In basic Q-Learning, if you think about the algorithm, it keeps a table","of all of the Q-values.","That looks something like this.","You have your agent and, for every cell of grid world and for every","action, you have a number that you're storing.","That's fine when there are too many Q-states.","In any kind of realistic situation, this won't actually work.","And the reason it won't work is because we can't learn about every","single state.","There are multiple reasons why we can't learn about every single state.","There are too many of them to visit them all in training.","You just can't get yourself in every configuration of Pacman.","In addition, even if you could, there's too many states even hold the","Q-values in memory.","So even if you wanted to do exhaustive Q-Learning, it's not an option.","In general, for a game like Pacman, if you had the Q-tables, it would just be","this infinite library.","And that's not going to work.","So what we want to do is we want to generalize.","We'd like to be able to take some small number of training examples, our","experiences in some small number of games, and generalize them to similar","situations.","This is actually a fundamental idea in machine learning.","We'll see it over and over again in this course.","You actually want to be able to generalize not just to save on time","and storage, but also because it's just better.","You're going to learn faster, if you don't have to repeat the lessons in","every similar state.","For example, let's imagine that we discover, through experience, that","this state is bad.","This is not a good state.","And in fact, we're going to discover that very soon.","So we learn that this state is bad with our reinforcement learning magic.","In basic Q-Learning, we know nothing at all about this state, even though,","as a human, you look at that and think, yeah, that's probably bad too.","In fact, it's kind of bad in the same way.","But it's a different state.","And so you know nothing about it in naive Q-Learning.","It's actually even worse than that.","You also know nothing about this state.","What's wrong with this state?","It's missing a dot.","It is a totally different state, as far as Q-Learning is concerned.","Something's wrong here, if you've got to learn about the ghosts in every","configuration of not only ghost, but also dot.","Let's look at what this would do.","So what happens when we run reinforcement learning in Pacman?","So this is going to be a very small Pacman board.","And what you're going to see is you're going to see a bunch of states fly by.","Each state is going to have a Q-value for each action.","And we're going to slowly learn, so here we go.","Tiny little board.","We died.","We died.","Oh, we won.","We died.","We died.","You mostly die.","Why?","Because you don't know what's going on.","You're like, I've never seen this state.","Let's go left.","And meanwhile, the ghost is like yum.","[LAUGHTER]","DAN KLEIN: Every now and then you accidentally eat the dot.","","You lose a lot at Pacman this way.","The regret is very high.","Now what I'm going to do is I'm going to let it run 2,000 times before we","see anything.","After 2,000 times, you've seen all these states before.","You know exactly which ones are good or bad.","And now you can do some amazing gymnastics and dodge the ghost.","And in fact, that was a win.","","So you win, and you win, and you win, and you win.","","So that's great.","Q-Learning on Pacman.","Or does it?","What about this?","This board isn't much bigger, but it just takes a long time before you find","your way to that dot.","And the ghost just isn't going to let you do it.","And what are you learning?","You're like, oh, in square 2, comma 3, it's kind of bad to","run into the ghost.","Oh, in square 1, comma 2, maybe I shouldn't run into the ghost.","[LAUGHTER]","DAN KLEIN: Yeah.","So that's basically not going to work.","So what's the solution?","You actually already know the solution, because you already","implement it in project two.","The solution is to take a state and, rather than thinking about it as its","own black box whose Q-values values are special and unlike any other","state's Q-values, instead, what we say is, really, states boil down to a","small number of properties, which we will call features.","So we describe a state using a vector of features, just like we did for","evaluation functions.","So what are features?","They take a state and they return a real number.","Sometimes it's 0/1, indicating something.","And sometimes it's a number indicating something that's a real number.","And so, for example, we might have the distance to the closest ghost, or the","reciprocal of the distance to the closest ghost, or the number of","ghosts, or 1 over the squared distance to a dot.","Is Pacman in a tunnel?","That might be a 0/1 thing.","So you could learn that being in a tunnel is dangerous.","You could conceivably have a feature which says, is it the exact state","that's shown on this slide?","Of course, if you did that, you wouldn't have 10 features.","You'd have 10 billion features.","And then it would be a lot harder to learn, even though you're using a","feature-based representation.","In your project twos, the thing you described with","features was a state value.","So you described a state.","You computed a value for that state by taking your features and doing some","weighted linear combination.","Of course, now we're going to have to describe Q-states, which gives us","features like, am I moving towards the ghost with this action?","This gives you linear value functions.","In project two, this was your linear value function.","It said, the value of a state is some weighted combination of the features.","And at the time we said, you come up with the features.","And once you have the features, you should fiddle around with the weights","until it basically works.","Now, you're going to still come up with the features, but the learning","algorithm is going to fiddle around with the weights for you.","For reinforcement learning, we have Q-values and not values.","And so what we have is Q-values that are weighted linear combinations of","functions over state action pairs.","The advantage of having a linear representation of your feature","function is that your experience is now summed up with a few powerful","numbers, like ghosts are bad.","The disadvantage is you can have states which are very similar in terms","of their features, but are very different in value.","For example, if there are two ghosts close to you, it matters a lot whether","they're on either side of you or both on the same side, because in one case","you're pinned and you're a goner.","And in the other case, you can kite them around.","Your job is to come up with features that make sure that important","differences in value are reflected in differences in features, so the","learning algorithm can do its job.","So how does approximate Q-Learning work?","Well, here's the magic equation.","This equation tells us, on the basis of the features of a state action pair","and some weights which are, for the moment, known and fixed so we're going","to update them, it lets us compute the Q-value.","So I give you a state.","You compute this function.","And out pops the Q-value.","How are we going to do Q-Learning with these Q-functions?","Well, the first part of the Q-Learning algorithm doesn't actually care where","the Q-value came from.","It says, give me a transition so that I can learn.","And you say, all right.","I was in state, s.","I chose action, a.","And I received reward, r.","And I landed in outcome s prime.","OK.","Well, I take a difference.","I say all right, I thought I was going to get Q of s, a.","That was my old guess on one hand.","And on the other hand, I now think I'm going to get this reward plus my","estimate of the value of the landing state, which is a","max over its actions.","So you compute this kind of error term, the difference between what you","thought you were going to get and what you actually seem to be about to get,","on the basis of this one step ahead experience.","Now what's the update?","For an exact Q-Learner, the update looks like this.","This is just an algebraic rewrite of the update that says, take alpha of","one and 1 minus alpha of the other.","So what we basically do is we keep our Q-value around, but we nudge it in the","direction of this difference, the difference between what we thought we","would get and what we appear to be getting.","So if we appear to be getting something a lot higher than we","thought, well, we should raise our estimate.","Now if your Q-values are a big table, you simply look up this entry, s, a.","You see it's 9.3.","You cross out the 9.3, and you replace it with 9.8.","It's really easy to increment a table.","The problem is now the only knobs we have are the weights.","You can't increment a single value in the Q-function.","But what you can do is you can say, all right, my Q-value wasn't high","enough for this state, so what I need to do is change the weights so that it","will be higher.","And the way we do that is, instead of increasing the Q-value directly, we","increase the weights.","But which weights should we increase?","You increase all of them, but in proportion to the feature value.","So if a certain feature is off, we don't change its weight.","If it's negative, we're actually going to decrease its weight, because of the","sign change.","And then, features that fire more strongly have a bigger","effect on the update.","So this is the difference.","It's the same idea.","You compute how wrong you were.","And then you try to make that error less.","Except now, we are tweaking the weights, rather than","the Q-values directly.","That looks like this.","That looks like you've got these sliders, like how bad is a ghost?","And whenever you're near a ghost and something bad","happens, blame the ghost.","And so you write down ghosts are a little worse than they were before.","The intuitive interpretation is we're adjusting these weights so that, if","something bad happens, all of the features that are active get a","penalty and so on.","OK, let's see an example of Q-Learning with feature-based approximations","being applied to Pacman.","So here's a very simplistic version of a Q-function for Pacman.","It says, in a given state, s, and for a given action, a, I'm going to","compute how close will I be to the dot, if I take action a.","And then I'll take the reciprocal of that distance.","That's fDOT, the DOT feature.","So a big number means I'm basically getting close to a dot.","There's also a GST feature which computes the same thing for the ghost.","So as I get closer to the ghost, f sub GST gets bigger.","And we've learned a Q-function that says multiply the DOT feature by 4.","So basically, dots are very good.","But subtract off the GST feature.","So a state that has ghosts nearby is very bad.","And let's see what happens, if we have an experience with this Q-function.","Well, here's a state.","It's not the best state to be in.","And here, fDot is 0.5, because the closest dot is two away.","And fGST might be 1.0, because the ghosts are one away.","When we plug these features into the Q-function with our current values of","4 and 1, we compute that our approximate Q is plus 1 for this","Q-state, so for this state and for the action NORTH.","What does that mean?","That means that, according to our approximate Q-value, we think our","score for the game from this point forward is going to be plus 1.","Well, what happens?","Well, reality happens.","You move NORTH, you die.","You receive a negative 500 reward.","And you end up in a state where the game is over.","And therefore, the Q-values are all 0, by definition.","So now we think hard and we say, well, I used to think, before I had this","experience, that this state on the left was worth plus 1.","Apparently, that isn't what happened this time.","This time, it looks like I'm on track for the negative 500 I received plus a","future discounted reward of 0 when the game ended.","All right, so I compare these two things.","And I say, well, it looks like I overestimated by 501.","So my difference here is negative 501.","And that means I should probably lower the Q-value.","Now remember, I can't lower the Q-value directly.","I have to lower it via the weights.","And so I do this update.","The weights mostly stay the same.","But I move them in the appropriate direction by an amount that is","proportional to my error, the learning rate, alpha--","that's a step size here--","and deactivation of the feature.","So the GST feature was more active here than the DOT feature, so it's","going to receive a larger update.","When I execute these updates, I end up with a new Q-function which has the","same functional form, uses the same features, but has different weights.","We like dots slightly less, because there was a dot relatively nearby at","our tragic demise.","And we really don't like ghosts now, because they were right there when all","the bad stuff happened.","Now we'll continue acting, but it seems reasonable that","this is what we learn.","We still like the dots, but now we're more scared of ghosts.","It seems like a good outcome.","Let's see what happens in practice.","What's nice about this is you learn so quickly.","From even one experience, you can learn that ghosts are bad.","So what you're going to see is you're going to see a game of Pacman in a","reasonable board with ghosts.","And what you should realize is, the first time you eat a dot, you get a","feedback that lets you learn that maybe dots are good.","And the first time you hit a ghost, you have an opportunity to learn that","ghosts are bad.","So instead of that kind of error and error and error and finally, after","2,000 tries, we master a 2x2 board, let's see what happens.","So mm, dots are good.","Ghosts are bad.","OK, dots are good.","Ghosts are bad.","You see it tried to run away.","And now, pretty good.","","Here, it's not eating the power pellets.","If it ate them, it would have a chance to learn that they're good, if the","feature functions allow that to be represented.","In this case, those features aren't present.","","What we're going to do now is we're going to take a quick look and see why","this update makes sense.","I told you this intuitive explanation, which was look at your error and","adjust the weight so that the error gets smaller.","In fact, we can use that idea to formally justify this approximate","Q-Learning update.","And the way we do that is we think back to a more general","case of least squares.","So in general, we might want to do some kind of linear approximation.","In particular, we have some feature vector.","Maybe we've only got one feature, f1 of our input x.","And we can have a prediction function which is linear.","In this case, there's a w0, which is an intercept term.","That will correspond to what's called a Bias feature that's","always equal to 1.","And here we have one real feature.","And in a linear function, I always have a prediction that's a linear","function of my input.","So the prediction here is y hat.","This can happen in multiple dimensions as well.","The dimension gets higher, as I have more features.","So this is a linear prediction function.","How do we figure out what weights we should use in this simple case where","we simply have inputs, outputs and some data points?","No Q-Learning, no robots, none of that.","Well, we say all right, we've got these data points that","are shown in red.","I've got some predictor, which is determined by the weights.","And my predictor makes mistakes, in particular, on a given data point.","Our actual value is y, but we predict y hat on the basis of our weights.","Now better weights will get the y's and the y hats closer.","Formally, we could write this down.","We could say the total error in our predictor is a sum over","all the data points.","For each data point, we look at the difference between the prediction and","the target.","And we square it, because being negative is bad, and being positive is","bad as well.","So that's our total error.","Remember that our predictor has this linear form.","For us, this linear form is with respect to the features of x.","And that's why you see these feature functions here that you wouldn't","normally see in a linear regression.","So we have this expression that is our total error.","And we should move the weights around to make this error small.","This is just how you drive linear regression.","Well, let's think about a particular case.","Let's say we wanted to minimize the error on one data point.","So we imagine we have one point x.","And this point x has some features f of x.","It's got some target value y that we'd like the linear function to","approximate.","And we've got some weights w.","So we can write out the error on just this one point.","So what's the error?","It's the difference between the target and the approximation.","We square it, because negative and positive are both bad.","And we stick a 1/2 there, because we think ahead to how the calculus is","going to work out.","But 1/2 doesn't do anything.","So what we could do, if we wanted to make this error small, is we could","pull out our calculus.","And we could take a derivative with respect to a certain weight, like w","sub m for the m feature vector.","And if we did that, we'd chug through the calculus and we'd get the","expression that says the derivative of the error is this expression.","So to make the error big, you want to go in the direction of the feature","vectors that are contributing to this mistake.","What you really want to do is make the error small.","So you go in the other direction.","And really, what that means is, for a small step size, alpha, you take your","weight and you take a step in the direction away from the derivative of","the error with respect to the weight.","And that is exactly our online Q-update, which corresponds to","fiddling this line up and down.","Now how does this work in Q-Learning?","Well, the weights are the weights.","The target that you're trying to reach is your experience from a","one-step-look-ahead.","And your prediction is your linear function.","So in fact, this approximate Q-Learning that had an intuitive","explanation, but came out of nowhere, it in fact corresponds to exactly the","case of online least squares.","Now we had this question of why we want to stick to linear things, why we","want to stick to some reasonably small amount of features.","Well, let's say you have this data.","What are you going to do with it?","Well, in general, linear is reasonably safe.","Let's imagine that was straight.","You might want to do something richer.","Like you might want to have not only x as a feature, but x squared.","And if you add x and x squared as features and you took linear","combinations of x and x squared, in fact, you would get","quadratic fits, right?","Let's imagine that's a quadratic.","And that might be better.","And so you think this is great.","With a linear combination, because I can be non-linear in my features, I","can fit more complicated things.","Let's throw in all kinds of crazy non-linearities.","And let's throw in all kinds of features, as many things as we can","possibly think of.","Well, what happens?","Well, what would happen if you tried to fit this thing with a degree 15","polynomial?","Well, guess what?","You can fit it.","Good job.","And this isn't very good because, although it gives you the right answer","on your data points, it's totally useless in between.","You might as well have given it to this guy and just said","connect these up.","This idea that you might do better, if you limit the capacity of your model","in some way, is a general idea in reinforcement learning where you want","to generalize and you don't want to over-fit.",""]}