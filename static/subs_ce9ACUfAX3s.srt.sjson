{"start":[0,1000,3480,6890,8210,10550,14350,16950,21400,23160,27770,32740,36090,40660,41780,46070,48360,51980,53750,55400,58270,61800,62890,64760,66880,68870,72820,75450,76580,79130,82620,83960,85750,89680,93760,95070,96710,99300,101020,103130,107540,110515,114550,117890,119420,122020,125090,126740,128970,131670,132360,134060,135970,137810,139870,143280,146710,148390,152890,155080,157790,161670,163810,167930,172630,175690,178240,181540,182890,186440,189160,189970,192130,193910,195110,198130,201160,202930,203920,206280,208390,211910,214180,215130,217760,218750,220460,221480,223360,225130,225920,227470,230790,235620,236950,240050,244670,246900,248200,252030,256600,259370,260589,264900,266430,269190,271700,273510,277240,281610,282910,287800,291710,293030,293800,295580,297170,299410,300730,302190,303650,305250,306750,308650,311180,313040,316020,317160,319170,320080,321250,322570,323490,324540,326040,330540,331370,334620,336060,338710,339510,342040,344060,344900,348140,350480,352610,355700,356770,358550,361370,364480,364880,366610,367340,369460,370830,371820,373850,377690,378960,381310,384490,387940,392630,393780,395160,396360,399040,400340,404950,406560,408640,410180,413780,416180,417710,420610,423730,427000,429580,431780,436030,436540,439590,443060,444370,447700,452950,456730,459730,461900,467090,469680,472750,474780,476550,479240,482720,484970,489180,492110,493890,496080,499110,501370,502590,504650,507640,511040,514110,517230,520080,524310,525740,528640,534030,535530,538730,543710,548470,553690,557930,561040,562500,566490,571110,575490,578290,581400,584220,586470,589780,590440,592860,595170,596580,600370,601300,601930,602920,604060,604800,606640,608620,612920,614250,617410,620880,623260,624500,627850,628930,631650,634660,638440,640260],"end":[1000,3480,6890,8210,10550,14350,16950,21400,23160,27770,32740,36090,40660,41780,46070,48360,51980,53750,55400,58270,61800,62890,64760,66880,68870,72820,75450,76580,79130,82620,83960,85750,89680,93760,95070,96710,99300,101020,103130,107540,110515,114550,117890,119420,122020,125090,126740,128970,131670,132360,134060,135970,137810,139870,143280,146710,148390,152890,155080,157790,161670,163810,167930,172630,175690,178240,181540,182890,186440,189160,189970,192130,193910,195110,198130,201160,202930,203920,206280,208390,211910,214180,215130,217760,218750,220460,221480,223360,225130,225920,227470,230790,235620,236950,240050,244670,246900,248200,252030,256600,259370,260589,264900,266430,269190,271700,273510,277240,281610,282910,287800,291710,293030,293800,295580,297170,299410,300730,302190,303650,305250,306750,308650,311180,313040,316020,317160,319170,320080,321250,322570,323490,324540,326040,330540,331370,334620,336060,338710,339510,342040,344060,344900,348140,350480,352610,355700,356770,358550,361370,364480,364880,366610,367340,369460,370830,371820,373850,377690,378960,381310,384490,387940,392630,393780,395160,396360,399040,400340,404950,406560,408640,410180,413780,416180,417710,420610,423730,427000,429580,431780,436030,436540,439590,443060,444370,447700,452950,456730,459730,461900,467090,469680,472750,474780,476550,479240,482720,484970,489180,492110,493890,496080,499110,501370,502590,504650,507640,511040,514110,517230,520080,524310,525740,528640,534030,535530,538730,543710,548470,553690,557930,561040,562500,566490,571110,575490,578290,581400,584220,586470,589780,590440,592860,595170,596580,600370,601300,601930,602920,604060,604800,606640,608620,612920,614250,617410,620880,623260,624500,627850,628930,631650,634660,638440,640260,641510],"text":["","DAN KLEIN: Now we're going to talk about model-free learning.","In model-free learning, we don't construct a model of","the transition function.","What we do is we take actions.","And every time we take an action, we compare what we thought was going to","happen to what actually did happen.","And whenever something is better or worse than what we expected, we adjust","our values up and down.","So what we track in a model-free approach is the values of interest","themselves, not the transition functions or the rewards.","To get us started off, we're going to think about the passive case.","So in passive reinforcement learning the idea is things are happening in","the real world.","Some agent is taking actions and getting specific outcomes that are","partially determined by chance.","You have to learn from these samples you observe, but you","don't control the actions.","Someone else is choosing the actions.","And you're just sitting there with your notebook trying to figure out,","based on the policy that is being followed, what are the values of all","the states.","So you can think about this here as this agent.","It's watching real things happen.","It can't rewind and try something else.","But it's not in control, it's just monitoring.","So the task we're going to think about in the passive case is","this simplified task.","We're not going to try to figure out how to act optimally.","We're going to try to figure out, essentially, exactly what policy","evaluation did.","The input is a fixed policy.","And so, in code, you would receive a policy and be asked to evaluate it.","In the sketch that I drew, the agent is just watching some other agent","execute this policy.","But in any case, there's a fixed policy.","The agent does not know the transitions or the rewards.","This is still reinforcement learning.","Our goal is to learn the state values.","Now, of course, because we're watching this policy, pi, execute, we're going","to learn the values under the policy pi.","If pi is really bad, we're probably going to learn values that are also","really bad, because they're the values for this policy.","So this is policy evaluation.","We're watching or executing a fixed policy.","And we're trying to figure out how good each state is under","that policy's execution.","In this case, the learner is along for the ride.","It doesn't get to actually control things, even if it's already learned","they're bad.","We have no choice about the actions.","And we're just executing the policy.","It's still not offline planning.","We're actually taking actions in the world.","We're just not being smart about picking them.","The simplest way you could imagine doing model-free learning is what's","called direct evaluation.","So remember, our goal is to compute the values for each state in our MDP","under this policy, pi.","And the idea of direct evaluation is super simple.","All we're going to do is watch action unfold.","We're going to act according to pi.","And every time we visit a state, we're going to write down what the sum of","discounted rewards, the utility, turned out to be in the end.","And so, in acting, we will have been in many states.","And for each one, we might have been there many times.","We're just going to record what happened, not in one step, but all the","way to the end.","When we average those samples together, well, that'll be an average","achieved score for that state.","That's the value.","And if we do it long enough, we'll got the right answer.","So this called direct evaluation.","Let's take a look.","We're given an input policy because, remember, this is passive","reinforcement learning, and we're only doing policy evaluation.","We're evaluating this policy.","So what do we do?","Well, we're going to get training episodes again.","And we're going to execute them one by one.","So maybe the first episode executes and we see it happen.","These are the same episodes as before.","So let's think about this one.","In this one, we were in B. We chose the action east.","We ended up in C.","We were in C. We chose the action east.","We ended up in D.","And then we were in D. We chose the action exit.","And we ended up in the terminal state.","The game ended.","We got our plus 10.","If we looked at just this episode, we could say, all right, well, I've been","in B. I know something about B. What utility did I receive from B?","It's not minus one.","That was the instantaneous one-step reward.","According to this episode, from B I received a total of plus 8.","So I might execute another episode.","This one turns out to be the same.","And again, from B, I received a total of 8.","So I've got two examples now of, according to this policy, I was in B.","And in the end, I received a plus 8 total.","There might be some other episode.","So here's one where I go from E to C to D. Again, I receive","plus 8 from the start.","But here, I receive that plus 8 from E.","And then here's the weird episode where I go from E to","C. I try to go east.","But when I execute the action east, I actually end up in A, which turns out","to be minus 10, which, by the way, I didn't know until I experienced that.","So what are we going to output?","Well, for each state we're going to write down what we saw happen.","So for example, from B, we already saw, we were in B twice, and each time","we received 8.","Great.","We've been in D a couple of times.","What's happened from D?","It's always been plus 10.","We've been in A once.","What's happened there?","Minus 10.","Now how about E?","We were in E twice.","Once we received plus 8.","And the other we received minus 12.","So what do we learn from all of that?","Well, we think 50-50, plus 8, minus 12.","What do I get?","Minus 2, right?","All right.","Here's the interesting one.","What about C?","How many times have we been there?","We've been there four times.","And what have we experienced?","Well, we got 3 plus 9's and 1 minus 11.","What's that?","It's plus 4 total.","Let's look at what we learned here.","Will this process work in the end?","Sure.","We're going to execute over and over and over from every state.","We'll eventually learn that some of the states are good and some of the","states are bad.","And eventually, they'll all be right because, from each state, we will","eventually have a bunch of executions.","And the averages will work out in the end.","But at least, for now, we've learned something that's slightly insane.","Why is that?","What do we know about E from our experience?","What happens when you follow this policy from E?","You go to C every time.","All right.","What score do you get from E?","Minus 2.","What score do you get from C?","Plus 4.","Hm.","Something doesn't compute here, right?","How can E be bad, but the state that it leads to every time","be reasonably good?","Similarly, B looks really good.","But it always leads to C, which looks kind of mediocre.","So somehow, even though this is going to work in the limit, we've thrown out","a huge amount of information about how the states are connected together.","So what's good about this?","It's really easy to understand.","You just act.","And you take averages for each state separately.","It will eventually work.","It satisfies our initial desire of computing values without knowledge of","T or R. They're not optimal values.","They're values for the policy being executed.","And it does the right thing in the end.","What's bad about it is clearly we're wasting information.","In particular, we're wasting information about how the states","connect up to each other.","If we know that a state is good, and we know that another state leads to","it, that state should be good as well.","And because we're learning each state separately, were failing to exploit","information across episodes.","That means it's going to take a long time to learn.","And we're going to have intermediate results that are weird like this one.","All right.","So the key thing that's wrong with these output values is that B and E","both go to the same state, yet somehow they have different values.","Something's weird.","So you think we already solved this problem.","We already have a way of exploiting the fact that some state s is","connected in one step to other state's s prime.","That's what policy evaluation did in the first place.","And that's what the Bellman equation said.","They said the value is determined by, essentially, a one-step expect-a-max","connected to the other values.","Now in the case of a fixed policy, the Bellman equation was","particularly simple.","And I draw these diagrams a lot.","We should make sure we understand how to read these diagrams.","These diagrams are a representation of the relevant","situation and Bellman equation.","When you're following some particular policy pi, whenever you're at a state","s, you don't have a bunch of actions to max over.","There's only one action, pi of s.","And that's shown here on the red line.","You then are in the q state of s pi of s.","And something s prime is going to happen.","Anything could happen.","And they're weighted by the transition function.","And that's why you see here multiple actions coming out.","So when I draw this, this represents the case of the action being fixed,","but the outcome being still up to chance.","And of course, if I can see all of those different outcomes, I need to","transition function to know how likely they are.","So the Bellman equations gave us this policy evaluation where we started out","with depth-limited values.","We started out with v of zero being zero.","And we computed the values under this policy pi for increasing depths.","What was the equation?","Well, it's just that diagram written down.","It says that, if you want to determine the depth k plus 1 values for some","state, you say what will I achieve from that state?","Well, whatever happens, I'll achieve 1 reward in the next time step and then","my future rewards, which are discounted by gamma.","And I have to average over all the outcomes according to","the transition function.","So what this is is it essentially says, if you've got values for the","states, replace them with a one-step-look-ahead, which in here is","just an average of real reward plus future values.","And that gets us one step further down into the tree.","And the rewards get incorporated one step at a time.","And the overall estimates become better.","So we'd like to do this, but we can't.","What's stopping us from running this in the reinforcement learning setting?","It's beautiful.","It takes states, it connects them up to other states.","It converges well.","We're missing something.","We're missing T, and we're missing R. OK, so what do we have?","We've got an average.","Look at this thing.","It's complicated.","There's pi's everywhere.","There's a discount.","But in the end, it's an average.","It's an average of a bunch of things.","Each thing is a one-step reward plus a discounted future from previous","computation.","And if we want to average a bunch of things, we're back to that case of","having a model-based or model-free option.","So this approach fully exploits the connections.","But we can't do it.","And the key question is going to be, how do we do this average without","knowing the weights?","And we already know how to take averages without knowing the weights.","And that is we look at samples of the things we're averaging.","And we add up those samples with equal weight.","So let's think about how we can turn that into an algorithm.",""]}