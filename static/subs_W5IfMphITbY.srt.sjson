{"start":[0,972,3780,5220,7210,9970,15000,18610,21180,23240,25900,28290,31670,34160,36230,39190,41990,43040,45780,48520,52570,54900,57910,60630,63650,67370,68550,69940,71480,73290,78270,79430,80740,86490,90890,92800,95180,99100,101610,107260,110420,112230,116290,120020,125380,128090,129070,131370,134680,136050,137370,141520,145160,149240,154010,158390,161570,163480,166570,168600,170500,174610,177890,181910,186230,191020,192630,193580,196100,199160,207170,211520,215120,221060,225240,226620,229440,230690,233210,235360,243130,244140,246710,248910,249820,253260,254840,256660,258050,260579,262019,265950,268940,271650,273090,275970,279190,280220,283360,285710,289130,294270,296340,300460,304740,306960,308120,311490,313640,316800,318960,319940,322380,323490,324200,325270,326580,329760,331750,334170,336100,337440,340000,343250,348960,352680,357410,361660,365840,367000,369800,373710,375080,377610,382070,383730,387760,388920,389930,392400,396510,399480,402750,405130,406470,407750,409320,411060,413000,414810,416540,418470,419790,424630,428220,430300,432470,433560,437780,440030,442260,446260,447470,448600,451030,453610,456440,460360,462440,465250,465570,471240,472670,473840,479220,482690,485870,490290,494870,498740,501150,503200,506810,507230,508320,511720,513600,516640,518830,522110,523709,526670,532230,534230,539480,541470,546980,550090,554670,556170,559100,560350,562640,564140,566120,567800,568170,569120,570030,574230,576410,578380,580930,583500,584680,585460,587370,589350,592580,595830,597330,602080,604640,605520,608980,611540,615050,620030,623380],"end":[972,3780,5220,7210,9970,15000,18610,21180,23240,25900,28290,31670,34160,36230,39190,41990,43040,45780,48520,52570,54900,57910,60630,63650,67370,68550,69940,71480,73290,78270,79430,80740,86490,90890,92800,95180,99100,101610,107260,110420,112230,116290,120020,125380,128090,129070,131370,134680,136050,137370,141520,145160,149240,154010,158390,161570,163480,166570,168600,170500,174610,177890,181910,186230,191020,192630,193580,196100,199160,207170,211520,215120,221060,225240,226620,229440,230690,233210,235360,243130,244140,246710,248910,249820,253260,254840,256660,258050,260579,262019,265950,268940,271650,273090,275970,279190,280220,283360,285710,289130,294270,296340,300460,304740,306960,308120,311490,313640,316800,318960,319940,322380,323490,324200,325270,326580,329760,331750,334170,336100,337440,340000,343250,348960,352680,357410,361660,365840,367000,369800,373710,375080,377610,382070,383730,387760,388920,389930,392400,396510,399480,402750,405130,406470,407750,409320,411060,413000,414810,416540,418470,419790,424630,428220,430300,432470,433560,437780,440030,442260,446260,447470,448600,451030,453610,456440,460360,462440,465250,465570,471240,472670,473840,479220,482690,485870,490290,494870,498740,501150,503200,506810,507230,508320,511720,513600,516640,518830,522110,523709,526670,532230,534230,539480,541470,546980,550090,554670,556170,559100,560350,562640,564140,566120,567800,568170,569120,570030,574230,576410,578380,580930,583500,584680,585460,587370,589350,592580,595830,597330,602080,604640,605520,608980,611540,615050,620030,623380,625100],"text":["","DAN KLEIN: This is active reinforcement learning.","You have to try things.","Sometimes, when you try something, it's bad.","You get a negative reward, and you keep on going.","You probably won't try that thing again, but you paid the price.","In full reinforcement learning, we would like to be able to compute","optimal policies like value iteration did.","Now, of course, we're going to make mistakes along the way.","That's going to a notion that we'll talk about later, called regret.","But we'd like to eventually learn the optimal thing.","So full reinforcement learning, we'd like to produce optimal policies.","We still don't know the transition or the rewards.","But now we're choosing the actions.","Our goal is to learn optimal things.","Eventually, we'd like to have the optimal policy and the optimal values","in our hands.","In this case, the learner is making choices.","And there's going to be a fundamental trade-off of doing things that are","known to be pretty good, versus learning about things like what","happens when I jump off the cliff.","That's a trade-off between what's called exploration and exploitation,","which we'll talk about more later.","In order to do this, we need to take a little detour.","In particular, let's remember back to the algorithm we have for computing","optimal values.","What does it do?","We had value iteration.","It starts with a vector of zeros.","And it computes new approximations to the values which converge to the","optimal values.","How did that work?","Well, what we do is we would take our current values, Vk, and we do a layer","of expectamax from every state, plugging those values in.","What's that layer of expectamax look like?","It looks like how expectamax always does.","We take a maximum over all of the actions available from s.","We average all the outcomes.","And then, for each outcome, we compute the utility as the instantaneous or","word plus the discounted future.","So this was correct.","Because it stuck a layer of expectamax, that's a one layer of","optimality, on top of whatever our old approximations were.","It buried those old approximations under one layer of optimality.","We can't do this update with samples.","Why not?","We can't do this because it's not an average.","And the only thing we can do with samples is compute averages of things.","And why isn't this an average?","It's because there's a max.","And we don't of a way to use samples to maximize.","The key idea here, in essence the biggest advance in reinforcement","learning in decades, was a breakthrough here where people","noticed, if you just shift down a half a layer in the tree and look at the","expectamax recurrence from a chance node, everything's different.","In particular, we know Q-values are going to be useful, because it's","easier to choose actions for them.","And we can write down a version of value iteration","that works with Q-values.","And let's think about what it would be.","Well, we imagine we start out with some approximation, Qk, which is the","Q-values in particular to depth k.","And so we think, all right, how am I going to figure out the Q-value for","depth k plus 1 from some state, s, and some action, a?","Well, if we knew the MDP and we had T and we had R, we would say, all, right","what is this thing?","Well, let's think.","What happens at a chance node?","What happens is you have some outcome s prime.","And for each outcome s prime, we have to average together according to T,","the utility we will receive, if this action leads to s prime.","Well, if this actually leads to s prime, what utility will we receive?","We'll receive an instantaneous reward, which is the reward of SaS prime.","And then we will receive a discount on our future value from the landing","state, s prime.","And so I might be tempted to write V of s prime here.","But I'm not learning values.","I'm learning Q-values, so I need to do one more layer.","What is the value?","The value of the state is just the maximum of all the Q-values going out","of that state.","And I have to maximize over all of the actions that I could","take from that state.","So here it is.","It's almost exactly like the other equation, but the recurrence has been","shifted a half a level.","The max is still there, right?","Here it is.","The average is still there.","There it is.","And there's still this flavor of taking reward plus discounted future.","The critical difference here is with this equation.","The thing in the front is an average.","This equation is an average.","And therefore, this equation we can do with samples.","That gives rise to an algorithm called Q-Learning.","So let's see Q-Learning.","And essentially, Q-Learning is this key algorithm that allows us to do a","lot of great things with reinforcement learning.","In Q-Learning, we're going to do Q-value iteration, that is execute","this update where we replace a Q-value with a one-step-look-ahead sample.","And we're going to do it as we go.","So every time we're in a state, s, we take some action, a.","When we do that, we're going to learn something about how good s, a is.","So what we're going to do is we're going to maintain a table that looks","something like this.","For every state and every action, it's going to maintain a number, which is","the Q-value approximation.","We're going to get some sample on the basis of the action we picked.","The Q-Learning algorithm doesn't actually care how","the action was chosen.","It does the same update, no matter how it was chosen.","So what do we get?","We get a sample.","We were in s.","We chose action a.","And so we're going to learn something about Q of s,a, that is, how good is","this action from this state?","We landed in s prime this time.","And we received reward r.","So what do we do?","Well, we consider, on one hand, we have this old estimate.","On the basis of all of our past experience, we are currently","estimating that action a from state s is worth this value Q of s, a.","On the other hand, we've just received some evidence from the world.","We've gotten the sample that says, on the basis of this particular outcome,","it looks like what we're on track to receive from s, a is the instantaneous","reward we just got, that's of R of s, a, s prime, plus","the discounted future.","And the discounted future depends on the s prime we land in.","And whatever s prime it is, we just maximize overall of the a primes that","are possible from there.","So here is our discounted future.","All right, so on one hand, we have the old estimate of how good Q of s, a is,","how good is this action from this state.","On the other hand, we see what we're on track to receive this time.","And we're going to balance them.","And it's just like before.","We're going to incorporate this new sample estimate into the running","average by saying, mostly, we're going to keep the old estimate, but with","some small weight we incorporate this new sample.","And as we do this over and over again, the samples, more or less, average","together where the recent ones are up-weighted a little bit.","And we learn over time.","This is Q-Learning.","And it's pretty awesome what it does.","Let's take a look.","All right, first, I'm going to execute it by hand.","So here is that cliff.","Everything on the bottom is bad.","And the exit on the right is good.","So watch as I go.","As I go, nothing really happens, until I receive a non-zero reward.","And as I receive the exit from here, in this case, it was plus 10.","And because my learning rate is still set to 0.5--","which is not a good learning rate, it's way too high, but it's","good for the demo--","if I keep doing this, we'll see that, as I leave here I will learn something","about what east does from this state.","And you can see I update east.","The other actions haven't been updated, because I didn't do them.","If I go here--","let's just do this for a while.","Let's build up some information about going east.","","So it looks like going east is pretty good.","What happens if I jump off a cliff?","I learn that the fire pit is bad.","And if I do it again, I learn that jumping off the cliff is bad.","[LAUGHTER]","DAN KLEIN: But I can do this all day long, and I will never learn that the","start state is bad.","Why?","I guess the important thing here is, when I move from here east, I land in","a state which has some good options and some bad options.","But when I do my Q update, I maximize over them.","And so I only incorporate that 9.8, not the negative 89.","So you can see that, even though I am jumping off the cliff every time, in","fact, the east Q-values are going up, because they look","at the optimal thing.","You can do that from the start here too.","Let's look at the crawler and its Q-values.","All right.","Ooh, flashy.","OK, do your best.","All right, so what's it doing?","Down here is, essentially, the two angles it has.","And that's its action space.","This is its state space down here.","And I'm going to skip a bunch of steps.","","And now what you can see is green means high Q-value over on the right","and high value over on the left.","And the little blue lines are the currently optimizing","action for each state.","So if I turn off all randomness, you'll see that what it's learned is","to execute the policy that's shown in the Q-values.","And that gives rise to a small loop, which gives it higher rewards, because","look at that thing trucking off to the right.","","There he goes.","","Let's talk about the properties of this.","There is an amazing result here.","Q-Learning, it looks just like value learning.","It's not.","It's so much cooler","This is amazing.","Q-Learning will converge to the optimal policy, even if you","do not follow it.","This is called off-policy learning.","You're doing whatever crazy stuff you're doing.","And it's still learning optimal values, even though you occasionally","jump off a cliff.","There's some caveats.","You have to explore enough, because you're never going to learn about a","state or an action you don't visit.","You have to eventually make the learning rate small enough.","But you can't make it decrease too quickly, or eventually it will stop","learning things.","Basically, in the limit, it does not matter how you select actions.","As long as you try lots of stuff, you'll eventually","learn the right thing.","Now selecting actions poorly may incur a large amount of regret, because you","jump off the cliff a little more often than was wise.","But in the end, you will still learn that cliff jumping is bad.","OK, next time we will talk about how to select actions and what to do when","your state space is so large that you can't keep a big table around.",""]}