{"start":[0,1720,1870,5810,9550,11680,13350,18180,20720,22420,25350,27260,29540,31070,35830,38690,42260,44270,46040,49250,54710,57050,58640,60730,63480,65860,67010,68840,70190,73070,76630,79105,81360,84850,86690,89660,90770,92680,96230,98940,101960,105380,109570,112580,115780,119730,121950,125780,126710,130070,131960,132840,135540,136250,139000,143130,145560,147520,149230,151710,155710,158780,159520,163670,166680,168930,170900,173050,174290,175800,177030,180170,183840,186070,187690,189910,190780,193000,195230,196970,201060,203330,204120,204890,208330,211230,214120,221560,224540,225790,229440,234670,236940],"end":[1720,1870,5810,9550,11680,13350,18180,20720,22420,25350,27260,29540,31070,35830,38690,42260,44270,46040,49250,54710,57050,58640,60730,63480,65860,67010,68840,70190,73070,76630,79105,81360,84850,86690,89660,90770,92680,96230,98940,101960,105380,109570,112580,115780,119730,121950,125780,126710,130070,131960,132840,135540,136250,139000,143130,145560,147520,149230,151710,155710,158780,159520,163670,166680,168930,170900,173050,174290,175800,177030,180170,183840,186070,187690,189910,190780,193000,195230,196970,201060,203330,204120,204890,208330,211230,214120,221560,224540,225790,229440,234670,236940,238190],"text":["","PROFESSOR: OK.","One last important thing about how these things work in practice--","is, in general, Q-Learning will only take you so far, and what people often","do in practice to make these really work is something","called policy search.","In policy search what you do is you directly try to improve the policy.","And the problem with things like Q-Learning is, what","does Q-Learning do?","It tries to figure out the Q value of the state.","It's modeling the states.","But that may not be the same setting of the weights","that makes good decisions.","So in particular, in project 2, you wrote down 5 times the distance to the","dot minus 2 times the distance to [? go ?] squared.","That was almost certainly not the actual value of the state, but it","distinguished good states from bad states.","And that's a general thing.","Q-Learning tries to get the values close in general.","It does not try explicitly to make the best action from a state have a higher","value than the worst action from the state.","It doesn't try to order the Q values.","It just tries to get their magnitudes right.","For action selection, you only care about the ordering.","Now, of course, if the Q values were close enough, their orders would be","correct as well.","But we're never perfect, and so there's a trade-off","between these things.","We'll also see this in much greater detail in the second half of the","course, that there's a trade-off between modeling, getting the values","right, and prediction, getting the ordering right and","making the correct decision.","The solution to this is to learn policies that maximize your rewards,","and to do that directly in some way.","And to not try so hard to figure out what the values that predict the","rewards are.","Just learn the policies.","So policy search, in general, starts with an OK solution.","For example, you might do some Q-Learning for a while, or if you've","got like a helicopter or something that's expensive to replace, you might","get an initial policy based on domain knowledge or simulation.","And then you fine tune by essentially hill climbing on the feature weights.","And here, when you hill climb, you just change the feature weights and","you see whether your rewards over time are better as opposed to just a","one-step look ahead, and then do a Q magnitude update.","So here's the simplest way you could do policy search.","Start with an initial linear valued function, or Q function, which is in","the ballpark.","Take each feature and nudge it up and down and see if your policy is better","than it was before.","It's pretty direct.","You don't actually need the Q update here.","There's some problems.","How do we tell if the policy got better in response to our nudge?","Well, we've got to run a lot of episodes because it might have only","gotten a little better, and it's going to take a lot of samples to figure out","whether or not we're better or worse, and that's going to","take a lot of trials.","If there are a lot of features, it's going to be impractical to go to each","one, one by one, nudging it up and down over and over again.","So, of course, better methods do exploit the look-ahead structure like","Q-Learning.","They do sample wisely so you make the best use of your experiences, and they","also change multiple parameters at a time.","Here's an example of so you can do with policy search.","This is an autonomous helicopter.","So this helicopter is flying itself.","But it's not actually that big.","You can't fit people in these helicopters.","This is a model helicopter.","But it turns out helicopters are very hard to fly.","Unlike a plane, which in many cases, if you just more or less keep pushing,","you'll stay in the air for a while.","Helicopters are very unstable.","If you're not constantly correcting, you don't stay in the","air for very long.","So it's hard to even get a helicopter to hover.","It's not just about blast air downward.","That actually does not work.","This is a way of getting helicopters to hover that's trained and tuned","using policy search.","So here it is.","It's amazing.","You can get these things that hover on their own, but it's actually even more","impressive than that.","Not only do they hover, they hover upside down.","","So not only can't you get people into these helicopters, people don't want","to go into these helicopters.","","We will see much more amazing things with helicopters when we take a look","at some advanced topics later on in the course.",""]}