{"start":[0,1570,4490,7450,10340,13010,14620,15710,19580,21890,24770,28930,32840,34850,38040,41380,45080,47920,49800,51840,55920,58290,59380,62520,64140,67630,71060,74780,78210,81500,85250,90320,94180,97300,101110,103290,107360,111700,113640,116020,118010,120360,123630,126940,130775,132140,135040,136470,139425,141050,144630,147600,151600,154270,156160,160910,163090,165770,167340,170200,171810,175330,178180,181460,183650,186550,190720,193580,194860,196430,199070,202370,203540,206910,209980,211550,215320,218630,221740,224210,227170,232170,235210,236340,240240,244220,247610,250720,254160,258820,260550,261800,263700,266760,268060,270850,272990,276980,281130,283730,284950,288410,292240,294420,297450,301590,303790,307350,311190,315200,319450,320280,322950,325240,329730,333280,334530,336950,339260,340780,343710,344960,348610,352490,356440,357830,360522,363550,366110,367140,369260,372070,373710,376120,379420,384300,387380,389100,393230,394210,397670,400990,405480,406680,409270,411350,414900,419810,423070,424960,428110,432680,435500,438560,440350,443470,447770,450660,452690,454350,457870,458970,462930,464930,468150,469500,474040,478030,483500,485310,486430,490070,494660,497240,501670,506420,510430,513190,514490,517090,519890,525090,528720,533410,534690,538000,541840,545000,546450,550250,551220,553340,557590,560930,563330,564100,567540,570560,573140,574510,576470,577720,581280,582190,585880,589930,592220,593320,597670,600970,602890,603725,606070,608280,612170,615350,617220,620060,623890,626220,633220,636060,639660,640380,643490,644700,647740,651010,654970,657620,660104,662030,665430,667580,668910,672360,674790,676150,677460,680010,683240,684590,686360,687870,690440,692770,694690,698050,701120,702670],"end":[1570,4490,7450,10340,13010,14620,15710,19580,21890,24770,28930,32840,34850,38040,41380,45080,47920,49800,51840,55920,58290,59380,62520,64140,67630,71060,74780,78210,81500,85250,90320,94180,97300,101110,103290,107360,111700,113640,116020,118010,120360,123630,126940,130775,132140,135040,136470,139425,141050,144630,147600,151600,154270,156160,160910,163090,165770,167340,170200,171810,175330,178180,181460,183650,186550,190720,193580,194860,196430,199070,202370,203540,206910,209980,211550,215320,218630,221740,224210,227170,232170,235210,236340,240240,244220,247610,250720,254160,258820,260550,261800,263700,266760,268060,270850,272990,276980,281130,283730,284950,288410,292240,294420,297450,301590,303790,307350,311190,315200,319450,320280,322950,325240,329730,333280,334530,336950,339260,340780,343710,344960,348610,352490,356440,357830,360522,363550,366110,367140,369260,372070,373710,376120,379420,384300,387380,389100,393230,394210,397670,400990,405480,406680,409270,411350,414900,419810,423070,424960,428110,432680,435500,438560,440350,443470,447770,450660,452690,454350,457870,458970,462930,464930,468150,469500,474040,478030,483500,485310,486430,490070,494660,497240,501670,506420,510430,513190,514490,517090,519890,525090,528720,533410,534690,538000,541840,545000,546450,550250,551220,553340,557590,560930,563330,564100,567540,570560,573140,574510,576470,577720,581280,582190,585880,589930,592220,593320,597670,600970,602890,603725,606070,608280,612170,615350,617220,620060,623890,626220,633220,636060,639660,640380,643490,644700,647740,651010,654970,657620,660104,662030,665430,667580,668910,672360,674790,676150,677460,680010,683240,684590,686360,687870,690440,692770,694690,698050,701120,702670,703920],"text":["","PROFESSOR: Today we're going to talk about how reinforcement learning works","for more substantial problems.","Let's remind ourselves what's going on in reinforcement learning.","In reinforcement learning, we imagine that the world is a","Markov decision process.","Was does that mean?","It means there's a set of states, and we can observe what state we're in.","It means there's a set of actions that we can take.","And in a given state, we know what actions we can take.","We imagine that each action has some non-deterministic result, so there's","some transition function that, for a given state in action, tells me what","can happen and with what probability.","The problem is I don't actually know the transition function.","There's also a reward function-- that for any given transition, meaning I","was in state S, I took action A, and I landed in S prime.","There's some reward, which we also don't know.","In reinforcement learning, we're looking for a policy.","In particular, we're looking for an optimal policy.","And the problem is, unlike for a known MDP, we don't know the transitions or","the rewards until we experience them.","So how's this going to work?","Well, because we don't know what the actions do, we actually have to try","various things out.","And that's going to commit us to a certain amount of trial and error,","because there's just no way to know how to act until you've experimented.","The big idea for reinforcement learning, for solving these problems","and learning to act optimally, is to look back to the things we used to do","with Markov decision processes when we knew the transition function.","And everywhere where we have an average over the outcomes, which in","order to compute requires knowing the transition probabilities, T, we're","going to instead approximate those with samples of outcomes.","So everywhere we saw an average over T, we're going to try to find a way to","change our algorithm to compute that average with samples rather than known","probabilities.","By now, you've seen a lot of things that qualify as Bellman equations,","equations that relate the value at one state to the value one state later,","after one action.","You've seen Q values in terms of Q values.","You've seen values in terms of Q values.","Some things had pi's, some things had stars.","By now, you're probably in Bellman equation overload.","All these things, when you aren't used to them, look the same.","The good news is when you're really used to these equations, they still","all look the same.","And that's because eventually, you'll realize, it's all just expectimax","written out in various ways.","Sometimes there's a max when I'm acting optimally, sometimes there's a","pi when I'm fixed to a policy.","And I can do anything I do for values or Q values, because it's just a","matter of where I start my recurrence in expectimax.","Do I start at a state, which evaluates a state, or do I start at a Q state,","which evaluates an action?","So how do we solve an MDP?","If we know the MDP, we can solve it offline without actually taking action","in the world, and we can compute anything we want.","We can compute the optimal values V star, optimal Q","values, optimal policies.","We can do that with value iteration, or policy iteration,","which we also saw.","We can also evaluate any given fixed policy, whether it's optimal or not,","and that was an algorithm called policy evaluation, where you basically","compute what happens if you follow this policy and not worry about","whether or not it's a good one.","For reinforcement learning, we don't know the MDP.","And we saw last time two different ways of dealing with the fact that","this critical quantity-- the transition probabilities that tells us","what the actions do--","was missing.","So one thing we could do that we didn't spend much time on, and we","won't come back to until much later in the second part of this class, is we","could build a model.","We could try to figure out what the transitions are by watching them.","Once we have approximate transitions, we can then reduce it to","the known MDP case.","And while that's appealing, that's not usually what people do.","Usually what people do is they take what's called the Model-Free Approach.","And in t he Model-Free Approach, we can still actually compute everything","of interest in a Markov decision problem.","What we do is we look for all of the averages and replace them with","samples, and that gives us analogs to value iteration and policy evaluation.","The analog to the policy evaluation was called value learning.","We saw that before.","And in that case, you basically just watch some agent, possibly yourself,","execute the policy and figure out how good it is based on observation.","What we're going to spend most of our time on today is a very important","algorithm, the central logarithm for reinforcement learning, really, called","Q learning, which enables you to compute optimal qualities just by","watching the outcomes of your own actions, even though you don't know","how to act optimally yet.","It's an amazing algorithm.","","So for today, we're in the paradigm of Model-Free Learning.","What's that mean?","Well, on the surface, we know it means we're not actually going to compute a","model and then solve with respect to that model.","But in practice what it means is that your agent experiences the world as a","stream of what are called transitions that form episodes.","So, for example, you start in some state, s.","You take an action.","Forget for a second where that action came from and how we knew to take it.","You receiver a reward r and then you land in the state s prime.","From s prime, you take action, a prime.","You receiver reward r prime and land s double prime, and so on.","And so there's a stream of states and actions and rewards that represents","your experience.","All of the Model-Free methods we are going to talk about in this class,","though there are others, update one transition at a time.","So every time you observe yourself going from some state, s, taking an","action, a, and getting a reward and an outcome, s, a, r, s prime, that's a","transition.","And every transition is going to cause you to improve your","knowledge and do an update.","The design of these updates is so that over time they will implement the","Bellman updates that we would have done with a known MDP in an algorithm","like value iteration.","","So most important algorithm and the one we're going to build on today is","called Q-Learning.","And Q-Learning has some amazing properties, but it took people a long","time to figure it out.","Essentially, people were held up for decades in reinforcement learning","because they were staring at value functions and trying to figure out how","to replace the value iteration update, which starts with a max, with a","sampling process.","And we don't know how to sample maxes.","But then, the key breakthrough was that what we should really do is not","be evaluating values, because that's not really what we're","interested in any way.","We should be evaluating actions.","The value of an action is its Q value.","So here's how that works.","It's that same equation we've seen over and over again.","In this case, this is Q value iteration.","And so we get this equation, which says if we knew the MDP, we would take","our Q values, which, in that algorithm, would have been to a","certain depth, k.","We take them and we use them to compute new Q values that are better.","Why are they better?","They're better because they have an extra layer of expectimax.","And the way we do that is we write down what expectimax would have done","for one layer if we started it, evaluating action a from state s.","Well, what would it have done?","It would have had to average all the outcomes, which we can't do in","reinforcement learning.","And then, for each outcome, s prime, we would have added together the","instantaneous reward for that outcome plus the discounted future, which","recurses into our current approximation, Q sub k.","So here's Q value iteration.","It basically does the same thing as value iteration, but it's shifted.","And what's cool about this is essentially this update is an average.","Now, instead of having a max on the outside, there's an average on the","outside, and we know how to do averages and samples.","So here's how it goes in Q-Learning.","We're going to receive sample transitions, so we're in some state,","s, we're in some action, a, we get a reward, r, and we land in s prime.","And, of course, that reward and outcome aren't the only possibility,","that's just what happened this time.","So what do we do?","Well we want to have the same idea of a one-step look ahead, but we've only","got one sample.","We think, what can I conclude about the Q state sa?","First of all, what is the Q state sa?","It's the value of action a from the state.","So we're evaluating action a.","We just took action, a, and we received a reward, r, and as far as we","can tell, to the best of our approximation, the reward r we just","received is going to be followed in the future by the maximum Q value for","the state s prime we land in.","Now why is it a maximum?","We assume that wherever we just landed, we will now behave optimally.","So that maximum right there essentially encodes optimal action in","the future in the right way.","So, according to this sample, it looks like we are on track to receive r,","what we just received, plus our discounted future estimate.","But then at the same time, we want to average these samples together.","And the reason we want to average the samples together is because this one","sample could be misleading.","It needs to kind of take its place with all the others.","And so what we do is we keep a running average.","And so this running average is going to be constantly maintaining Q of sa,","and every time we get one of these instantaneous one-step look ahead","samples, we're going to roll it into the running average with weight alpha.","Alpha here is the learning rate.","The bigger alpha is, the more importance we place on new samples,","which means we learn faster but less stably, because we give a lot of","weight to each sample.","Now, Q-Learning is amazing.","The reason why Q-Learning is amazing is you learn the optimal policy.","And you think, OK, fine.","We should have algorithms that learn the optimal policy.","Q-Learning learns the optimal policy even though you don't follow it.","So whereas value learning watched what you did, and it said, well, it looks","like you're getting about 10 points from this state.","Well, that seems reasonable.","It doesn't seem like magic because of course we can figure out how well our","algorithm is working for the policy we're already following.","Q-Learning tells you how well you would do optimally even though you're","messing up left and right.","This is what's called Off-Policy Learning.","Let's take a look.","","So what do we see here?","First of all, remember this is a table of Q values, which means for each","state that has multiple actions, we evaluate each action separately.","Because just because a state may be good doesn't mean all of","its actions are good.","For example, you can see here that the right thing to do is to walk straight","towards the plus 10, even though all of the squares, in fact, on the cliff","have a bad action available.","You shouldn't take it.","You shouldn't jump off the cliff.","Now, of course, this agent jumps off the cliff every now and then.","","This agent is not following the optimal policy.","In the optimal policy, you don't occasionally jump off","the cliff for kicks.","But if you look at the Q Values, the Q Value, for example, in this state","here, in the middle on the left--","the Q value to the right encodes what you will receive under optimal action.","It doesn't encode what you would receive given that you have a","propensity for jumping into the cliff.","And that's amazing.","It's not telling me how well I have been doing, it tells me how","well I could do.","That's off-policy learning.","Even though you aren't following the optimal policy, you still compute its","Q values, which means that any time I could stop bumbling around and just do","the optimal thing once I've done this process long enough to","have learned it.","A Couple other things to note.","First of all, I have no idea what this square does because I can't get there.","Of course in reinforcement learning, you're not going to learn about things","you can't try out.","Secondly, there's some stuff up here that I have never really been in a","position to try.","So maybe they're great.","I don't know.","We'll talk about these issues in a bit.","So for basic Q-Learning, what does it required to accomplish","this off-policy magic?","It requires that you explore enough.","You have to try everything.","It also requires that you make the learning rate small enough so that you","can construct a good average.","Of course, it can't decrease too quickly, because once the learning","rate hits zero or close enough to it, you stop learning.","In the limit, it doesn't matter how you select actions, as long as you","basically try everything all the time.",""]}