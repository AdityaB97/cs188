{"start":[0,1864,3600,5930,8140,11070,13790,16420,19770,23070,27110,28160,32140,34720,36410,39990,42680,44430,48430,52910,55760,58400,61930,63210,65800,68200,71470,73760,75720,78170,79580,84150,88650,92390,96620,97890,100840,103340,106030,110260,112670,114750,116880,119330,122340,123610,126640,130470,133190,135650,138180,141200,142120,144560,145760,149050,153840,157800,162150,167440,170750,175000,177630,184290,186800,189360,192950,197840,199930,201380,203660,204960,208070,211080,214220,216940,219640,221250,223450,228200,230650,233590,238060,242650,247550,252560,254590,255390,258410,259450,260950,263790,265000,269990,273600,278010,281520,282920,284660,287350,291890,294650,299350,303100,305930,308550,313630,314330,316860,320480,321970,323770,327020,328640,331650,336550,337300,340030,340890,342500,344410,351640,354540,355510,359410,362080,363530,364620,367490,371320,375150,378200,381280,383070,387090,390030,393240,394860,398080,399560,402850,407540,409890,413870,415300,417960,421000,422500,425680,429770,434520,435890,438770,440370,444050,446440,449760,451240,454270,458450,461980,463700,466230,469010,470920,474380,475500,479170,480940,483020,485340,487180,490920,492450,496290,497540,500910,503740,509050,510450,512539,513770,515350,518090,520570,524420,525950,529700,534150,535900,539480,540700,545620,547110,550340,551780,554490,557290,558250,559960,561660,564510,566910,568580,570870,573670,575540,579050,580040,581050,582630,585250,587750,588950,590025,591160,595430,598780,602600,605020,609390,613190,616110,618070,621400,624900,625730,627510,630760,632140,634700,638920,642500,644690,648130,649530,651250,653020,654270],"end":[1864,3600,5930,8140,11070,13790,16420,19770,23070,27110,28160,32140,34720,36410,39990,42680,44430,48430,52910,55760,58400,61930,63210,65800,68200,71470,73760,75720,78170,79580,84150,88650,92390,96620,97890,100840,103340,106030,110260,112670,114750,116880,119330,122340,123610,126640,130470,133190,135650,138180,141200,142120,144560,145760,149050,153840,157800,162150,167440,170750,175000,177630,184290,186800,189360,192950,197840,199930,201380,203660,204960,208070,211080,214220,216940,219640,221250,223450,228200,230650,233590,238060,242650,247550,252560,254590,255390,258410,259450,260950,263790,265000,269990,273600,278010,281520,282920,284660,287350,291890,294650,299350,303100,305930,308550,313630,314330,316860,320480,321970,323770,327020,328640,331650,336550,337300,340030,340890,342500,344410,351640,354540,355510,359410,362080,363530,364620,367490,371320,375150,378200,381280,383070,387090,390030,393240,394860,398080,399560,402850,407540,409890,413870,415300,417960,421000,422500,425680,429770,434520,435890,438770,440370,444050,446440,449760,451240,454270,458450,461980,463700,466230,469010,470920,474380,475500,479170,480940,483020,485340,487180,490920,492450,496290,497540,500910,503740,509050,510450,512539,513770,515350,518090,520570,524420,525950,529700,534150,535900,539480,540700,545620,547110,550340,551780,554490,557290,558250,559960,561660,564510,566910,568580,570870,573670,575540,579050,580040,581050,582630,585250,587750,588950,590025,591160,595430,598780,602600,605020,609390,613190,616110,618070,621400,624900,625730,627510,630760,632140,634700,638920,642500,644690,648130,649530,651250,653020,654270,655320],"text":["","PROFESSOR: Today we're going to continue talking about","Markov decision processes.","Our running example was grid world.","And grid world was basically an agent stuck in a maze.","There were squares, and the agent was in some square in the grid.","And there were walls that you couldn't move through.","The main thing about grid world is that the movement was noisy.","That is, we didn't really know the outcome of an action.","So an action like north probably would move you one square north,","but it might not.","So 80% of the time, you go where you expect, and 10% of the time, you","either go west or east, if you're trying to go north.","You'll never actually go south.","And that was important in some of the weird edge cases that we saw.","Any movement that would have put you into a wall instead leaves","you where you are.","The other thing about grid world, in addition to the fact that the actions","were non-deterministic, is that there were rewards at each timestep.","There are two kinds of rewards in grid world.","This is just one of many, many MDPs.","In this particular MDP, the big rewards come at the end when","you exit the grid.","And in this particular grid, you either get a plus 1 or a minus 1,","depending on which exit you take.","And there's also a living reward that you receive each step.","And that living reward could be zero, it could be","positive, it could be negative.","This distinction between big rewards at the end and a living","reward along the way--","this is the reward structure for grid world, not for MDPs in general.","The goal of an agent in an MDP is to maximize its sum of rewards.","And there was one tricky part, which was we had this notion of discount,","that if we chose, we could set rewards in the future to be worth less than","rewards in the present.","And that was a factor gamma that could be anywhere between zero and one.","","There were a bunch of important quantities in MDPs.","Most of them line up to something we know from search or from Expectimax.","But it's very important that we keep these quantities straight, because","some of them are very subtly different.","And especially today we're going to see lots of subtle","variations of these things.","These quantities are also important because we'll need them next week for","reinforcement learning.","So let's make sure we're totally clear on what these things are.","For the Markov decision process itself, whenever you see an MDP you","think non-deterministic search problem.","So just like a search problem, it's got states.","It's got actions that you can take in those states.","And the critical new bit is the transition function.","And this is important.","We're going to be seeing the transition function in equations all","over today.","The transition function tells you what your actions do.","That is, for any state s and action a, the transition function looks at an","outcome, s-prime, and it tells you whether it's possible, meaning its","probability is non-zero, and if it's possible, how likely it is.","And that's the probability of s-prime, the outcome state, given your action.","And we write that as T of s, a, s-prime.","Whenever you see T of s, a, s-prime, you think, oh, that's the probability","of landing in s-prime.","Similarly, each transition from s, a to s-prime comes along with a reward","that is part of the definition of the MDP.","These rewards can be positive, they can be negative.","And in addition to having a reward each timestep, there's also a discount","gamma that tells you whether or not the future rewards are worth less.","There's also a start state.","So what are the quantities?","Some of the quantities are pretty straightforward, because we've had","them around for a while.","So for example, there's a notion of a policy.","A policy is a map of states to actions.","You can think of it as a lookup table, and sometimes it is.","It takes a state and it tells you what to do.","Of course, there's good policies and bad policies, And we'll think about","that idea today.","There's this notion of the utility.","The utility for the agent is more or less the sum of the rewards it gets.","So each timestep, you get a reward.","But the utility is over an entire sequence.","It's more or less the sum, but future rewards may be discounted.","Now a critical difference is between the utility of a sequence of rewards,","or to a first approximation you add them up, and the value of a state.","The value of a state is what you expect your future utility to be under","optimal action.","So remember that.","When we talk about the value of the state, it's not the next reward you're","going to receive.","It's not a certain number--","I know I'm going to get 10 points-- because you don't know what your","actions will do.","This is an average outcome under optimal action.","And whenever you think about that average outcome over optimal action,","what should pop into your head is an Expectimax tree that looks like this.","And what you want to think about is, well, from any given state s, what is","this notion of a value?","It's not what I'm definitely going to achieve.","It can't be that, because I don't control my actions perfectly.","What it is is it's the average outcome under optimal action, meaning","essentially I do Expectimax computations.","I think, when I have an action that I control, I'm going to take a max.","And whenever I hit a chance node in my evaluation of this tree, I'm going to","have to take an average of the children, where the weights and the","average are the transition function.","So when you see value, you think, oh, that's what Expectimax gives if I run","it from a state.","It's an Expectimax value.","The critical other quantity that's new is a Q-value.","What's a Q-value?","It's just like a value.","It is what you would get in an Expectimax tree if you ran the","Expectimax computation--","that is, maxing when you max and averaging when you average.","The difference is values are what you get when you ask, what happens from","this state?","Well, I choose the best action and something happens, then I choose the","best action.","That's Expectimax.","So this is values here.","That's V. Q-values, Q, are what you get at a chance node.","At a chance node, it's too late to change the action a","that you just took.","A chance node is a state and you've committed to an action.","The first thing that's going to happen from a chance node is","the action will resolve.","Who knows what will happen?","Your transition function tells you what the likelihoods are.","But whenever you see a Q-value, you think, oh, that's what Expectimax","would give if I started it at a chance node, meaning first it would average","and then thereafter it would take maximal actions.","When we think about an MDP, we think about optimal","quantities most of the time.","So in general, we want to know for a state what is the optimal value?","That is, what is the value if I act optimally?","I take all the right decisions, and then I average over the things that","are out of my control.","Whenever I mean something optimal and I want to be explicit, I","add this star here.","This star is always going to mean optimal for us.","So V* is the value that we get starting in s and acting optimally.","That's just what Expectimax computes.","Q* is the value of the Q-state acting optimally.","Again, same thing.","Just what Expectimax computes.","Finally, there are many policies.","Some are good, some are bad.","An optimal policy can be written as pi*.","A lot of what we do with MDPs is we take in the problem specification,","which has transitions and rewards and discounts, and we compute various","quantities like this.","Sometimes we're interested in the values, but usually we're interested","in the policies.","Just like before, when we did, say, min, max, or Expectimax, we had an","algorithm for computing the values.","The reason that we ran this algorithm was because we wanted to use it to","figure out the policies.","And so at the very top of the computation, you do something special","to figure out which policy gave rise to that optimal value.","Let's take a look at what the values look like in grid world.","So here is a grid world.","Remember, grid world has lots of parameters.","In addition to the maze layout, we can vary the discount.","We can vary the living penalty.","And as we vary these things, the shape of the policy and the values","are going to change.","We saw last time that for some values of the living reward, you're very","cautious and take your time.","For other values, you jump straight into the pit.","And in between, you get other kinds of actions.","Here's one particular setting.","And the important point here is what you're looking at is values.","The numbers here are the values.","And that is that if you are in this square, where the only action","available to you is exit--","upon which you immediately receive a reward of plus 1--","your value is plus 1.","Down here, in the start state in the lower left, your value is 0.49.","What's that mean?","You're not usually going to get 0.49.","Sometimes you're going to get more.","Sometimes you're going to get less.","This is the long-term average you're going to get.","And that average bakes in a lot of things.","It bakes in the fact that you're going to get a small penalty each step until","you get to the exit.","And if your actions mess up, you might get that penalty more.","It bakes in the fact that the plus 1 is far away, and by the time you get","there it will be discounted.","It bakes in all of the probabilities in the transition function.","All of that's in the value.","If you act optimally, you'll get 0.49 as your average score.","Now what do I mean by act optimally?","Well, the optimal policy for this particular setting is also shown.","That's these arrows.","So when you look at these arrows, they're basically a lookup table.","From this state, take the action north.","It doesn't mean you're going to go north.","You'll probably go north, but you might not.","You might slip.","From this square here-- this is the interesting tricky square, where","you're on the ledge between the wall and the pit--","it says go north.","And with some probability, yeah, you're going to fall in the pit.","But for these particular settings, that's the risk you take in the","optimal policy.","So here you see a policy and values.","What about Q-values?","What do those look like?","There's a value for every state.","There's a Q-value for every state/action pair.","And for each of these states, there are four actions.","So here they are.","So let's look at the tricky case.","It's the interesting one.","That's the square here, where you're between the wall and the pit.","And what you can see here is for the action east--","so the Q-state being on the ledge and going east--","the Q-value is pretty low.","It's an optimal Q-value, but it represents being committed to going","east, so you're in that chance node, and then acting optimally.","Well, you can only do so much from there, so the value is","low for that Q-state.","The value for the Q-state going north, which you'll remember was the optimal","action from this state, is 0.57, which was the value of that","state to begin with.","So this is what Q-values look like.","They're kind of like values but partitioned, where there's a value for","each action.","And that's nice, because among other things, you can tell which option was","good by looking at the Q-values and comparing them.","A very important point about these values and rewards.","Rewards are for one timestep.","Values are from that point forward until the end of the game, or forever","if the game doesn't end.","Rewards are instantaneous.","Values are cumulative.","It's a very important distinction.",""]}