{"start":[0,692,3150,6340,10600,15060,18870,21210,21945,23760,25860,29180,30820,33100,35170,36780,39630,41890,43120,44150,48460,50410,52840,56830,59570,60730,61880,64239,67170,70010,71940,74850,76490,80670,86380,88390,90980,96710,100060,101580,107160,112030,117080,118430,119690,123910,126150,129500,131370,135460,138420,141190,143300,145590,146570,148400,151950,153090,154390,158850,161460,163350,165840,168720,169520,170560,173100,176470,178610,182880,183960,187860,191110,192650,196320,197450,200690,204460,205950,211160,216570,217460,221080,223850,227810,228910,230770,234900,238990,243650,248730,251020,254680,256860,260950,263670,266980,270480,271530,272280,273390,274470,278180,281900,284980,287260,288870,291720,293160,296960,302320,305280,306600,311250,313250,316110,319100,320780,323420,325290,327860,333480,338300,340710,341580,342600,343720,347610,352100,353980,355740,358080,361290,364170,364980,368150,370940,373810,376840,381810,384210,385660,388030,392400,395310,400490,406890,409690,411850,416560,418620,423570,427410,431266,437770,442150,443440,447420,450560,451900,456080,457010,458420,461240,462880,464420,468750,470680,473040,478410,480040,482550,486850,488490,494230,497990,502250,504130,505655,508015,508900,511330,514919,518750,521350,524330,528920,534060,535760,539950,542880,547000,549010,551380,555230,558820,560740,563330,566360,570620,571990,575220,580320,584740,589220,591240,593730,594950,599830,602480,603650,607080,610080,611610,615300,616760,620710,623240,629450,633100,635460,639220,641310,645510,647940,650460,652120,654590,657600,662270,667490,671390,674720,678570,682030],"end":[692,3150,6340,10600,15060,18870,21210,21945,23760,25860,29180,30820,33100,35170,36780,39630,41890,43120,44150,48460,50410,52840,56830,59570,60730,61880,64239,67170,70010,71940,74850,76490,80670,86380,88390,90980,96710,100060,101580,107160,112030,117080,118430,119690,123910,126150,129500,131370,135460,138420,141190,143300,145590,146570,148400,151950,153090,154390,158850,161460,163350,165840,168720,169520,170560,173100,176470,178610,182880,183960,187860,191110,192650,196320,197450,200690,204460,205950,211160,216570,217460,221080,223850,227810,228910,230770,234900,238990,243650,248730,251020,254680,256860,260950,263670,266980,270480,271530,272280,273390,274470,278180,281900,284980,287260,288870,291720,293160,296960,302320,305280,306600,311250,313250,316110,319100,320780,323420,325290,327860,333480,338300,340710,341580,342600,343720,347610,352100,353980,355740,358080,361290,364170,364980,368150,370940,373810,376840,381810,384210,385660,388030,392400,395310,400490,406890,409690,411850,416560,418620,423570,427410,431266,437770,442150,443440,447420,450560,451900,456080,457010,458420,461240,462880,464420,468750,470680,473040,478410,480040,482550,486850,488490,494230,497990,502250,504130,505655,508015,508900,511330,514919,518750,521350,524330,528920,534060,535760,539950,542880,547000,549010,551380,555230,558820,560740,563330,566360,570620,571990,575220,580320,584740,589220,591240,593730,594950,599830,602480,603650,607080,610080,611610,615300,616760,620710,623240,629450,633100,635460,639220,641310,645510,647940,650460,652120,654590,657600,662270,667490,671390,674720,678570,682030,683280],"text":["","PROFESSOR: That's an algorithm called value iteration.","You can think of this as basically building your computation from the","bottom up, all the way up to the top, where you will receive the computation","that expectimax would have done, but with a whole lot less work, assuming","we have a small number of states.","So what does value iteration do?","Here's the algorithm.","It's like building that cake.","You begin at the bottom with V0.","And for every state, V0 is 0.","There are no time steps left.","And that means there are no rewards left.","So what does this actually look like in code?","It is a vector.","There's a 0 for every state in this vector.","So this is different than expectimax, which you think about as","computing for one state.","This is all states.","It's a vector of 0's.","Let's say we've computed V sub k.","And at the beginning, that's V0.","So at any point we know, you tell me any state, and I will tell you what","happens if you run a depth k expectimax search.","We have that cached.","It's in a vector.","What we're going to do is we're going to do one ply of","expectimax from each state.","So we're going to visit state by state and run a tiny","little expectimax search.","Except this expectimax search is going to be super fast.","Let's see what it does.","What it's going to do is start at state s.","It's going to consider all actions a, just like a max node always does.","That's reflected in this equation right here.","We're going to max over all values a.","Then for each of the chance nodes s comma a, the queue states, we're going","to now have to consider every result s prime.","That's reflected here.","Again, it's this average over all of the s prime outcomes of the action a.","And for each s prime, it has some weight, T s, a, s prime.","For each of these s primes, we then look up the instantaneous reward for","that one step.","And then we recurse.","But the recursion is into an expectimax tree that","we've already computed.","So we'll add in the instantaneous reward and the recursion.","But the recursion won't actually happen.","Because the recursion has already been computed, and that's V sub k.","We already have that entire vector for every state.","And so what does this actually do?","It goes from 0 to 1 to 2.","And we keep going until we decide to stop.","When are we going to stop?","What is that magic value of k?","We can either know that our goal is 100, and we're going to stop.","That would be fine.","But we can do something even better now.","Because we're going bottom up, we can keep doing this until it converges.","And if this is the kind of MDP where you have to go very deeply into the","tree, we just run it for longer.","And we can tell convergence when we see it.","We'll talk about that in a second.","So we have this algorithm.","We start with V0.","We do this update for every level.","How expensive is each iteration of this update?","Well, what do we do?","Well, we need to compute a quantity for each state.","So that's a factor of s.","We're going to do this update for each state s.","Then for each state, we do a mini expectimax.","We have to consider every a.","So there's going to be some for-loop over actions, just like a max node","always has.","Then there's going to be a sum over resulting states.","Which in the worst case, any possible state could result, though it's often","much sparser than that.","And so we get another for-loop over states, which is another factor of s.","So every iteration of this costs s squared times the number of actions.","Is that good or bad?","Well, it's good in that it doesn't grow with the number of iterations","like expectimax grows with the depth.","It's bad in that expectimax doesn't have to touch every state if it","doesn't go too deep.","This always touches every state.","So it's all about the trade-off of how many states you have and how connected","they are and how deep you need to go into the tree.","We have a theorem that says that this process, value iteration starting at","V0 and working your way up to larger time-limited V sub k, will converge to","unique optimal values.","And the basic idea of this is that the approximations get better and better","as you go deeper into the tree.","And in particular, the policy may converge long before the values do.","","Let's see an example that will show us both that issue of the policies","converging fast and what this actually does.","Let's take a look.","We start at the bottom.","We start with V sub 0.","That's our base case.","And we always know that if there's no rewards left, then for every state, we","can put in a big 0 for the value.","Whatever policy you have, you're going to get 0 from that point on because","there is no \"that point on.\"","Let's think about V sub 1.","We have to visit each of the possible states.","So we're going to do this three times.","And for each one, we're going to do a little expectimax search.","Except only one more time step will happen, and then we'll plug in V0.","So let's do blue, the cool state, first.","So let's think about this.","You are in this state here, the cool state.","And we do a little expectimax search.","Again, as an equation, that's this.","In practice, it means we think, all right, we have to","consider the two actions.","It could be the best thing to do is go slow.","Or the best thing to do is go fast.","If I go slow, what score will I get?","I'll get a plus 1 in the next time step, and then V0, which is 0.","If I go fast, I'll get a plus 2 for either outcome.","And then the various states will be 0's.","So what should I do?","I should go fast.","I'll get a 2 there.","Similarly from the warm state, in one more step I've got the choice of going","slow and receiving a plus 1 or going fast and receiving a minus 10.","Simple choice there.","You go slow and you receive a plus 1.","And from overheated, you have no actions available.","So we can just say that that's still 0.","All of the interesting action comes at V2.","And it's not over here.","Because for overheated, you still have 0.","So let's think about how we fill these in.","Let's do the cool state first.","I've got two possibilities from this expectimax.","Either the best thing to do is to go slow and then act optimally","thereafter, meaning plug in V1.","Let's figure out that.","So I break out this computation.","If I go slow, then what am I going to get?","I'm going to get a 1 as an instantaneous reward.","And if there's no discount, then I'm going to land back at cool.","And then I'm going to plug in V1 of cool, which is 2.","And I get 3.","Or I could go fast.","If I go fast, I receive a 2 instantaneously.","But what happens after that?","Well, I have a 50/50 chance of plugging in this number, because I got","sent back to cool, and a 50/50 chance of plugging in this number.","And so what I end up with here is plus 1.5.","And if I work all of that out, now I have 3.5.","And if I work that out for the warm state, it'll come out to 2.5.","And so what do we see?","We see, one, that as I go further up, the numbers are increasing.","And that makes sense because as I have more time steps in this MDP, I can get","more rewards.","I also see that at every layer, it's better to be in the cool state than to","be in the warm state.","And of course it's better.","Because you can then safely go fast.","Now, you can look at this, and you can probably figure","out the optimal policy.","This is designed so the optimal policy is if you're cool, go fast.","And once you warm up, you go slow.","And you never risk overheating.","Now, that optimal policy of from cool go fast, from warm go slow, at what","point did we find it?","We actually found it already at V1.","These numbers 2 and 1 already reflect when you're cool go fast and when","you're warm go slow.","But it takes longer for the values to actually figure out how good that is.","Now, in general, V1 is not enough to find the best policy.","And in particular, this MDP, because there's no discount, the Vs aren't","going to converge.","So how do we know this thing is going to converge?","We're computing these V sub k vectors, and how do we know","they're going to converge?","Well first of all, we don't.","Because if there's no discount, and the rewards are all positive, and the","game's never going to end, like for racing, the values are infinite.","And you're never going to get there.","But there are cases where we can show that it will converge.","One case is when the underlying tree has maximum depth M. That means for","some reason, for some knowledge about this particular MDP, you can say that","it will end after M steps.","There's no possible sequence of actions and outcomes that lasts longer","than M. Well, then it seems reasonable that this should converge, because","once I hit VM, I've done the full expectimax search, and any further","iterations will do nothing.","But that's easy case.","The general case about why this usually converges is because usually","the discount is less than 1 And all I'm going to do is sketch this proof.","But it goes something like this.","In value iteration, we compute V sub k.","And then in the next step, we compute V sub k plus 1.","So we can think of that here as we're going from V sub k to V sub k plus 1.","And we're doing this for all states.","But let's think about a specific state.","V sub k and V sub k plus 1 both represent expectimax searches, except","the V sub k plus 1 search goes deeper by one layer.","Now, I can imagine the V sub k1 also goes deeper by one layer, except that","bottom layers filled with 0's.","That will give me the equivalent computation.","So I've got these two things.","And I can imagine they're both of depth k plus 1, except for V sub k,","that last layer is just 0.","So how can you be different?","We're doing some computation along these trees.","Well, what's the difference?","What's down here?","Well, what's down here is a mix of stuff that's the actual","rewards from the MDP.","But at best, they're all the maximum reward possible.","At worst, it's all the minimum reward possible.","And that means that V sub k, which plugged in 0, we could imagine it","either having R MIN or R MAX down there, and V sub k plus 1 is in","between those things.","And because of that, we know that the difference from that last layer is","really the maximum reward.","And because you're buried k plus 1 layers down, everything down there's","discounted by gamma to the k.","And that means V sub k and V sub k plus 1 can't","actually be all that different.","Because that last layer can only be so big or small.","And it's squashed down by the discount anyway.","What this means is that as k increases, for any given state, the","difference between V k and V k plus 1 has to shrink exponentially fast.","And that means it's going to converge, provided gamma is less than 1.","How fast it converges depends on the effect of depth.","And the effect of depth is either the actual depth of the tree or, as gamma","gets closer to one, the effect of depth, or the horizon, increases.",""]}