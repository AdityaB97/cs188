{"start":[0,800,3210,7840,9030,10560,14900,19630,22280,24330,28230,30590,32460,34220,35580,38995,41220,43840,46090,47500,50130,52140,54300,56150,59450,62320,64060,65349,68670,72760,76950,81160,82440,86130,89200,91080,93940,95600,99680,102560,106480,110780,117150,118940,121140,124640,126460,130370,132390,133310,136390,137990,139860,142400,143360,145990,147260,149800,152530,154420,156630,159730,161870,163510,164930,168330,170620,174780,177240,180640,181510,184430,185470,189560,192400,194050,195650,196640,197600,198530,201470,202930,206650,208590,209760,212550,213660,215080,217040,218850,220230,223720,226860,228160,229652,230760,231810,232600,235830,238650,239350,240670,242250,243620,245370,251110,253340,256510,258440,259720,262160,264610,266650,269530,271130,273610,278430,280800,283520,285560,289830,291550,293960,296100,298290,300370,302410,303375,305000,306420,308630,310050,312470,313400,316040,317522,319110,320240,321370,323550,324910,326010,326845,328390,331440,332530,335260,335400,336755,337760,338910,339920,340790,343510,345620,346980,348110,349140,350490,351680,352030,352770,354160,356410,357910,359080,361830,363860,366780,368150,369115,371570,374750,376240,378070,379850,383990,386210,389350,391850,395960,399350,401850,404510,405870,410450,414690,418020,419060,421390,423850,428580,430720,435050,438430,440070,441330,442710,445030,449470,451830,455260,459030,460520,464020,467430,468970,470720,473910,477710,481430,485030,489790,492560,496160,498630,500980,502470,506680,509540,512100,513679,516980,519679,522140,526220,529960,531840,535740,540060,541310],"end":[800,3210,7840,9030,10560,14900,19630,22280,24330,28230,30590,32460,34220,35580,38995,41220,43840,46090,47500,50130,52140,54300,56150,59450,62320,64060,65349,68670,72760,76950,81160,82440,86130,89200,91080,93940,95600,99680,102560,106480,110780,117150,118940,121140,124640,126460,130370,132390,133310,136390,137990,139860,142400,143360,145990,147260,149800,152530,154420,156630,159730,161870,163510,164930,168330,170620,174780,177240,180640,181510,184430,185470,189560,192400,194050,195650,196640,197600,198530,201470,202930,206650,208590,209760,212550,213660,215080,217040,218850,220230,223720,226860,228160,229652,230760,231810,232600,235830,238650,239350,240670,242250,243620,245370,251110,253340,256510,258440,259720,262160,264610,266650,269530,271130,273610,278430,280800,283520,285560,289830,291550,293960,296100,298290,300370,302410,303375,305000,306420,308630,310050,312470,313400,316040,317522,319110,320240,321370,323550,324910,326010,326845,328390,331440,332530,335260,335400,336755,337760,338910,339920,340790,343510,345620,346980,348110,349140,350490,351680,352030,352770,354160,356410,357910,359080,361830,363860,366780,368150,369115,371570,374750,376240,378070,379850,383990,386210,389350,391850,395960,399350,401850,404510,405870,410450,414690,418020,419060,421390,423850,428580,430720,435050,438430,440070,441330,442710,445030,449470,451830,455260,459030,460520,464020,467430,468970,470720,473910,477710,481430,485030,489790,492560,496160,498630,500980,502470,506680,509540,512100,513679,516980,519679,522140,526220,529960,531840,535740,540060,541310,542500],"text":["","PROFESSOR: Let's do an interlude, which is going to help us see how","MDPs, which we've just studied for a unit here and reinforcement learning,","which we're about to do--","how they connect.","So let's imagine the agent, which is basically us, for this interlude,","walks into a game where there are two slot machines that it can play.","There's a blue slot machine and a red slot machine.","And they behave a little bit differently.","When you pull the handle on the blue slot machine, you receive $1.","So this is basically a good slot machine.","Real slot machines aren't quite so nice.","You pull the handle, you get $1.","What about the red slot machine?","You pull the handle, and you're either going to get $2 or $0.","It's a little more exciting, right?","How should the agent act?","Well, let me tell you the MDP then we'll figure out how the","agent should act.","You can think of this as a Markov decision process.","It's actually a super boring one.","The state structure isn't very interesting.","The interesting thing is the actions.","You've got the blue action, where you play the blue bandit, and you've got","the red action, where you play the red bandit.","There are two states in this.","But they're really shouldn't be.","In essence, you're always in the same state, which is, what should I do?","The reason why we have two states in this formalization is because, in","order to actually make this idea work out, that you might get $0 or you","might get $2, it's a little tricky because the rewards so far haven't","been non-deterministic.","They're deterministic, it's a result state that is not deterministic.","So in order to have different rewards for winning and losing, I need","different states for winning and missing.","So here's one way to formalize this MDP.","The actions are blue and red.","And the states are, I just won or I just lost the last pool.","So if you look at this, let's first look at the blue action.","Remember the blue bandit, the blue slot machine, always wins.","So with probability 1, you get $1 and transition to the I just won state.","The red bandit wins 75% of the time and gives you $2 and loses 25% of the","time and gives you zero dollars.","And so transition structure reflects that.","It's a little redundant because the red action does the same thing whether","you just won or just lost.","That's the kind of independence that you can't see from the diagram here.","Let's say there's no discount because then things are just","going to get messy.","Let's say there's 100 times step, so that the answer isn't that everything","has infinite value.","No discount, 100 time steps.","And if we look at this, we can see that win and loss, really","there's only one value.","It doesn't matter which state you're at because the actions do the same","thing from the states.","So let's think about this MDP.","We are going to do offline planning.","So we're going to solve this MDP.","And we're going to solve it in our heads.","So let's think, you know the MDP.","We can determine things through computation.","What the value for playing blue?","You always get $1.","And if you only play blue, that's a policy, if you only play blue, you're","going to play it 100 times and get $100.","So assuming dollars here are the utilities for this agent, the value of","the play blue policy is 100.","The value of the play red policy is 150.","Why?","Because we're going to play 100 times, and on average, I mean, who knows what","will actually happen.","But on average, 3/4 of those are going to be $2.","And when we multiply all that together, we get 150.","So here's two policies and their values.","How do I figure out those values?","I did the math, right?","I thought about the MDP.","I did some computation.","In principle, I ran value iteration, but this is so simple an MDP I didn't","even have to do that.","OK, so I used math and my knowledge of the MDP to figure out the values.","Did we actually play this game yet?","No, we have not played this game.","We have just thought deep and profound thoughts about it.","I know the optimal policy.","What's the optimal policy?","It's always play red.","OK, that was offline planning.","We did not play the game.","We used our knowledge of the MDP to figure out various things, such as","values of policies, optimal actions, and so on.","Now let's actually play.","And remember that 150,000?","That was an average, right?","Let's see what actually happens.","So we're going to play.","And we decided that the awesome policy is to play red.","So we're not even going to look at the blue slot machine here.","So let's play.","We pull the lever.","We get $2, great.","What happens next?","We play red again, $2.","And again, $0, $2, $2, $2, $2, $0, $0, $0.","OK, well we just played 10 times.","We could imagine that that was 100, but we played 10 times.","So we just played our policy.","How did we do?","Looks like we got $12.","What did the values suggest we would get?","Well, on average, we should have gotten 15.","So we were a little bit unlucky, but not ridiculously unlucky.","Here, it's very important.","We solved it offline, in our heads.","We actually played in the real world, using the policy that we determined to","be optimal in our heads.","This playing, where we got the two and the zero, and we were slightly","unlucky, that actually happened.","We actually won real fake money, OK?","Let's change the rules.","You still have these two slot machines.","Blue always pays out $1.","Red always pays out $0 or $2.","But we don't know the probabilities.","All right, whats the optimal policy?","What should I do?","You 're like, I don't know, right?","We don't know the MDP.","And if you don't know the MDP, you can't figure out the","values in your head.","And you can't figure out whether playing red or playing blue is the","better policy.","You can could only use those offline planning methods when","you know the MDP.","And you don't.","OK, so what do we do?","Do we give up and go home?","Or do we play some slots?","Let's play.","All right, what would you like to do.","What should you do?","Everyone wants to play red.","Are you sure that is the optimal policy?","I mean, who knows, right?","So what we're going to play red.","Why?","Why are we doing this?","Because red might be good.","And we're not going to know until we try.","So we play red.","$0.","Player red again, $0.","Red, $0.","Now what you want to do?","Who wants red?","$2.","Who wants red?","$0.","Blue?","All right.","Blue gets you $1.","[LAUGHTER]","What's next?","Blue, ok it got you $1.","What's next Red, we got $2.","Red, we got $0.","$0, $0, are you ever going to give up on red?","OK, maybe you will eventually give up.","Smart move.","OK, so this is a different setting.","This is a setting where there is an MDP.","Red has a payoff.","And you know that it has a payoff.","But you don't know what it is.","So in essence, there's an MDP, but you don't know its parameters.","How are you going to figure out what to do?","Well, it's no longer the case that you can close your eyes, and meditate, and","figure out what's optimal for that MDP.","What you have to do is, you have to do a kind of optimal amount of discovery","mixed in with trying not to lose all your money.","What just happened here when we played that game?","What happened was not planning.","It was learning.","So this is the first time in this course where we see not just this idea","of planning on the basis of something that you know, but learning from","experiences that you have, specifically, you just did","reinforcement learning.","You took actions and received rewards, and thereby","learned how to act optimally.","There was an MDP, but you couldn't solve it with pure computation because","you didn't know all of its parameters.","You needed to actually act and pull the lever to figure it out.","Because of that, you just saw all of the important ideas in","reinforcement learning.","So let me go through some of them now.","We'll see all of these again.","One of this idea of exploration.","And that is, you have to try unknown actions to gather information.","Another idea is exploitation.","And exploitation says, eventually, you have to use what you know.","You can't just keep pulling the red lever forever out of curiosity of just","exactly how bad it is.","At some point, you have to do things that you know are successful.","And balancing exploration and exploitation is a big problem in","reinforcement learning.","There's an idea of regret.","And that is, even though that you all did the right thing, you played red","for a while until it was obvious that it was not very good, even though you","acted optimally, in the sense that given what you knew, that was the","right thing to do, you still incurred what's called regret, which is you","didn't do as well as if you would actually known the MDP and used the","optimal policy with respect to the actual in MDP.","And of course, there's always going to be regret against perfect knowledge","because you need to make some mistakes in order to figure","out how to be optimal.","You observed sampling.","Because there's chance involved, you have to try things many times before","you actually know how they're going to shake out.","And you still never really know.","You also saw it was more difficult.","This is the simplest MDP you could possibly imagine.","And it was instantly obvious when I showed you the numbers that you should","always play red in the original case.","All I have to do is take that payoff probability away, and suddenly we have","a hard problem of how long to play red, and when to give up,","or if to give up.","So these problems become much more difficult, even the MDP is simple,","your lack of knowledge then triggers a much more difficult reasoning problem","about how you should act.",""]}