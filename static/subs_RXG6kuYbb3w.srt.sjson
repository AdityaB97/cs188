{"start":[0,800,4470,8090,10500,13660,15190,18600,23660,26470,32750,34230,36750,40140,45480,47230,50470,52230,55180,58080,59420,64160,68820,70150,71430,72830,75140,75760,80690,83330,85360,86320,89590,92190,96170,99620,103460,104470,108090,111490,113030,116950,119420,120700,121780,122410,124150,128780,132650,136270,138800,142290,144800,149910,151410,154320,157870,160990,163960,165490,166360,167650,171450,175150,178300,179820,181300,184750,188520,190355,194170,197320,200490,204120,204900,208460,213780,216500,220180,222800,224710,227220,229030,233550,237920,242230,244840,247030,247830,251430,254690,257120,260230,264390,266900,270920,274120,276510,279940,282960,285750,289890,291590,294790,296970,301190,304360,307600,310100,313960,316420,319590,320920,323670,326560,327530,331000,333320,335320,336780,339220,341740,343460,344560,347310,350750,352540,355540,356990,360860,363790,367590,371290,374380,375560,377750,378810,382240,383590,387520,389160,391390,394760,397590,400220,403540,405700,408980,412400,415850,419750,423060,426420,430330,434350,438450,442140,443580,446960,449370,452740,456300,459040,461050,464820,466160,469160,470380,473790,475840,478460,481040,484010,487320,492250,495110,497700,500810,504130,507260,510194,511000,512150,514950,519240,521870],"end":[800,4470,8090,10500,13660,15190,18600,23660,26470,32750,34230,36750,40140,45480,47230,50470,52230,55180,58080,59420,64160,68820,70150,71430,72830,75140,75760,80690,83330,85360,86320,89590,92190,96170,99620,103460,104470,108090,111490,113030,116950,119420,120700,121780,122410,124150,128780,132650,136270,138800,142290,144800,149910,151410,154320,157870,160990,163960,165490,166360,167650,171450,175150,178300,179820,181300,184750,188520,190355,194170,197320,200490,204120,204900,208460,213780,216500,220180,222800,224710,227220,229030,233550,237920,242230,244840,247030,247830,251430,254690,257120,260230,264390,266900,270920,274120,276510,279940,282960,285750,289890,291590,294790,296970,301190,304360,307600,310100,313960,316420,319590,320920,323670,326560,327530,331000,333320,335320,336780,339220,341740,343460,344560,347310,350750,352540,355540,356990,360860,363790,367590,371290,374380,375560,377750,378810,382240,383590,387520,389160,391390,394760,397590,400220,403540,405700,408980,412400,415850,419750,423060,426420,430330,434350,438450,442140,443580,446960,449370,452740,456300,459040,461050,464820,466160,469160,470380,473790,475840,478460,481040,484010,487320,492250,495110,497700,500810,504130,507260,510194,511000,512150,514950,519240,521870,523120],"text":["","PROFESSOR: In an MDP, the rewards come to you step by step.","And because they come to you step by step, we need to figure out what our","utility function actually is.","In the simplest case, you just add up the rewards, but it can be a little","more subtle than that.","For example, as shown here, you might care whether or not you get these four","gems step by step or all at the end in one big prize.","This raises a general question for MDPs.","What preferences or utilities should an agent have for reward sequences?","Now you might think that's easy.","You're asking me whether I want more rewards or less rewards.","So for example, if I give you the choice between the reward sequence","plus 1, plus 2, plus 2, plus 2, or plus 2, plus 3, plus 4, which one","should be agent want?","Well, it seems pretty reasonable that the agent should want the one where","the numbers are bigger.","So adding up the rewards will give you that effect.","But it's a little trickier, because there's one other question, which is","now or later?","So for example, if I have the choice between 0, 0, and 1 in three","successive timesteps, or the sequence 1, 0, 0, the sum is the same.","Which one should I prefer?","Or should I be indifferent?","If I added them up, I'd be indifferent.","They have the same sum.","What do you think?","Should the agent prefer A or B?","Most people think the agent should prefer B. And that makes sense,","because if you're going to get this reward, you might as well get it now.","Well, why?","It's the same reward in three days.","The answer is, it's kind of not, right?","It's perfectly reasonable to maximize the sum of your rewards, but it's also","reasonable to prefer a reward now to a reward later.","One solution to this is to imagine that the values of your awards decay","exponentially.","So for example, the same gem that's worth plus 1 now--","if I get it tomorrow, it's only worth some smaller amount","which we call gamma.","That gamma is called the discount or the decay rate here.","In two steps, it's only worth gamma squared.","I don't even want it anymore.","Well, its sign's not going to change.","I'm still going to want it.","It's just not going to be worth nearly as much.","So let's look a little bit more closely about how and why we do this.","So first of all, we've got this tree which we might solve with a variant of","Expectimax, although we're going to have new methods very shortly.","How exactly do we do this thing where things are","discounted gamma every timestep?","Well, we're going to be recursing into the tree, and we could imagine that","every time we descend a level, we multiply in gamma against whatever the","recursion returns.","So we look at the sub-tree and say, oh, you're worth 35 points.","No, gamma times 35, because you're in the future.","And if we do it that way where we multiply in one gamma every step down","the tree, the deeper we get into the tree, the more gammas we get, and","we'll get this exponential decay.","So that's how we do it.","But why should we do it?","Well, one answer is actually we do like the rewards to be sooner rather","than later, but it also turns out that this discounting has other nice","effects, like it helps our algorithms converge.","Now why is it exponential?","Again, there's two answers.","One is, if you just think about money and investment, there are many cases","where if you have the reward now versus later, you could just invest it","at some kind of exponential growth.","So there might be some natural reason why exponential decay is appropriate.","But it's also going to turn out to be a mathematical consequence of some","basically reasonable assumptions that we're going to make.","So we did this discounting, and we had a discount of 0.5-- which is really","steep, right?","Rewards are a half as much every time tick that passes.","Then if we had the sequence 1, 2, 3 and I said, how many [? U-tiles ?]","is this worth, it wouldn't be 1 plus 2 plus 3.","The first 1 would be worth its full amount, but the 2 would only be worth","half, because it's one timestep in the future, and the 3 is","only worth a quarter.","And the effect of that would be 1, 2, 3 would not be as","desirable as 3, 2, 1.","And now we have this preference being reflected, where even if the sum of","the rewards is the same, I'd rather have them soon.","So what will we want from an agent that looks at sequences of rewards in","order to consider it reasonable?","Remember, back when we talked about utilities and preferences, we have","these axioms?","And we said, these preferences are rational, and if they are rational,","then I can conclude they can be represented by a utility function.","There's kind of an analogy here with sequences.","And that is, there's a notion of stationary preferences.","We say that preferences are stationary if, whenever I prefer one sequence to","another-- so this A sequence to the B sequence--","if I stick the same rewards in front of both-- say, I stick an extra","timestep in front at plus 0.4--","my preferences should be unchanged.","If I liked A better than B now, I should like it better shifted into the","future as well, and vice versa.","This seems pretty reasonable that preferences might be stationary.","If preferences are stationary, then it turns out that there's only two ways","you can define utilities.","And that is add up the rewards or add them up discounted.","And if you stare at that, you'll see that's actually only one way, because","if gamma were 1, the discounted utility would be the additive utility.","So under the stationary preference assumptions, we're going to end up","discounting by some multiplicative factor.","Now, there are cases where your preferences won't be stationary.","For example, if certain things change at certain times, that's not what","we're looking at here.","There's another problem with these games like the racing game, which is","they can last forever.","The smart race car, it turns out, will never overheat.","You can see that this is possible because if you always go slow, you'll","never overheat.","And maybe there's a better way to get more rewards but still not overheat.","And so you have these games that can last forever.","And so what kinds of rewards are you going to get?","You're going to get infinite rewards.","Plus 1, plus 2, plus 2, plus 1.","You add these up forever, and you end up with-- pretty much whatever you do","that's reasonable is going to be infinite.","That's kind of bad.","I mean, it's good if you like rewards, but it's bad if you're trying to","decide between policies, because a string of plus 1's and a string of","plus 2's looks the same.","So what do we do about these games that last forever?","There are multiple solutions.","And these will have analogs in the further discussion that we have.","One solution is to say, there's a finite horizon.","So I'm going to say, yeah, if I let the race car plan forever for its","infinite road trip, we would get infinite results and we wouldn't be","able to decide fast versus slow, because anything that doesn't overheat","is equally infinite.","I can say, look, racecar, you've got 100 steps.","Do your best.","And now because the game ends, we don't have this problem of comparing","infinities.","This actually gives non-stationary policies, so that what you actually do","may depend on the amount of time left.","And we'll see examples of this.","Another solution, no surprise, is discounting.","If we use a discount factor that is less than 1--","and to make sense, it should be greater than 0--","then even if I have an infinite sequence of positive rewards, then","there's this exponentially decaying thing that's going to make sure their","sum converges, provided the rewards themselves are bounded.","What does it mean when I change the discount from more like 1, meaning not","much discounting, to more like 0, meaning I don't care about the future?","This changes what's called the horizon of the agent, which is when things are","steeply discounted, we care more about achieving rewards right now.","When things are not simply discounted, we don't care so much about how soon","they are, and we try more for bigger rewards even if they're later.","Another solution is to look at an absorbing state.","So if we change the racing game such that you always had a probability of","overheating, no matter what you did, then the game would eventually end","with probability 1.","Sure, there are some sequences where you just magically never overheat, but","as time goes on those get less and less likely.","And so if we have this way of guaranteeing that for any sequence of","actions that you do, you'll eventually hit a terminal state with probability","1, that also fixes the infinite utilities problem.","So here are multiple possible solutions.","In general, we're going to have discounts, and that usually saves us.","So where are at?","We've defined Markov decision processes, which are non-deterministic","search problems.","They're defined by a set of states and a start state.","They have a set of actions.","And then they have a transition function which tells you what your","actions do and with what probability.","And they have a reward function which tells you for every possible","transition what your reward will be.","They also have a discount which tells you how much you should downweight","future utilities when you plan.","We've also talked about two important quantities.","We've talked about the notion of a policy, which is a mapping from states","to actions, and we've talked about this notion of a utility.","And so far, the utility has just been on a sequence of rewards.","So I say, here's a sequence of rewards, and I add them up or I add","them up discounted.","That's the utility.","Of course, because we don't control what happens, we're going to need to","start talking about expected utilities and have a notion of value, which is","going to turn out to correspond to Expectimax values.",""]}