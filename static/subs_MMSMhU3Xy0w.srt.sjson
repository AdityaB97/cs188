{"start":[0,1648,4070,6990,10280,12750,16640,20030,21570,22610,24190,25705,26770,30170,31420,33500,34890,37590,39150,41010,42290,44900,46490,49510,51750,54360,58090,60830,62880,67380,69890,71320,73470,75150,76840,80780,82850,87070,90510,92820,94320,98300,101760,102980,106330,109990,113710,114950,117720,120020,122900,124480,125760,129910,131670,134230,137180,138850,142860,145450,148190,149820,153200,156630,158600,161450,162980,166320,168220,174040,178040,180770,184260,187420,191590,193730,196550,197700,200330,202980,205120,207550,209760,211010,215030,217770,219300,220960,224980,227370,230240,232640,234150,237250,239040,241310,244160,245800,247800,253130,254280,256579,259104,262520,267920,270760,273830,280190,282020,284770,287990,291960,295380,298990,300710,302500,305860,307250,312400,316890,320010,321630,326800,327740,330990,332250,336110,338230,340150,343180,346000,348490,349780,351600,354340,357230,358920,362510,366450,368190,371760,373660,378170,381010,383470,384520,386380,389160,390750,394460,400250,402310,404110,405980,408940,411830,414610,416710,419400,421180,422940,424230,425420,425930,427730,430940,432620,436870,438170,440620,441600,444450,445690,450000,452590,454060,455390,456930,458040,460510,462080,465400,466600,469280,472860,475860,478630,481320,483980,487120,491045,493690,497540,501580,505940,507430,511300,514780,516409,520030,523710,526870,529950,534890,537980,539320,542090,543700,546420,548280,551280,555410,556430,559820,561910,563960,566780,569930,574350,577940,581030,583370,587540,589190,590590,592160,594860,598170,599500,601910,606700,607880,609920,612170,613830,618300,620290,622200,624170,626130,627780,630600,635130,639560,645330,646330,648860,652520,656040,657860,660800,662240,664680,667220,669080,671050,673290,677290,680690,682150,685030,688760,691060,694020,698030,699030,703600,705240,707850,709930,713990,715790,720480,724060,726190,729050,731850,736500,737950,739800,742420,746310,749290,753030,755560,757430,759940,762810,762956,766660,769250,772310,774800,779180,783420,787360,790830,793550,797830,800980,804440,809770,813340,815120,818550,821590,823470,826900,828620,831960,833120,836700,840360,843090,845600,846180,849910,851530],"end":[1648,4070,6990,10280,12750,16640,20030,21570,22610,24190,25705,26770,30170,31420,33500,34890,37590,39150,41010,42290,44900,46490,49510,51750,54360,58090,60830,62880,67380,69890,71320,73470,75150,76840,80780,82850,87070,90510,92820,94320,98300,101760,102980,106330,109990,113710,114950,117720,120020,122900,124480,125760,129910,131670,134230,137180,138850,142860,145450,148190,149820,153200,156630,158600,161450,162980,166320,168220,174040,178040,180770,184260,187420,191590,193730,196550,197700,200330,202980,205120,207550,209760,211010,215030,217770,219300,220960,224980,227370,230240,232640,234150,237250,239040,241310,244160,245800,247800,253130,254280,256579,259104,262520,267920,270760,273830,280190,282020,284770,287990,291960,295380,298990,300710,302500,305860,307250,312400,316890,320010,321630,326800,327740,330990,332250,336110,338230,340150,343180,346000,348490,349780,351600,354340,357230,358920,362510,366450,368190,371760,373660,378170,381010,383470,384520,386380,389160,390750,394460,400250,402310,404110,405980,408940,411830,414610,416710,419400,421180,422940,424230,425420,425930,427730,430940,432620,436870,438170,440620,441600,444450,445690,450000,452590,454060,455390,456930,458040,460510,462080,465400,466600,469280,472860,475860,478630,481320,483980,487120,491045,493690,497540,501580,505940,507430,511300,514780,516409,520030,523710,526870,529950,534890,537980,539320,542090,543700,546420,548280,551280,555410,556430,559820,561910,563960,566780,569930,574350,577940,581030,583370,587540,589190,590590,592160,594860,598170,599500,601910,606700,607880,609920,612170,613830,618300,620290,622200,624170,626130,627780,630600,635130,639560,645330,646330,648860,652520,656040,657860,660800,662240,664680,667220,669080,671050,673290,677290,680690,682150,685030,688760,691060,694020,698030,699030,703600,705240,707850,709930,713990,715790,720480,724060,726190,729050,731850,736500,737950,739800,742420,746310,749290,753030,755560,757430,759940,762810,762956,766660,769250,772310,774800,779180,783420,787360,790830,793550,797830,800980,804440,809770,813340,815120,818550,821590,823470,826900,828620,831960,833120,836700,840360,843090,845600,846180,849910,851530,852780],"text":["","PROFESSOR: OK, today we're going to be talking about Markov decision","processes, which are a way of formalizing the idea of","non-deterministic search, which is search when your action's","outcomes are uncertain.","Why would we be unsure what the outcome of our actions is going to be?","Well, maybe we've got a robot on a ledge, and we take the action","to cross the ledge.","What's going to happen?","Well, maybe we'll cross the ledge.","Maybe we'll fall into the fire pit.","We're not really sure.","We can commit to the action, but of course the outcome isn't entirely","under our control.","","That's going to happen in general.","There's just all kinds of times where you take some action--","maybe, you're a can opener robot.","And you take the can and you open it.","And what's underneath?","More can, right?","So let's try to formalize this.","Our running example is going to be this grid world.","You're going to see this as well on your projects.","So grid world is a maze-like problem.","You've got an agent living in a grid, just like we did for project one.","There are going to be walls that block the agent's path.","And then there are going to various rewards.","The tricky thing is going to be that the actions of the agent can now fail,","and in particular, movement is going to be noisy.","So if I take an action--","for example, north is an action--","I can attempt to go north.","It doesn't mean that I actually will.","In this particular instance of grid world, 80% of the time, the action you","take has the expected result.","For example, the action north actually moves you 1 square to the north.","10% of the time, however, an action will take you clockwise--","and then 10% of the time, counterclockwise--","from the intended direction.","So in this particular case, if I try to go north, 80% chance of going","north, but I've also got a 10% chance of going west and a 10% chance of","going east.","If for any reason there's a wall in the way of the direction this rule","would take you, you stay put.","Like I said, there are rewards in this maze, and the whole point is to gather","the rewards.","There are two kinds of rewards in the grid world.","One are the terminal utilities.","And here these are shown as plus 1 and minus 1, and they're","visualized here as a gem.","That's a plus 1.","If you get to that gem and you take the action \"exit,\" the game ends and","you get a plus 1.","The red fire pit is shown as a minus 1.","If you get into that square, you take the action \"exit,\" and you","receive a minus 1.","There's another kind of reward in grid world, which is every step you take","comes along with a little tiny reward.","And this is sometimes called a living reward or a living penalty, based on","whether it's positive or negative.","And if it's positive, well, you walk around and get happier.","If it's negative, then it costs a little bit of pain every step, and you","want things to end quickly.","The big rewards are, in general, going to be at the end.","So what should the agents goal be?","Well, the simplest goal and the one we'll take for now is the agent should","maximize the sum of its rewards.","In general, that's going to involve getting to a big reward and taking it.","If this were deterministic search in a normal maze, this is what your","successor function and your actions would do, right?","You're in some state, you take the action north, and the result of that","is you are now one square to the north.","In grid world, because it's noisy, you can still take the action north, but","now you're not sure what the result's going to be.","So in this particular case, when the agent tries to go north,","maybe it goes north.","In fact, that's the 80% outcome in this case.","But there's a 10% chance of going to the left or the right.","And if you end up going to the right, that's OK.","If you end up going to the left, that's pretty bad.","And so when you plan, you're going to have to take these","outcomes into account.","","Formally speaking, this kind of problem--","grid world in this case--","is a Markov decision process.","A Markov decision process is a lot like a search problem.","It's got a set of states, just like search did.","And it's kind of got a successor function like search did.","Unlike in search, we're going to take the successor function and break it","into a few pieces.","We're going to have an idea of actions, which are the actions you","take, like north, south, east, or west.","We're then going to have a transition function.","And this notation is important because we're going to see it over and over","again in the next few lectures.","So let's look at the transition function.","The transition function here we write T of s, a, s-prime.","What is that?","You are in some states s.","You take some action a.","s-prime is a possible result.","The function T(s, a, s-prime) tells you how likely that result is.","And in that sense, it's a conditional probability.","So when you see the transition function, you think, oh, that's the","probability that I end up in s-prime, if from state s I take action a.","This is called the transition function.","It's also sometimes called the model, because it's your way of representing","what actually happens in the world, and sometimes called the dynamics,","because it represents how the world evolves in response to your actions.","This transition function is basically the successor function.","The difference is now there are lots of different s-primes that can happen.","And they all have various probabilities, T,","associated with them.","Also very much like search, but slightly different, is this idea of a","reward function.","The reward function is R of s, a, s-prime, meaning you get a reward that","depends on the state you were in, the action you took, and the outcome.","You might not know your actual reward until you see whether or not you fell","into the pit.","In some formulations, R will only depend on s or s-prime.","What's this?","This is basically the cost function from search, except in search, we want","the cost to be small.","In the case of MDPs, in general, we want the rewards to be big.","Of course, we've also got this idea of a start state.","We might have a terminal state.","Here's another important difference between MDPs and search problems,","because MDPs very often go on forever.","And we'll see an example of that soon.","So what are MDPs?","They're non-deterministic search problems.","They're basically taking search that we know and love and adding the","necessary machinery to support the idea that actions can","have multiple outcomes.","In fact, this is the class of problems that Expectimax search solved.","We'll actually get a new tool very soon that can solve them in sometimes","a better way than Expectimax.","But to a first approximation, when you see an MDP, you think, oh, that's a","lot like what Expectimax did.","I have a choices at a state, but then my actions have uncertain outcomes.","Let's take a look at grid world actually happening.","So here's grid world.","For now, ignore the numbers.","They'll become important later.","And you can also watch down here at the bottom, which is going to echo","what I press.","So what I'm going to try to do is I'm going to try to get the agent, which","here is the blue dot, into the good reward square, which is up here.","Remember, there's a plus 1 there.","And I'm going to walk it step by step.","And I'm going to go to the right and up.","You won't be able to see what I press except when it echoes below.","The important thing is usually the key I press happens.","But sometimes the agent slips.","So I'm going to press east.","And look, you can see I took the action east, but I","ended up where I started.","I'm going to take it again.","I went east.","I'm going to try east again.","It worked.","I'm going to try north.","Now this is a little tricky, because if you think about this square here,","I'm taking a little bit of a risk.","Because this lower exit here is actually a pit.","Hopefully I won't fall into it.","90% chance.","It worked out.","I'm going to go east again and that succeeded.","I end up in this square.","Now in the exact formulation of grid world here that we have, when you get","into one of these exit squares, you're not done yet.","You don't receive your reward yet.","You have to take a special action.","It's the only action you can take.","It's called exit.","And so now I'll be forced to take the exit action, and I will","receive my plus 1.","And if I scroll through the rewards, you'll see I got a bunch of little","negative 0.1's.","That was the living reward, or in this case, a living penalty.","And then at the very end, I ended in the exit state.","I took the action to exit, and I got a reward of 1.","That's how it works in grid world.","OK, Markov decision problems.","Who's this Markov guy and what's he doing in my search problem?","In general in this course, Markov is going to mean the same thing.","It's going to mean that, given the present state, the future and the past","are independent in the appropriate way.","For a Markov decision process, what that specifically means is that the","probability distribution over your outcomes depends only on the current","state and action, not on the whole history of how you got there.","This is actually just like search.","If you remember, for search, we had to make sure our state was appropriate","such that the successor function only depended on the state and not the","history of how I got there.","And so sometimes we had to encode things into our state like which food","pellets were left and so on, in order to get that property.","An important property in an MDP is to make sure that you define your","transition function and your state in such a way that the transition","probabilities depend only on the current state and action.","If we remember back to single-agent search problems, what did we do?","We input a problem.","Some solution method like A star search ran.","And the output was a plan.","The plan was a sequence of actions to transform the start","state into a goal state.","That's not going to work here, because for example, if you remember, when I","was doing the grid world by hand, that first step I tried to go east, but it","didn't happen.","So if I just blindly executed east, east, east, north, north,","north, east, exit--","well, it wouldn't have worked in that case.","And in general, I'm not going to know what sequence of actions to take,","because I don't know what those actions are going to do.","The relevant idea is not a plan now but a policy.","And what a policy is, is a mapping from states to actions.","And it tells me in each state what action to take.","So actually, you can see here in the illustration--","you can see an example of a policy for that same grid world that in every","state tells you what to do.","So it might tell you to go north.","It might tell you to go east.","If you end up trying to go east and you end up in the state where you","started, it just tells you, go east again, because that's what you do in","that state.","So a policy gives an action for each state.","An optimal policy is the one that maximizes the agent's expected utility","if followed.","So there are good policies and bad policies.","We're going to want to be able to find the best ones, which","are the optimal policies.","If we actually wrote the policy out explicitly, meaning we wrote down for","every single state what to do--","like what's shown here on the map--","then in fact this defines a reflex agent.","You look at your state, you do your action.","No more thinking is required.","All the thinking went into coming up with the policy.","Now Expectimax didn't really compute an explicit policy in this sense.","What Expectimax did for these kinds of problems is, from a given state, it","did a forward-thinking computation that produced one entry of the policy,","which you then took.","And wherever you landed, you ran Expectimax again.","So on one hand, Expectimax is a way of solving these problems.","And on the other hand, it doesn't compute an explicit policy.","That can be a good thing or a bad thing.","It can be bad because you might have to redo a lot of work if you keep","ending up in the same states.","It can be good because sometimes you have so many states you couldn't write","down an explicit policy anyway.","Let's see some optimal policies.","These are all instances of grid world.","They all have a plus 1 and minus 1 exit.","Where they differ is the reward you receive on every step.","So what's shown here is the optimal policy if the living reward--","in this case, living penalty--","is very, very mild.","So there's not a lot of pressure to hurry up and end the game.","So let's look and see what this policy does.","Well, mostly it's pretty interpretable.","For example, from here, we're going to go up and around.","Let's see what else it does.","Well, it kind of does this funny thing in this square here.","What's it doing there?","It's going into the wall.","Why does it go into the wall?","This is an example of, we feed the rules of the game in, we compute the","optimal behavior, and maybe--","like that AIBO that likes to poop the soccer ball out in order to shoot--","sometimes the behavior isn't what we expected, but it's still optimal.","So in this case, what is this position?","Essentially you've got this pit right behind you.","You really, really do not want to fall into the pit.","Rather than trying to move north or south, you move into the wall.","What can happen?","Well, you're probably going to stay put.","That's not too bad, because the living penalty is small.","But you might go north or south, both of which are safe.","If you did anything else, you would risk the pit.","So in fact, this is the agent pressing its chest against the wall and just","waiting, waiting, waiting, until it gets out of this","scary situation safely.","This agent is very conservative, because each","timestep costs very little.","All right.","What happens if we make this living penalty more severe?","So we're putting a little bit of pressure on the agent, that if it just","wanders around, it starts losing points.","Well, here now it's minus 0.03.","And you can see it's still the case that we go around to the exit.","Something that's changed here is from that scary position we now go straight","towards the exit, and we risk the pit, because that small chance of falling","into the pit is not worth the time it's going to take to shimmy our way","out by trying to bang into the wall.","However life is still cheap enough that we go the long way around from","the bottom squares.","What happens if we make the living reward even worse?","Well, again, from some squares we head towards the positive exit.","But now we're willing to take the shortcut.","We're not going to take the long way around.","Yes, the shortcut risks the pit.","What happens if we make the living reward extremely severe?","What happens if we make it minus 2?","It costs you minus 2 every step regardless, of what you do?","What do you want to do?","You want to end the game, which means straight into the exit.","Any exit will do.","The pit looks bad, but it's not as bad as moving.","So you get a slightly depressed agent when the living rewards are negative.","And maybe that's not surprising, given what we've input","as a utility function.","OK.","So this is another example of, we input the utilities and the behavior","emerges from a unified computation.",""]}