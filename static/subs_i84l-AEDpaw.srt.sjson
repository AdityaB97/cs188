{"start":[0,1290,5660,11260,14480,17840,19470,21900,26460,29010,30210,31740,35670,37820,39270,42990,46830,49620,51260,54680,57960,61700,63220,67400,68920,72000,76320,79330,81200,83670,86580,89760,91750,95180,98120,99590,101980,105170,109410,111850,114970,117010,120530,121340,125610,127350,129620,133820,135410,137880,139920,142610,146700,147950,152170,153400,157300,160080,162880,164120,166950,168110,169740,173530,177280,182290,183570,185040,187620,189930,192890,194040,195390,201180,205540,209530,210550,212500,214420,217820,221430,222510,224890,226390,230560,231460,235770,239280,243300,249790,253360,254120,255150,258779,259910,262410,263720,265750,268080,271110,274250,275980,276950,277930,278480,280520,283190,286610,288510,291000,294620,295920,298370,299600,301990,306110,310220,312160,315680,319530,322560,324450,326040,326940,330610,332030,334490,335940,336600,337870,338870,343560,344920,346270,348070,349930,354770,357630,359620,360610,361900,364590,365890,368280,369500,374210,375450,377950,382270,384450,388070,389380,392950,396440,400390,401390,403330,407830,408940,412650,415620,419950,422220,423410,426270,429050,430020,433030,434720,439450,442650,445040,447950,450780,453100,454170,456970,458980,462500,464870,467330,470250,473500,477050,481440,484930,486340,489940,493230,495360],"end":[1290,5660,11260,14480,17840,19470,21900,26460,29010,30210,31740,35670,37820,39270,42990,46830,49620,51260,54680,57960,61700,63220,67400,68920,72000,76320,79330,81200,83670,86580,89760,91750,95180,98120,99590,101980,105170,109410,111850,114970,117010,120530,121340,125610,127350,129620,133820,135410,137880,139920,142610,146700,147950,152170,153400,157300,160080,162880,164120,166950,168110,169740,173530,177280,182290,183570,185040,187620,189930,192890,194040,195390,201180,205540,209530,210550,212500,214420,217820,221430,222510,224890,226390,230560,231460,235770,239280,243300,249790,253360,254120,255150,258779,259910,262410,263720,265750,268080,271110,274250,275980,276950,277930,278480,280520,283190,286610,288510,291000,294620,295920,298370,299600,301990,306110,310220,312160,315680,319530,322560,324450,326040,326940,330610,332030,334490,335940,336600,337870,338870,343560,344920,346270,348070,349930,354770,357630,359620,360610,361900,364590,365890,368280,369500,374210,375450,377950,382270,384450,388070,389380,392950,396440,400390,401390,403330,407830,408940,412650,415620,419950,422220,423410,426270,429050,430020,433030,434720,439450,442650,445040,447950,450780,453100,454170,456970,458980,462500,464870,467330,470250,473500,477050,481440,484930,486340,489940,493230,495360,496610],"text":["","DAN KLEIN: So the question is, how are we going to be able to learn how to","act when we don't know what the actions do or what rewards we'll get?","You might think we can just reduce this to what we had before.","We can somehow still use value iteration or expect a max.","And that's true.","What we're going to talk about now is model-based learning.","In model-based learning, we reduce the reinforcement learning problem to the","previous case of a known MDP.","So how do we do that?","Well, we try to build a model.","We try to figure out what the transitions and rewards do and then","pretend that's the truth.","So here's how it works.","We're going to learn an approximate model, based on our experiences.","Once we have that approximate model, we will solve for the values, for","example, the optimal values or policies, as if the","learn model were correct.","If we learned a model that's pretty close, maybe the policy that our","dynamic programs produce will be good ones.","So we'd like to learn an MDP that we can use for value iteration.","How are we going to do that?","Well, we're going to, at times, find ourselves in certain states, s, and","take certain actions, a.","Let's not worry for now about how we decide what action to take.","Whenever we're in state s and we take action a, various things happen.","So we end up with the various s prime outcomes.","Over time, we count them up.","Some of them happen more and some of them happen less.","Maybe one of them happens 90% of the time.","When we take those counts and normalize them, meaning divide them by","their sum, we get probabilities.","For example, that frequent one will have probability 0.9.","That is our estimate of the transition function for that","state and action pair.","We do that for all state and action pairs.","In addition, we discover that the rewards associated with a state s and","action a and a state s prime whenever we experience that transition.","So we're going to act for a while.","We're going to accumulate counts of various things.","We're going to turn them into probabilities.","And once that's all done, we have a transition function t and a reward","function r.","They're probably not correct, but they are structurally what we need to run","things like value iteration.","So we learn an MDP, and then we solve it.","Now there are a lot of small points here that are very important, like how","do you know how to act?","How do you know how many counts you need?","How do you know how close you're going to be?","We're not going to get very much into those details now.","We'll come back to the idea of how to learn a good model in the second half","of the course.","But for now, I just want to give you a sketch of how this works.","Here's an example.","Let's say we're a grid world and somebody gives us this policy pi.","I'm trying to, for now, sidestep the issue of how we know how to act.l","we're going to follow this policy for better or for worse.","And so what are we going to do?","We imagine our gamma is one, so there's no discounting to make this","example simple.","And we have some experiences.","So for example, we might have the following episodes of training.","So in these episodes, this is four times playing the game.","For example, in the first episode, we start at B. We go east into C. We go","east into D.","And then we take the exit action.","We transition to the terminal state where the game is over.","And we receive our big reward of plus 10.","In this particular case, you get a minus 1 living reward and a plus 10","good exit reward.","Episode 2 is the same.","Episode 3 starts in E, goes north into C, and then east into the good exit.","Episode 4 is interesting because we start in E, we go north into C. And","when we try to go east, we actually end up in A because there's some","non-determinism.","And there, we're forced to take the bad exit.","So this is what we've got to work with.","In a model-based setting, what we do with all of this experience--","which is not everything that could happen, it's just what we personally","have seen--","what we do with this experience is we try to figure out what the","transitions must be.","So we think, all right, I've been in B. I've taken the action east, and","what's happened?","Well so far, 100% of the time, we've ended up in C. So I'll write down to","the best of my knowledge right now, let's just say that east from B has a","100% chance of resulting in C. That's a transition function.","Similarly, east from C, we know that 3/4 of the time it actually went east","into D. But 1/4 of the time it went north into A. Are those the right","probabilities?","Almost surely not.","But they're a reasonable starting point for this simplistic version.","So what comes out?","We've learned now a model where we have transitions.","This is probability one.","This is probability 0.75 and so on.","And we've learned reward functions.","Now, of course, we've only learned about the things we've actually tried.","But let's set that aside until later in this lecture.","What do we do with this learned model?","Well, now it's an MDP.","It's the wrong MDP.","Oh well.","But we still know how to solve it.","That's the idea of model-based learning.","So let's think about exactly what we just did and what","model-based learning is.","Let's imagine we had a much, much simpler problem.","Let's imagine that my goal was to compute the average age of people in","this class.","So I want to compute the expected age, the weighted average.","How would I do that?","Well, we all know how to compute expectations.","And so, for example, if I knew the probability distribution over ages,","how many people are each age, then I would have a straightforward way of","computing this weighted average.","And the way I do that is I'd say, well, the expectation of this random","variable, A, I sum over all of the possible ages.","And I weight each one by its probability.","So it's just a weighted average according to the","distribution over ages.","Easy.","In fact, this is all that's really going on in these MDPs.","This is what happens at a chance node.","You take a weighted expectation of values that are","one step in the future.","So here it is.","Here's the weighted expectation.","What's that mean?","That means I actually look up for each value, oh, how many people are age 20?","Oh, that's 0.35.","And then I continue.","That's like solving an MDP.","You know the probability distribution.","And so you just compute the expectation by summing everything.","Now what happens if we don't know the distribution?","We're forced to rely on samples.","We could take a few samples.","We could take a lot of samples.","Obviously, the more samples, all else equal, the more accurate","these things will be.","We're going to collect samples, instead of the probability","distribution.","The model-based view is, all right, I don't know the distribution over ages,","but I've got samples.","What do I do with the samples?","The model-based view is, I use the samples to reconstruct an","approximation of what I didn't know.","So I say, all right, my empirical distribution over ages is whatever the","samples say.","And now I've reduced it to the case of the known distribution.","And I compute expectations the way I always do, by summing over all of the","different ages and weighting each one by their relative frequencies in the","distribution.","Why does this model-based thing work?","The reason it works is because, eventually, you learn the right model.","So what else could we do?","We could stay in the same setting where we don't know P of A. And we","could do what's called model-free approaches here.","What we do in a model-free approach is we average the samples directly.","So for example, this person's 20.","This person's 30.","I add up all of the samples and give them equal weight.","And I take their, in this case, unweighted average.","It's super important.","When I do this model-free stuff, the averages are unweighted.","Why does that work?","That works because the ages that are more frequent show up in more samples.","So they're already up-weighted by virtue of the fact that they pop up","more often in the sample outcomes.","And because the samples are going to appear with the right frequencies,","either approach is going to do the same thing.","Now in this case, you look at this and you think, it's pretty","much exactly the same.","It's just about how and when you group things.","Nothing important is happening here.","And that's because there's no structure to this example problem.","The algorithms are going to be very different in the reinforcement","learning case, because there's going to be structure.","Things are going to depend on the other states around them.","But this is the high-level view of model-based versus model-free.","In model-based, you learn the probability distributions.","And then you reduce it to the case of solving a known MDP.","In the model-free case, we just take our samples as they come and average","them together.","And while the model-based way may seem more comfortable at first, because it","reduces to a known case, we'll see that the algorithms for the model-free","case are, in many cases, much simpler.",""]}