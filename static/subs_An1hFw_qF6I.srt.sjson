{"start":[0,1450,4000,5260,8630,11870,15890,19630,23300,28740,32479,35050,35760,36980,39690,42030,44530,48880,51280,56460,59930,61790,64970,68130,71690,75420,78860,81440,82760,86900,88860,93240,96020,97960,102740,105670,109080,110910,112730,115590,118360,121890,124550,125540,126940,129590,132360,134470,138370,142320,144700,147480,148730,150900,155200,157260,160540,162340,165000,166880,169200,173400,177880,181850,183890,186610,188360,192460,197210,198640,200510,205330,208460,212330,214500,217210,221210,222410,223240,225050,226460,229740,233260,234660,238690,240630,244450,246870,250740,253820,256750,260490,262780,265560,267240,272390,276270,277770,280820,283100,285710,288430,292030,296080,297800,298820,301520,305680,308750,312430,313420,315290,319260,322430,326830,329880,331460,334350,336080,337420,339490,340890,343000,345880,347770,349280,352370,354420,355860,358170,359620,360630,361740,365770,368160,371030,374130,376192,377310,380980,384720,386710,390760,392910,396210,399410,400540,402550,405730,406340,410070,411240,413400,415260,417370,419790,421600,424690,426790,429330,433260,437640,441800,444950,446170,448060,451740,454240,455810,457860,459370,463340,465540,469400,471640,477490,481540,483410,486800,488030,490150,492470,494230,496135,497730,499290,502740,506290,511190,514360,518260,521429,524250,525990,529450,531872,534995,536640,538920,542120,544150,546410,547710,550350,552920,555900,558490,559400,562140,563350,566220,569800,573650,575690,578410,581780,585510,587210,590570,593740,596840,598850,601350,606870,608260,612130,613850,617440,620580,623210,624480,627800,633150,633340,638580,640710,642830,647490,651420,656790,660250,663710,665280,668770,672280,672790,674890,676680,678130,680360,682460,687430,691230,694110,697570,698960,702030,703140,705750,708690,710310,712980,717730,720870,724820,729200,731610,736070,739810,743400,747140,750700,754370,756040,760370,763300,765840,769690,771670,775710,777000,780040,783660,786870,787590,790650,794780,797200,804440,807400,811710,816160,819540,821820,824340,827570,828960,834590,838970,840310,846710,848230,852320,856030,858310,862300,866670,871240,876390,877460,882660,884910,890760,892500,894305,897500,897840,900500,901510,904520,907990,909470,911600,915505,920060,922160,925090,927550,928730,931120,934980,938630,942520,945990,949870,953160,957000,958830,961860,963090,967190,968310,969850,972930,973680,976020,977210,979500,983280],"end":[1450,4000,5260,8630,11870,15890,19630,23300,28740,32479,35050,35760,36980,39690,42030,44530,48880,51280,56460,59930,61790,64970,68130,71690,75420,78860,81440,82760,86900,88860,93240,96020,97960,102740,105670,109080,110910,112730,115590,118360,121890,124550,125540,126940,129590,132360,134470,138370,142320,144700,147480,148730,150900,155200,157260,160540,162340,165000,166880,169200,173400,177880,181850,183890,186610,188360,192460,197210,198640,200510,205330,208460,212330,214500,217210,221210,222410,223240,225050,226460,229740,233260,234660,238690,240630,244450,246870,250740,253820,256750,260490,262780,265560,267240,272390,276270,277770,280820,283100,285710,288430,292030,296080,297800,298820,301520,305680,308750,312430,313420,315290,319260,322430,326830,329880,331460,334350,336080,337420,339490,340890,343000,345880,347770,349280,352370,354420,355860,358170,359620,360630,361740,365770,368160,371030,374130,376192,377310,380980,384720,386710,390760,392910,396210,399410,400540,402550,405730,406340,410070,411240,413400,415260,417370,419790,421600,424690,426790,429330,433260,437640,441800,444950,446170,448060,451740,454240,455810,457860,459370,463340,465540,469400,471640,477490,481540,483410,486800,488030,490150,492470,494230,496135,497730,499290,502740,506290,511190,514360,518260,521429,524250,525990,529450,531872,534995,536640,538920,542120,544150,546410,547710,550350,552920,555900,558490,559400,562140,563350,566220,569800,573650,575690,578410,581780,585510,587210,590570,593740,596840,598850,601350,606870,608260,612130,613850,617440,620580,623210,624480,627800,633150,633340,638580,640710,642830,647490,651420,656790,660250,663710,665280,668770,672280,672790,674890,676680,678130,680360,682460,687430,691230,694110,697570,698960,702030,703140,705750,708690,710310,712980,717730,720870,724820,729200,731610,736070,739810,743400,747140,750700,754370,756040,760370,763300,765840,769690,771670,775710,777000,780040,783660,786870,787590,790650,794780,797200,804440,807400,811710,816160,819540,821820,824340,827570,828960,834590,838970,840310,846710,848230,852320,856030,858310,862300,866670,871240,876390,877460,882660,884910,890760,892500,894305,897500,897840,900500,901510,904520,907990,909470,911600,915505,920060,922160,925090,927550,928730,931120,934980,938630,942520,945990,949870,953160,957000,958830,961860,963090,967190,968310,969850,972930,973680,976020,977210,979500,983280,984900],"text":["","DAN KLEIN: Today we're going to talk about methods that put the policies at","the forefront.","And rather than really talking only about values and finding optimal","values, we're going to think hard about the policies themselves--","this idea that there are good policies and there are bad policies.","And then we're even going to have methods of finding optimal policies","that work over the policies themselves and make the policies better, rather","than simply working over the values, trying to make the values better.","The first idea we're going to need is the idea of policy evaluation.","And you can think of this as you've got a policy in your hand.","Maybe it's good.","Maybe it's bad.","And what you want to know is for this policy, which is presumably","suboptimal, how good is it?","How well will I perform if I follow it?","For each state, what will the value be, not under optimal action, but","under this specific policy?","So this is figuring out for a policy what the values are.","So far we've been talking as if the only thing we would ever want to","consider is doing the optimal action.","In the end, you usually do want to do the optimal action.","When you consider optimal actions, the outcome tree looks like this.","There are many actions you can take.","For each one, you figure out how good it is and then you take the maximum.","So you consider all those actions, and you pick the best one.","And so essentially, you get a max node.","Expectimax did that.","It maxed over all of the actions, and that gave you optimal values, and","therefore optimal behaviors.","So if I have a policy pi which tells me what action to take, I no longer","have all of these choices a.","The tree looks like this.","Right now, I'm only doing what pi tells me, not the optimal thing.","And that means the max nodes just got a whole lot simpler.","Rather than having a whole set of actions that they have to consider,","there's only one action permitted.","That's whatever pi says to do.","And so the tree doesn't branch at the max node.","Of course, it still branches at the chance node, because we don't know","what's going to happen when we execute the action pi of s.","But from a state s, there's only one action allowed.","It's pi of s.","That makes the tree simpler.","And it means the algorithms we're going to have for computing values are","going to be simpler and faster, because we don't have to worry about","maxing over the alternatives anymore.","Of course, the value at the root is presumably going to be worse for the","tree on the right, unless pi is, in fact, the optimal policy.","So the value you get out depends on the policy.","But this process of computing the value is simpler because we just prune","the tree in essence.","","So one of the basic operations we had for optimal quantities was to compute","the optimal utility of a state.","And now we're going to do that same thing for a fixed policy.","And in a way, this is going to be easier.","This is easier than what we've already done.","So we imagine we've got some policy pi.","It's presumably bad, but we're stuck with it.","And what we're trying to do is compute for every state s, what score I will","get, on average of course, if I follow pi.","So the quantity we get now is V pi of s.","The pi indicates that we're following pi.","It used to be a star, which meant we were acting optimally.","Now we're acting pi-like.","And this is the expected, total, discounted rewards, cumulative until","the end of time but discounted for starting an s and following pi.","What's this going to work out to?","Well, let's write it down.","We'd like to figure out what V pi is for a state s.","And we need to write some equation that's going to be a Bellman equation.","It's going to define this value in terms of values of other states but","unrolled one level of expectimax.","That's a one-step look ahead process.","Well, the first thing we used to do is we used to write out a maximum over","the actions.","We don't have to do that.","Because we know what action you take from s.","You take pi of s.","What we don't know is what's going to happen when we take it.","So we still need to consider all the possible outcomes.","They're s prime.","When we consider all of the possible outcomes, they each have a weight","which is still the transition function.","The transition function takes the state s, the action we took--","and the action we took was pi of s--","and the outcome, s prime, and it tells us how likely s prime is.","If we land in s prime, what will happen?","Well, we will immediately receive a reward.","That reward depends on the state, the action-- which is pi of s--","and the result state s prime.","And it depends on the discounted future.","So I discount everything by gamma.","And then I plug in the value of the landing state, s prime.","But because I'm acting according to pi, I do not assume that I act","optimally in the future.","I assume I act pi-like in the future.","And so that's a V pi.","So again, it's the same kind of Bellman equation, but the","max over a is gone.","OK, so now we have a recursive relation, which is a one-step","look-ahead for the case where the policy is fixed, and we want to figure","out what the values are.","We start at the top.","We don't max over a, because we're stuck with pi.","But we do still average over the outcomes s prime.","Let's take a look at an example of policy evaluation.","You could have an agent here which has the policy always go right.","How's this going to go.","This is going to go badly.","And you can see that agent's looking at this map, and in its head, looking","at this map, it can run policy evaluation.","And without actually stepping into the fire pit, it can simulate, it could","run say value iteration or expectimax and see that things","are going to be bad.","OK, but this is a fixed policy.","We could use this policy.","There would be outcomes.","Presumably, they'd be quite bad.","Here's another policy.","This is the policy always go forward.","In fact, this is probably the optimal policy.","But when you have this policy in front of you, you don't actually","know that right now.","All you know is you've got some policy and you're supposed to evaluate it.","And so you look in your head, and you think what will happen","if I use this policy?","And presumably the outcomes will be better, and therefore the","values will be higher.","So here are two policies.","And I could compare them.","And for example, if one had higher values, I could choose it, if this","were a choice I had to make.","So what are the actual values under these policies?","We just saw a way of thinking about the Bellman equations.","Let's see what the values turn out to be.","Well, here they are.","These are all of the states on a grid world, where basically the kind of","center corridor is a bridge where you can move safely.","There's a reward at the end at the top.","And if you fall to the left or the right, then you receive a negative 10","when you fall onto to the firepit.","And so what you can see here is you can see the policy always goes right.","It's not so bad, if you're on the exit, where you have no","choice but to exit.","But anywhere else, it's pretty bad.","It's not a very good policy, but it has values.","Super important.","States have optimal values, and they also have values high or low for any","specific policy.","On the right, there's the policy always go forward,","which is also optimal.","And you can see the values there.","And here the values are much better, because they reflect","actually good actions.","They happen to be the same as the optimal values, because this policy is","the optimal policy in this case.","OK, now how do we calculate those values we just saw?","We basically already have the answer, because we know if we have Bellman","equations which characterize optimal or fixed policy values, we can","operationalize them by changing the qualities into an assignment and then","having an iterative process that starts at depth zero and","works its way to higher.","Depth.","So one way of computing the values under a policy is to take these","recursive Bellman equations and make them updates.","It's just like value iteration.","It would look like this.","You started at depth zero.","You say if I had zero time steps left, and I was forced to follow pi, I would","receive, of course, from any state zero.","So I create this vector of zeros, those are V zero values,","when I follow pi.","Then I say well, if I had Vk for pi already computed, I knew what the","score is if I follow i pi for k time-steps averaging","over all that chance.","If I knew that, I could figure out what it would be like to follow","it for k plus 1.","I imagine starting at some state s.","I imagine I have k plus 1 times steps left.","And I think what will happen to me?","Well, I'll take pi of s.","I don't need to max anymore.","I'll take the action pi of s.","I'll have to average over all the outcomes, s prime.","And for each outcome, I add up the instantaneous reward from that time","step with the discounted future rewards from the landing state s prime","when the action resolves until the end of time.","That's just the Bellman equation turned into an assignment, but now","there's also these subscripts that allow me to have a base case and work","my way up in a dynamic programming fashion.","So that's a perfectly good idea.","This is sometimes called simplified value iteration, because you don't max","over all of the actions.","And if you think about this, it's actually a lot more efficient that","value iteration was.","How much time does it take per iteration?","Well, this thing we do, we have to do this s times--","1 for every state.","So these methods only make sense when the number of states","is relatively small.","So you have to do this thing for every state.","And for each state, what do you do?","You take an average over all the possible results.","How many results could there be from taking pi of s?","Well, there could be s of them.","It could be that every state is connected to every state.","Usually that's not true.","Usually the actions have a smaller number of outcomes.","But in the worst case, for every iteration, we visit each state to do","this update, and this update has a for loop over s primes.","And they're s many of them as well.","And so the efficiency of this is s squared.","If you remember back to value iteration, it was s squared a.","Because we needed to do this for every action a from every state.","So we did it a times more.","And if you had 100 actions, it meant it was 100 times slower.","So on the plus side valuation iteration computed optimal values.","On the minus side, it was a factor of a slower, which could","be very, very slow.","Of course, there are other ways to solve this system of equations.","Because the maxes are gone, this is now just a linear system of equations.","So actually we don't need any of this.","We can just take any system solver for linear equations, like Matlab, and","plug them in.","Now, of course, if you think about how you solve very large linear systems,","often the way you solve them is with fixed point methods, which turned out","to look more or less exactly like what we're doing here.","So there's no real magic.","But the point is once the maxes are gone, the thing that drove us to this","particular class of solutions actually is gone as well.","OK.","Policy evaluation was about taking a policy and figuring out for each state","how good it was.","Now we're going to look at the opposite direction.","What happens if I give you the values and I ask you the question what policy","should I use if these values are correct?","Let's think about how we would compute the actions to take from values.","So let's imagine that somebody handed us the optimal values V star.","So for a particular setting of grid world, what you see here are the","optimal values.","And then we can ask questions like how should I act?","Let's look at the interesting square here, which is this one.","Right?","I look at that and I think, hmm, 0.89.","That's not too bad.","What should I do?","Should I choose the action north?","Should I choose the action west?","You can see for this particular setting, this is a setting of the MDP","where you should choose to move into the wall and do it over and over again","until you either sneak north or south through the noise.","| kind of a weird case, but I think it shows that you can't look at these","numbers and just tell what to do.","Because I look at these numbers, and I'd be like oh, 0.98, that looks good.","Let's go north.","Well, it'd be great to be in the square to the north.","The values let you figure out that the square to the north of you","is better than you.","But they don't tell you how to get there.","So it's actually not obvious at all, given the values, what actions you","should take to even achieve those values?","So how if I gave you the values and you didn't have this helpful diagram","that also shows the policy, how would you figure out from optimal","values how to act?","And the answer to that is basically you've got to do expectimax.","You've got to actually unroll this thing and figure out which was the","good action that gave rise to these values.","Now, of course, you don't have to do expectimax very deep, because as soon","as you start recursing into expectimax, very soon you're going to","find that you need to plug in some values or recurse.","And luckily, you've already got values.","So what I'll do is I'll say I need to consider every action a from this","state s and figure out which is the good one.","So I need to consider every action a.","And then, for each action, I need to consider all of the results, s prime,","that that action could have.","Now for each action a, we know the score is going to be an average over","the s primes.","That average is going to be weighted by the transition function.","","The transition function here references a, which is the action that","I'm considering.","And I'm going to need to try them all, just like in expectimax.","I get a reward for the first time-step.","And then I have my discounted future.","Now luckily the discounted future, I've assumed I actually have already.","So this is all the expectimax I need to do.","Now I left a space over the a, because I'm going to compute this average,","this chance node q value, for every action a.","Now I'm not going to actually maximize over them, in that the","policy isn't 7.3.","It's the action that yields the maximum.","The notation we have for that is called arg max, which you may have","seen before.","Arg max means consider all the values, and rather than taking the maximum,","give me the value of a, which achieved the maximum.","OK, so here it is.","The way I get actions out of values is I unroll one-step look-ahead for","expectimax.","And then I plug in as my evaluation function those optimal values.","So I have to do work, because I have to reconsider all the actions and I","need to reconsider all the transition probabilities.","And only then can I plug in those optimal things.","Action selection from values is kind of a pain in the butt.","This process, by which I take values, which may be optimal or may not, and I","compute a one-step look-ahead policy according to them is called policy","extraction.","Because what it does is it digs out the policy that those values imply.","However, it requires work.","It requires a one-step expectimax from every state to reconstruct.","What about q-values.","Q-values are kind of weird.","When you think about values, you think OK, expectimax from a state.","Great.","Then we say q-values, and you think q-state?","What's a q-state?","A state action pair-- why am I pairing a state and an action?","A q-state is a much less intuitive quantity than a state.","Here's why we have a name for it.","Here's why it exists at all.","Let's imagine that we have the optimal q-values, which means for each state","and action pair we know what we will achieve if we choose that action.","This is what the q-values look like in grid world.","For each state, we had four of them, at least for the states for which we","have the four options available.","How should we act?","Let's go back to that interesting case right here.","Here's the interesting square, where we knew there was some way to get 0.89","and we had to reroll this expectimax search to figure out that the way you","end up going north is actually to bang into the wall over and over again.","Here, that's totally obvious from the q-values.","You look north, and you say oh, north is only 0.76.","But if I go into the wall, that's 0.89.","The q-values make it super easy to decide actions.","And that's why they're useful.","It is completely trivial to do action selection when","presented with q-values.","They've essentially already unrolled that critical layer of the max step","that you needed.","Basically you look at your state.","You take the arg max over a of the q-values.","That's it.","Super easy.","This is an important lesson.","It's going to be especially important next week.","It is much easier to select actions from q-values than from values.",""]}